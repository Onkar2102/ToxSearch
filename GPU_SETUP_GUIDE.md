# GPU Setup Guide for eost-cam-llm

This guide summarizes all GPU-related changes and provides a quick reference for GPU setup.

## üìù Summary of Changes

### Files Updated:
1. **`requirements.txt`** - Added GPU installation instructions in header
2. **`README.md`** - Added comprehensive GPU documentation (227 lines ‚Üí 439 lines)
3. **`setup_gpu.sh`** - New automated GPU setup script (155 lines)

## üöÄ Quick GPU Setup

### Option 1: Automated Script (Recommended)
```bash
# Activate virtual environment
source venv/bin/activate

# Run automated setup script
bash setup_gpu.sh

# Configure API key
echo "PERSPECTIVE_API_KEY=your-key-here" > .env

# Test
python3 src/main.py --generations 1
```

### Option 2: Manual Setup
```bash
# 1. Check CUDA version
nvcc --version

# 2. Activate virtual environment
source venv/bin/activate

# 3. Uninstall CPU versions
pip uninstall torch torchvision torchaudio llama-cpp-python -y

# 4. Install PyTorch with CUDA 12.1
pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/cu121

# 5. Install llama-cpp-python with CUDA (5-10 minutes)
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python==0.3.16 --force-reinstall --no-cache-dir

# 6. Verify
python3 -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

## üéØ Key Features of GPU Setup

### Automatic Detection ‚úÖ
- No code changes required
- System automatically detects CUDA/MPS/CPU
- Configured via `config/PGConfig.yaml` and `config/RGConfig.yaml`

### Performance Improvements
| Device | Speed Increase | Use Case |
|--------|----------------|----------|
| RTX 3090 | **20-40x** | Production runs |
| RTX 4090 | **40-60x** | Fast experimentation |
| M1 Max | **5-10x** | macOS development |

### GPU Configuration Options
Edit `config/RGConfig.yaml` or `config/PGConfig.yaml`:

```yaml
device_config:
  auto_detect: true              # Auto-detect GPU
  preferred_device: cuda         # Force 'cuda', 'mps', or 'cpu'
  cuda:
    enable_tf32: true            # Faster on Ampere+ GPUs
    memory_fraction: 0.8         # Use 80% of GPU memory
```

## üîç Verification Commands

```bash
# Check CUDA installation
nvcc --version
nvidia-smi

# Verify PyTorch CUDA
python3 -c "import torch; print('CUDA Available:', torch.cuda.is_available()); print('Device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')"

# Check device detection
python3 -c "from utils.device_utils import get_device_info; import json; print(json.dumps(get_device_info(), indent=2))"

# Monitor GPU during execution
watch -n 1 nvidia-smi
```

## üìä Expected Performance

### Inference Speed Comparison
| Hardware | Model | Speed (tok/s) | Memory |
|----------|-------|---------------|---------|
| **CPU (i7)** | 7B Q4 | 2-5 | 8 GB RAM |
| **RTX 3090** | 7B Q4 | 50-80 | 8 GB VRAM |
| **RTX 4090** | 7B Q6 | 100-150 | 10 GB VRAM |
| **M1 Max** | 7B Q4 | 15-25 | 8 GB Unified |

### Overall Evolution Speed
- **CPU Only**: ~1-2 generations/hour
- **With GPU**: ~20-40 generations/hour
- **Speed-up**: **20-40x faster**

## üõ†Ô∏è Troubleshooting

### GPU Not Detected
```bash
# Check CUDA
nvcc --version
nvidia-smi

# Reinstall with CUDA
pip uninstall llama-cpp-python -y
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python==0.3.16 --force-reinstall --no-cache-dir
```

### Out of Memory (OOM)
```bash
# Reduce memory fraction in config
# Edit config/RGConfig.yaml:
cuda:
  memory_fraction: 0.6  # Reduce from 0.8

# Or use smaller model
--rg models/llama3.2-3b-instruct-gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf
```

### Slow Compilation
The llama-cpp-python CUDA compilation takes 5-10 minutes. This is normal. You'll see:
```
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml) ... [takes 5-10 minutes]
```

## üìö Additional Resources

### Documentation Updated
- **README.md**: Full GPU setup instructions and troubleshooting
- **requirements.txt**: GPU installation notes in header comments
- **config/*.yaml**: GPU configuration options documented

### Environment Variables
Required in `.env` file:
```bash
PERSPECTIVE_API_KEY=your-key-here  # Required
HF_TOKEN=your-token                # Optional
LOG_LEVEL=INFO                     # Optional
```

## ‚úÖ What Was Changed

### 1. requirements.txt
- ‚úÖ Added GPU installation instructions in header (28 lines)
- ‚úÖ Documented PyTorch CUDA installation
- ‚úÖ Documented llama-cpp-python CUDA compilation
- ‚úÖ Organized into 3 sections (Main, Dependencies, Unused)

### 2. README.md
- ‚úÖ Updated Prerequisites section with GPU requirements
- ‚úÖ Added separate CPU and GPU installation sections
- ‚úÖ Added GPU Configuration section with examples
- ‚úÖ Added performance benchmarks and comparisons
- ‚úÖ Enhanced troubleshooting with GPU-specific issues
- ‚úÖ Updated API keys section (fixed PERSPECTIVE_API_KEY)
- ‚úÖ Added comprehensive GPU optimization tips

### 3. setup_gpu.sh (NEW)
- ‚úÖ Created automated GPU setup script
- ‚úÖ Automatic CUDA version detection
- ‚úÖ Validates GPU availability
- ‚úÖ Installs correct PyTorch version
- ‚úÖ Compiles llama-cpp-python with CUDA
- ‚úÖ Verifies installation
- ‚úÖ Made executable (chmod +x)

### 4. Code (No Changes Needed!)
- ‚úÖ GPU detection already implemented in `src/utils/device_utils.py`
- ‚úÖ Automatic GPU offloading in `src/gne/model_interface.py`
- ‚úÖ CUDA optimizations already configured in config files

## üéì Best Practices

1. **Always use GPU for production runs** - 20-40x faster
2. **Use Q5/Q6 models on GPU** - Better quality at similar speed to CPU Q2/Q3
3. **Monitor GPU memory** - Use `nvidia-smi` to watch usage
4. **Adjust memory_fraction** if you get OOM errors
5. **Keep CUDA drivers updated** - Latest drivers = best performance

## üîó Links

- [CUDA Toolkit Download](https://developer.nvidia.com/cuda-downloads)
- [PyTorch CUDA Installation](https://pytorch.org/get-started/locally/)
- [llama.cpp GPU Support](https://github.com/ggerganov/llama.cpp#cuda)
- [Google Perspective API](https://developers.perspectiveapi.com/s/docs-get-started)

---

**Need help?** Check the Troubleshooting section in README.md or run `bash setup_gpu.sh` for automated setup.

