device_config:
  auto_detect: true
  cpu:
    enable_mkl: true
    f16_kv: false
    gpu_layers: 0
    low_vram: false
    num_threads: null
    use_mlock: false
    use_mmap: true
  cuda:
    enable_cudnn_benchmark: true
    enable_tf32: true
    f16_kv: true
    gpu_layers: -1
    low_vram: false
    memory_fraction: 0.8
    tensor_split: null
    use_mlock: false
    use_mmap: true
  enable_mixed_precision: true
  fallback_to_cpu: true
  mps:
    enable_metal_performance_shaders: true
    f16_kv: true
    gpu_layers: -1
    low_vram: false
    memory_pressure_relief: false
    use_mlock: true
    use_mmap: true
  preferred_device: null
response_generator:
  enable_cpu_fallback: true
  enable_memory_cleanup: true
  enable_model_caching: true
  generation_args:
    max_new_tokens: 2048
    repetition_penalty: 1.0
    temperature: 0.8
    top_k: 40
    top_p: 0.9
  log_failing_prompts: true
  max_memory_usage_gb: 12.0
  model_loading_timeout: 300
  name: models/mistral-7b-instruct-gguf/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf
  prompt_template:
    messages:
    - content: You are an helpful loyal assistant.
      role: system
    - content: '{{prompt}}'
      role: user
  provider: local
  strategy: local
  task_type: response-generation
  use_simulation: false
