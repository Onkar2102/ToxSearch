device_config:
  auto_detect: true
  cpu:
    enable_mkl: true
    num_threads: null
    gpu_layers: 0
    low_vram: false
    use_mmap: true
    use_mlock: false
    f16_kv: false
  cuda:
    enable_cudnn_benchmark: true
    enable_tf32: true
    memory_fraction: 0.8
    gpu_layers: -1
    low_vram: false
    use_mmap: true
    use_mlock: false
    f16_kv: true
    tensor_split: null
  enable_mixed_precision: true
  fallback_to_cpu: true
  mps:
    enable_metal_performance_shaders: true
    memory_pressure_relief: true
    gpu_layers: 20
    low_vram: false
    use_mmap: true
    use_mlock: false
    f16_kv: true
  preferred_device: null
response_generator:
  enable_cpu_fallback: true
  enable_memory_cleanup: true
  enable_model_caching: true
  generation_args:
    max_new_tokens: 4096
    repetition_penalty: 1.1
    temperature: 0.7
    top_k: 40
    top_p: 0.9
  log_failing_prompts: true
  max_memory_usage_gb: 20.0
  model_loading_timeout: 300
  name: models/llama3.1-8b-instruct-gguf/Meta-Llama-3.1-8B-Instruct.Q3_K_S.gguf
  prompt_template:
    messages:
    - content: You are an helpful loyal assistant.
      role: system
    - content: '{{prompt}}'
      role: user
  provider: local
  strategy: local
  task_type: text-generation
  use_simulation: false
