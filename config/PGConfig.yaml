device_config:
  auto_detect: true
  cpu:
    enable_mkl: true
    f16_kv: false
    gpu_layers: 0
    low_vram: false
    num_threads: null
    use_mlock: false
    use_mmap: true
  cuda:
    enable_cudnn_benchmark: true
    enable_tf32: true
    f16_kv: true
    gpu_layers: -1
    low_vram: false
    memory_fraction: 0.8
    tensor_split: null
    use_mlock: false
    use_mmap: true
  enable_mixed_precision: true
  fallback_to_cpu: true
  mps:
    enable_metal_performance_shaders: true
    f16_kv: true
    gpu_layers: -1
    low_vram: false
    memory_pressure_relief: true
    use_mlock: true
    use_mmap: true
  preferred_device: mps
prompt_generator:
  enable_cpu_fallback: true
  enable_memory_cleanup: true
  enable_model_caching: true
  generation_args:
    max_new_tokens: 4096
    repetition_penalty: 1.1
    temperature: 0.9
    top_k: 40
    top_p: 0.9
  log_failing_prompts: true
  max_memory_usage_gb: 20.0
  model_loading_timeout: 300
  name: models/llama3.1-8b-instruct-gguf/Meta-Llama-3.1-8B-Instruct.Q5_K_S.gguf
  provider: local
  strategy: local
  task_type: prompt-generation
  use_simulation: false
