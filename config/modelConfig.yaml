llama:
  generation_args:
    # do_sample: true
    max_new_tokens: 2048
    num_return_sequences: 1
    pad_token_id: 128001
    eos_token_id: 128001
    repetition_penalty: 1.1
    # no_repeat_ngram_size: 3
    # temperature: 0.6
    top_k: 50
    top_p: 0.95
  max_batch_size: 8
  generation_batch_size: 5  # Batch size for population processing during generation
  evaluation_batch_size: 5  # Batch size for population processing during evaluation
  use_simulation: false  # Set to true for testing without real model generation
  padding_side: "left"  # "left" or "right" - some models expect right padding
  log_failing_prompts: true  # Log prompt text when generation fails for debugging
  name: ./models/llama3.2-3b-instruct
  # Alternative smaller model for faster loading and less memory usage
  # name: microsoft/DialoGPT-medium  # Smaller model for testing
  prompt_template:
    assistant_prefix: ''                    # No prefix for instruction-following
    format: '{{prompt}}'                   # Direct instruction format
    style: chat                      # Instruction-following style
    user_prefix: 'Adult User'                        # No user prefix for instructions
  provider: local
  strategy: local
  task_type: text-generation
  # Memory management settings
  enable_memory_cleanup: true  # Enable automatic memory cleanup
  max_memory_usage_gb: 12.0  # Maximum memory usage in GB (increased from 4.0)
  adaptive_batch_sizing: true  # Enable dynamic batch size adjustment based on memory
  min_batch_size: 1  # Minimum batch size when memory is critical
  max_batch_size_memory: 8  # Maximum batch size when memory is good
  # Model loading settings
  model_loading_timeout: 300  # 5 minutes timeout for model loading
  enable_cpu_fallback: true  # Fallback to CPU if GPU loading fails
  enable_model_caching: true  # Cache loaded models in memory
  # Task-specific prompt templates and generation parameters
  task_templates:
    translation:
      en_to_target: |
        System: You are a precise {target_language} translator.
        Rules:
        - Output EXACTLY one tag pair: <trans>...</trans>
        - No explanations, no extra text, no quotes, no markdown.
        - Preserve meaning faithfully; be concise and literal unless idioms require natural phrasing.
        User:
        Source language: English
        Target language: {target_language}
        Text: "{text}"
        Return only: <trans>THE_TRANSLATION</trans>
      target_to_en: |
        System: You are a precise English translator.
        Rules:
        - Output EXACTLY one tag pair: <trans>...</trans>
        - No explanations, no extra text, no quotes, no markdown.
        - Preserve meaning faithfully; be concise and literal unless idioms require natural phrasing.
        User:
        Source language: {source_language}
        Target language: English
        Text: "{text}"
        Return only: <trans>THE_TRANSLATION</trans>

  # Task-specific generation parameters
  task_generation_args:
    translation:
      # do_sample: true        # Deterministic greedy decoding for exact translation
      # temperature: 0.0        # Greedy
      top_k: 50
      top_p: 0.95
      repetition_penalty: 1.1
      max_new_tokens: 2048     # Plenty for sentence/paragraph translation
      num_return_sequences: 1
      # no_repeat_ngram_size: 3

# ---------------------------------------------------------------------------
# Optional EvoTox search profiles (commented out). Enable to switch exploration
# vs. exploitation without touching core defaults. Un-comment and pipe into the
# generator when you want phase-specific sampling.
# profiles:
#   explore:
#     temperature: 1.1
#     top_p: 0.95
#     top_k: 50
#     num_return_sequences: 4
#   exploit:
#     temperature: 0.2
#     top_p: 0.90
#     top_k: 0
#     num_return_sequences: 1
# ---------------------------------------------------------------------------
