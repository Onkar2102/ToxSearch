# Dataset Formation: 696 Unique Elite Prompts

## How the 696 Dataset is Formed

### Collection Process

The dataset is formed by the `main()` function in `experiments/analysis.py` (lines 1726-1801):

1. **Source Elite Collection**: Uses the **same pattern as RQ2**: `run*_comb/elites.json`
   - Only collects from `run*_comb` directories (same as RQ2)
   - **Excludes** `run*_ops` directories

2. **Target Model Data**: Also uses model directories for target model evaluations:
   - Model directories (direct children that don't start with 'run' or '2025')
   - These contain evaluations of the source elites on target models

3. **Consistency with RQ2**: RQ3 uses the exact same source data pattern as RQ2 for collecting source elites

### Processing Steps

1. **Initial Collection**: All elites from all `elites.json` files are collected
2. **Sorting**: Sorted by maximum toxicity score (across all models)
3. **Deduplication**: By prompt text (keeping the elite with highest toxicity per unique prompt)
4. **Question Filtering**: Only keeps prompts ending with "?"
5. **Result**: 696 unique elite prompts

### Source Information

Each elite in the final dataset contains a `source` field with:
- `genome_id`: Original genome ID
- `operator`: Operator that created it
- `dir_name`: Run directory name (e.g., `run1_ops`, `run2_comb`)

### Key Points

- **Comb mode only**: The 696 prompts come from ONLY `run*_comb` runs (not `run*_ops`)
- **Deduplicated**: If the same prompt appears in multiple runs, only the one with highest toxicity is kept
- **Question-only**: Only prompts ending with "?" are included
- **All comb runs included**: Includes all `run*_comb` directories, plus model directories for target model evaluations

### To Verify Breakdown

You can check the `rq3_combined_elites.json` file (generated by the analysis) and examine the `source['dir_name']` field to see which runs contributed each elite prompt.
