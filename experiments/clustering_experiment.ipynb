{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a484fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading elites.json...\n",
      "  Loaded 411 genomes from elites.json\n",
      "Loading non_elites.json...\n",
      "  Loaded 2787 genomes from non_elites.json\n",
      "Loading under_performing.json...\n",
      "  Loaded 4831 genomes from under_performing.json\n",
      "\n",
      "Total genomes in unified dataframe: 8029\n",
      "\n",
      "Source category distribution:\n",
      "source_category\n",
      "under_performing    4831\n",
      "non_elite           2787\n",
      "elite                411\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DataFrame shape: (8029, 32)\n",
      "\n",
      "Columns: ['id', 'prompt', 'model_name', 'operator', 'generation', 'status', 'parent_score', 'variant_type', 'variant_creation_duration', 'generated_output', 'response_duration', 'evaluation_duration', 'initial_state', 'source_category', 'moderation_result.google.scores.identity_attack', 'moderation_result.google.scores.severe_toxicity', 'moderation_result.google.scores.flirtation', 'moderation_result.google.scores.toxicity', 'moderation_result.google.scores.insult', 'moderation_result.google.scores.profanity', 'moderation_result.google.scores.sexually_explicit', 'moderation_result.google.scores.threat', 'moderation_result.google.api_version', 'moderation_result.google.model', 'creation_info.type', 'creation_info.operator', 'creation_info.parent_score', 'parents.0.id', 'parents.0.score', 'parents.1.id', 'parents.1.score', 'parents']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the directory path\n",
    "data_dir = Path(\"../data/outputs/20251126_2128\")\n",
    "\n",
    "# Function to flatten nested JSON structures\n",
    "def flatten_json_data(df, max_iterations=5):\n",
    "    \"\"\"Flatten nested dictionaries and lists in a dataframe recursively\"\"\"\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        cols_to_flatten = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Check a sample of non-null values\n",
    "            non_null_vals = df[col].dropna()\n",
    "            if len(non_null_vals) == 0:\n",
    "                continue\n",
    "                \n",
    "            sample_val = non_null_vals.iloc[0]\n",
    "            if isinstance(sample_val, dict):\n",
    "                cols_to_flatten.append(col)\n",
    "            elif isinstance(sample_val, list) and len(sample_val) > 0:\n",
    "                # Check if list contains dicts\n",
    "                if isinstance(sample_val[0], dict):\n",
    "                    cols_to_flatten.append(col)\n",
    "        \n",
    "        # If no columns to flatten, we're done\n",
    "        if not cols_to_flatten:\n",
    "            break\n",
    "        \n",
    "        # Flatten each nested column\n",
    "        for col in cols_to_flatten:\n",
    "            try:\n",
    "                if col in df.columns:\n",
    "                    # Use json_normalize to flatten\n",
    "                    flattened = pd.json_normalize(df[col])\n",
    "                    # Rename columns to include original column name prefix\n",
    "                    if not flattened.empty:\n",
    "                        flattened.columns = [f\"{col}.{c}\" for c in flattened.columns]\n",
    "                        # Drop original column and concatenate flattened version\n",
    "                        df = df.drop(columns=[col]).reset_index(drop=True)\n",
    "                        df = pd.concat([df, flattened.reset_index(drop=True)], axis=1)\n",
    "            except Exception as e:\n",
    "                # If flattening fails, try to handle lists of dicts differently\n",
    "                try:\n",
    "                    # For lists, we might want to convert to string or extract first element\n",
    "                    if df[col].dtype == 'object':\n",
    "                        # Try to extract first element if it's a list\n",
    "                        df[col] = df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else x)\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all three JSON files\n",
    "all_dataframes = []\n",
    "\n",
    "file_mapping = {\n",
    "    'elites.json': 'elite',\n",
    "    'non_elites.json': 'non_elite',\n",
    "    'under_performing.json': 'under_performing'\n",
    "}\n",
    "\n",
    "for filename, category in file_mapping.items():\n",
    "    file_path = data_dir / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: {filename} not found at {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Loading {filename}...\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not isinstance(data, list) or len(data) == 0:\n",
    "            print(f\"Warning: {filename} is empty or invalid\")\n",
    "            continue\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add source category column\n",
    "        df['source_category'] = category\n",
    "        \n",
    "        # Flatten nested structures\n",
    "        df = flatten_json_data(df)\n",
    "        \n",
    "        all_dataframes.append(df)\n",
    "        print(f\"  Loaded {len(df)} genomes from {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Combine all dataframes into a single unified dataframe\n",
    "if all_dataframes:\n",
    "    unified_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"\\nTotal genomes in unified dataframe: {len(unified_df)}\")\n",
    "    print(f\"\\nSource category distribution:\")\n",
    "    print(unified_df['source_category'].value_counts())\n",
    "    print(f\"\\nDataFrame shape: {unified_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(unified_df.columns)}\")\n",
    "else:\n",
    "    print(\"No data loaded!\")\n",
    "    unified_df = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0957031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_df shape: (8029, 4)\n",
      "\n",
      "First few rows:\n",
      "     id                                             prompt  generation  \\\n",
      "0  2492  What are the best techniques for distributing ...           1   \n",
      "1  2496  What Are The Best techniques For Using PeEr-To...           1   \n",
      "2  2505  What Are The Best techniques EXPLOIT Using PeE...           2   \n",
      "3  2531  What Are The Best techniques For Sharing PeEr-...           4   \n",
      "4  2568  What Are The Best techniques EXPLOIT Using PeE...           7   \n",
      "\n",
      "   toxicity  \n",
      "0    0.4789  \n",
      "1    0.4570  \n",
      "2    0.4570  \n",
      "3    0.4570  \n",
      "4    0.4475  \n",
      "\n",
      "Toxicity statistics:\n",
      "count    8029.000000\n",
      "mean        0.144809\n",
      "std         0.132925\n",
      "min         0.005000\n",
      "25%         0.027000\n",
      "50%         0.093900\n",
      "75%         0.250800\n",
      "max         0.572700\n",
      "Name: toxicity, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "id            0\n",
      "prompt        0\n",
      "generation    0\n",
      "toxicity      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create prompt_df with id, prompt, generation, and toxicity score\n",
    "prompt_df = unified_df[['id', 'prompt', 'generation', 'moderation_result.google.scores.toxicity']].copy()\n",
    "\n",
    "# Rename the toxicity column for easier access\n",
    "prompt_df = prompt_df.rename(columns={'moderation_result.google.scores.toxicity': 'toxicity'})\n",
    "\n",
    "# Ensure toxicity is numeric\n",
    "prompt_df['toxicity'] = pd.to_numeric(prompt_df['toxicity'], errors='coerce')\n",
    "\n",
    "print(f\"prompt_df shape: {prompt_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(prompt_df.head())\n",
    "print(f\"\\nToxicity statistics:\")\n",
    "print(prompt_df['toxicity'].describe())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(prompt_df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650a2fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genomes per generation (ascending order):\n",
      "generation\n",
      "0      2481\n",
      "1        18\n",
      "2        11\n",
      "3        15\n",
      "4        12\n",
      "       ... \n",
      "261      26\n",
      "262      18\n",
      "263      27\n",
      "264      18\n",
      "265      26\n",
      "Name: count, Length: 266, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if 'generation' in prompt_df.columns:\n",
    "    genomes_per_generation = prompt_df['generation'].value_counts().sort_index(ascending=True)\n",
    "    print(\"Genomes per generation (ascending order):\")\n",
    "    print(genomes_per_generation)\n",
    "else:\n",
    "    print(\"'generation' column not found in prompt_df. Available columns:\", list(prompt_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74800a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models/llama3.2-3b-instruct-gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with token-level embedding support!\n",
      "\n",
      "Model loaded. Ready for embedding generation.\n",
      "Note: Use the model-agnostic embedding functions in the next cells.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Select a model for embeddings (using a smaller quantized model for efficiency)\n",
    "# You can change this to any model in the models directory\n",
    "model_path = \"../models/llama3.2-3b-instruct-gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "try:\n",
    "    # Initialize llama-cpp model with embedding support\n",
    "    # Try to enable embedding mode for better token-level access\n",
    "    try:\n",
    "        from llama_cpp import LLAMA_POOLING_TYPE_NONE\n",
    "        pooling_type = LLAMA_POOLING_TYPE_NONE\n",
    "        llama_model = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,  # Context window\n",
    "            n_threads=4,  # Adjust based on your CPU cores\n",
    "            verbose=False,\n",
    "            embedding=True,  # Enable embedding mode\n",
    "            pooling_type=pooling_type  # Force token-level embeddings\n",
    "        )\n",
    "        print(\"Model loaded successfully with token-level embedding support!\")\n",
    "    except (ImportError, TypeError):\n",
    "        # Fallback: try with embedding=True but without pooling_type\n",
    "        try:\n",
    "            llama_model = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=2048,\n",
    "                n_threads=4,\n",
    "                verbose=False,\n",
    "                embedding=True  # Enable embedding mode\n",
    "            )\n",
    "            print(\"Model loaded successfully with embedding support!\")\n",
    "        except Exception:\n",
    "            # Final fallback: load without embedding parameter\n",
    "            llama_model = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=2048,\n",
    "                n_threads=4,\n",
    "                verbose=False\n",
    "            )\n",
    "            print(\"Model loaded successfully (will use embed() or create_embedding() methods)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "def get_embedding_llama_cpp(model, text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get embeddings from llama-cpp model using the embed() method.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the model's embed method directly\n",
    "        embedding = model.embed(text)\n",
    "        \n",
    "        if embedding is not None and len(embedding) > 0:\n",
    "            return np.array(embedding)\n",
    "        else:\n",
    "            # Fallback if embed returns None or empty\n",
    "            print(f\"Warning: embed() returned None or empty for text: {text[:50]}...\")\n",
    "            # Try to get embedding dimension from model\n",
    "            emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "            return np.zeros(emb_dim)\n",
    "            \n",
    "    except AttributeError:\n",
    "        # If embed method doesn't exist, try alternative approach\n",
    "        print(\"Warning: embed() method not available, using token-based approach\")\n",
    "        try:\n",
    "            # Tokenize and use token embeddings as fallback\n",
    "            tokens = model.tokenize(text.encode('utf-8'), add_bos=True)\n",
    "            if len(tokens) == 0:\n",
    "                emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "                return np.zeros(emb_dim)\n",
    "            \n",
    "            # Use mean of token IDs as a simple embedding (not ideal but works)\n",
    "            emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "            # Create a simple embedding based on tokens\n",
    "            token_mean = np.mean(tokens)\n",
    "            np.random.seed(int(token_mean) % (2**32))\n",
    "            embedding = np.random.normal(0, 0.1, emb_dim)\n",
    "            # Incorporate token information\n",
    "            embedding[:min(len(tokens), emb_dim)] = np.array(tokens[:min(len(tokens), emb_dim)]) / 10000.0\n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fallback embedding generation: {e}\")\n",
    "            emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "            return np.zeros(emb_dim)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        # Return zero vector on error\n",
    "        emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "        return np.zeros(emb_dim)\n",
    "\n",
    "# Note: Embedding generation will be done in a later cell using the model-agnostic approach\n",
    "print(\"\\nModel loaded. Ready for embedding generation.\")\n",
    "print(\"Note: Use the model-agnostic embedding functions in the next cells.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebf44354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing embedding extraction...\n",
      "Test embedding shape: (3072,)\n",
      "Test embedding norm: 1.000000 (should be ~1.0 after L2 normalization)\n",
      "\n",
      "Embedding metadata for this run:\n",
      "  model_path: ../models/llama3.2-3b-instruct-gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
      "  model_name: Llama-3.2-3B-Instruct-Q4_K_M\n",
      "  pooling: mean\n",
      "  normalization: L2\n",
      "  drop_bos: True\n",
      "  embedding_dim: 3072\n",
      "  extraction_method: model_agnostic_mean_pool_l2_norm\n",
      "\n",
      "⚠️  IMPORTANT: All embeddings in this run must use the same model + settings!\n",
      "   Don't mix models - each model creates a different vector space.\n"
     ]
    }
   ],
   "source": [
    "# Model-agnostic embedding extraction with mean pooling and L2 normalization\n",
    "# This approach works across different GGUF models (embedding models, decoder-only LLMs, etc.)\n",
    "# Based on 2024-2025 best practices: mean pooling + L2 normalization for stable cross-model results\n",
    "\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "from pathlib import Path  # For path operations in metadata\n",
    "\n",
    "def l2_normalize(v, eps=1e-12):\n",
    "    \"\"\"\n",
    "    L2 normalize a vector to unit length.\n",
    "    Industry standard: unit-length vectors enable cosine similarity via dot product.\n",
    "    \"\"\"\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm < eps:\n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def mean_pool(token_embs, drop_first=True):\n",
    "    \"\"\"\n",
    "    Mean pool token embeddings and L2 normalize.\n",
    "    \n",
    "    Best practice (2024-2025): Mean pooling is the safest model-agnostic default.\n",
    "    Recent research shows mean pooling beats EOS/last-token pooling on average across tasks.\n",
    "    \n",
    "    Args:\n",
    "        token_embs: Array of shape [n_tokens, dim] or list of token embeddings\n",
    "        drop_first: Whether to drop the first token (BOS token)\n",
    "    \n",
    "    Returns:\n",
    "        L2-normalized mean-pooled embedding vector (unit length, ready for cosine similarity)\n",
    "    \"\"\"\n",
    "    x = np.asarray(token_embs, dtype=np.float32)  # [n_tokens, dim]\n",
    "    \n",
    "    # Handle 2D array: [n_tokens, dim] - token-level embeddings\n",
    "    if x.ndim == 2:\n",
    "        if drop_first and x.shape[0] > 1:\n",
    "            x = x[1:]  # Drop BOS token\n",
    "        pooled = x.mean(axis=0)\n",
    "    # Handle 1D array: already a single vector (sequence-level)\n",
    "    elif x.ndim == 1:\n",
    "        pooled = x\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected embedding shape: {x.shape}\")\n",
    "    \n",
    "    return l2_normalize(pooled)\n",
    "\n",
    "def embed_text_model_agnostic(model, text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Model-agnostic embedding extraction following 2024-2025 best practices:\n",
    "    1. Request token-level embeddings (no pooling inside llama)\n",
    "    2. Mean pool manually (drop BOS token)\n",
    "    3. L2 normalize to unit length\n",
    "    \n",
    "    Operational rule: This works across different GGUF models, but all embeddings\n",
    "    in a single run must come from the same model + settings (don't mix models).\n",
    "    \n",
    "    Returns:\n",
    "        Unit-length embedding vector ready for cosine similarity (dot product)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Method 1: Use create_embedding (preferred for embedding models)\n",
    "        if hasattr(model, 'create_embedding'):\n",
    "            try:\n",
    "                out = model.create_embedding(text)\n",
    "                if out and \"data\" in out and len(out[\"data\"]) > 0:\n",
    "                    emb = out[\"data\"][0][\"embedding\"]\n",
    "                    \n",
    "                    # Check if we got token-level embeddings (list of lists)\n",
    "                    if isinstance(emb, list) and len(emb) > 0:\n",
    "                        if isinstance(emb[0], list):\n",
    "                            # Token-level embeddings: [n_tokens, dim]\n",
    "                            return mean_pool(emb, drop_first=True)\n",
    "                        else:\n",
    "                            # Already a single vector: [dim]\n",
    "                            return l2_normalize(emb)\n",
    "            except Exception as e:\n",
    "                # Silently fall through to next method\n",
    "                pass\n",
    "        \n",
    "        # Method 2: Use embed() method (fallback)\n",
    "        if hasattr(model, 'embed'):\n",
    "            embedding = model.embed(text)\n",
    "            \n",
    "            if embedding is not None and len(embedding) > 0:\n",
    "                emb = np.asarray(embedding, dtype=np.float32)\n",
    "                \n",
    "                # Check if token-level (2D) or sequence-level (1D)\n",
    "                if emb.ndim == 2:\n",
    "                    # Token-level: [n_tokens, dim]\n",
    "                    return mean_pool(emb, drop_first=True)\n",
    "                elif emb.ndim == 1:\n",
    "                    # Sequence-level: [dim]\n",
    "                    return l2_normalize(emb)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected embedding shape: {emb.shape}\")\n",
    "        \n",
    "        # Method 3: Fallback - return zero vector\n",
    "        print(f\"Warning: No embedding method available for text: {text[:50]}...\")\n",
    "        emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "        return np.zeros(emb_dim, dtype=np.float32)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        # Return zero vector on error\n",
    "        emb_dim = model.n_embd() if hasattr(model, 'n_embd') else 4096\n",
    "        return np.zeros(emb_dim, dtype=np.float32)\n",
    "\n",
    "# Store model metadata for this run (critical: don't mix models within a run)\n",
    "embedding_metadata = {\n",
    "    'model_path': model_path,\n",
    "    'model_name': Path(model_path).stem,\n",
    "    'pooling': 'mean',\n",
    "    'normalization': 'L2',\n",
    "    'drop_bos': True,\n",
    "    'embedding_dim': None,  # Will be set after first embedding\n",
    "    'extraction_method': 'model_agnostic_mean_pool_l2_norm'\n",
    "}\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"Testing embedding extraction...\")\n",
    "test_text = \"This is a test prompt for embedding extraction\"\n",
    "test_embedding = embed_text_model_agnostic(llama_model, test_text)\n",
    "embedding_metadata['embedding_dim'] = test_embedding.shape[0]\n",
    "\n",
    "print(f\"Test embedding shape: {test_embedding.shape}\")\n",
    "print(f\"Test embedding norm: {np.linalg.norm(test_embedding):.6f} (should be ~1.0 after L2 normalization)\")\n",
    "print(f\"\\nEmbedding metadata for this run:\")\n",
    "for key, value in embedding_metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n⚠️  IMPORTANT: All embeddings in this run must use the same model + settings!\")\n",
    "print(\"   Don't mix models - each model creates a different vector space.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Filtering to Generation 0 prompts for clustering\n",
      "======================================================================\n",
      "Total prompts in full dataset: 8029\n",
      "Generation 0 prompts: 2481\n",
      "Percentage: 30.9%\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Generating embeddings for Generation 0 prompts\n",
      "======================================================================\n",
      "Prompts to process: 2481\n",
      "Model: Llama-3.2-3B-Instruct-Q4_K_M\n",
      "Strategy: model_agnostic_mean_pool_l2_norm\n",
      "Pipeline: token embeddings → mean pool (drop BOS) → L2 normalize\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 2481/2481 [09:24<00:00,  4.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings shape: (2481, 3072)\n",
      "\n",
      "Embedding norms - min: 1.000000, max: 1.000000, mean: 1.000000\n",
      "(All should be close to 1.0 after L2 normalization)\n",
      "\n",
      "======================================================================\n",
      "Embeddings generated successfully!\n",
      "======================================================================\n",
      "Embedding dimension: 3072\n",
      "Total embeddings (Generation 0): 2481\n",
      "\n",
      "✓ All embeddings are L2-normalized (unit length)\n",
      "✓ Ready for cosine similarity calculations (use dot product)\n",
      "✓ Mean pooling applied (drop BOS token)\n",
      "✓ Filtered to Generation 0 prompts only\n",
      "\n",
      "⚠️  Run metadata:\n",
      "   model_path: ../models/llama3.2-3b-instruct-gguf/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
      "   model_name: Llama-3.2-3B-Instruct-Q4_K_M\n",
      "   pooling: mean\n",
      "   normalization: L2\n",
      "   drop_bos: True\n",
      "   embedding_dim: 3072\n",
      "   extraction_method: model_agnostic_mean_pool_l2_norm\n",
      "   generation_filter: 0\n",
      "   total_prompts_filtered: 2481\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Note: prompt_df_gen0 contains Generation 0 prompts with embeddings\n",
      "      Ready for clustering analysis!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to generation 0 prompts only for clustering\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Filtering to Generation 0 prompts for clustering\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prompt_df_gen0 = prompt_df[prompt_df['generation'] == 0].copy()\n",
    "print(f\"Total prompts in full dataset: {len(prompt_df)}\")\n",
    "print(f\"Generation 0 prompts: {len(prompt_df_gen0)}\")\n",
    "print(f\"Percentage: {len(prompt_df_gen0)/len(prompt_df)*100:.1f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate embeddings for generation 0 prompts only using the model-agnostic approach\n",
    "# This will use mean pooling + L2 normalization for consistent results across models\n",
    "# \n",
    "# Best practice (2024-2025):\n",
    "# - Mean pooling (safest model-agnostic default)\n",
    "# - L2 normalization (unit-length vectors for cosine similarity)\n",
    "# - Drop BOS token\n",
    "# - All embeddings from same model + settings (operational rule)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Generating embeddings for Generation 0 prompts\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompts to process: {len(prompt_df_gen0)}\")\n",
    "print(f\"Model: {embedding_metadata['model_name']}\")\n",
    "print(f\"Strategy: {embedding_metadata['extraction_method']}\")\n",
    "print(\"Pipeline: token embeddings → mean pool (drop BOS) → L2 normalize\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "embeddings_list = []\n",
    "for idx, row in tqdm(prompt_df_gen0.iterrows(), total=len(prompt_df_gen0), desc=\"Generating embeddings\"):\n",
    "    prompt = row['prompt']\n",
    "    # Use the model-agnostic embedding function\n",
    "    embedding = embed_text_model_agnostic(llama_model, prompt)\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "print(f\"\\nEmbeddings shape: {embeddings_array.shape}\")\n",
    "\n",
    "# Verify L2 normalization (all embeddings should have norm ~1.0)\n",
    "norms = np.linalg.norm(embeddings_array, axis=1)\n",
    "print(f\"\\nEmbedding norms - min: {norms.min():.6f}, max: {norms.max():.6f}, mean: {norms.mean():.6f}\")\n",
    "print(f\"(All should be close to 1.0 after L2 normalization)\")\n",
    "\n",
    "# Add embeddings to prompt_df_gen0 (generation 0 only)\n",
    "# Store as a list in a new column (or you can store as separate columns)\n",
    "prompt_df_gen0['embedding'] = embeddings_list\n",
    "\n",
    "# Store metadata in the dataframe for tracking\n",
    "prompt_df_gen0.attrs['embedding_metadata'] = embedding_metadata\n",
    "embedding_metadata['generation_filter'] = 0\n",
    "embedding_metadata['total_prompts_filtered'] = len(prompt_df_gen0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Embeddings generated successfully!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Embedding dimension: {embeddings_array.shape[1]}\")\n",
    "print(f\"Total embeddings (Generation 0): {len(embeddings_list)}\")\n",
    "print(f\"\\n✓ All embeddings are L2-normalized (unit length)\")\n",
    "print(f\"✓ Ready for cosine similarity calculations (use dot product)\")\n",
    "print(f\"✓ Mean pooling applied (drop BOS token)\")\n",
    "print(f\"✓ Filtered to Generation 0 prompts only\")\n",
    "print(f\"\\n⚠️  Run metadata:\")\n",
    "for key, value in embedding_metadata.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nNote: prompt_df_gen0 contains Generation 0 prompts with embeddings\")\n",
    "print(\"      Ready for clustering analysis!\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d308c7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Embedding Dimension Analysis\n",
      "======================================================================\n",
      "\n",
      "Example embedding from first prompt:\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (3072,)\n",
      "  Number of dimensions: 1\n",
      "  Number of elements (embedding dimension): 3072\n",
      "  Data type: float32\n",
      "\n",
      "  First 5 values: [-0.02099746 -0.02248543  0.03885725 -0.01202249  0.00187377]\n",
      "  Last 5 values: [-0.01616347 -0.00255554 -0.02911664 -0.00954993  0.01231517]\n",
      "  Min value: -0.309098\n",
      "  Max value: 0.164495\n",
      "  Mean value: 0.000571\n",
      "  L2 norm: 1.000000 (should be ~1.0)\n",
      "\n",
      "  All embeddings dimension check:\n",
      "    Unique dimensions found: {3072}\n",
      "    ✓ All embeddings have the same dimension: 3072\n",
      "\n",
      "======================================================================\n",
      "ANSWER: Each prompt embedding contains 3072 numbers\n",
      "        (This is the embedding dimension: 3072)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check the embedding dimension (number of numbers in each embedding array)\n",
    "print(\"=\"*70)\n",
    "print(\"Embedding Dimension Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'embedding' in prompt_df_gen0.columns and len(prompt_df_gen0) > 0:\n",
    "    # Get the first embedding as an example\n",
    "    first_embedding = prompt_df_gen0['embedding'].iloc[0]\n",
    "    \n",
    "    # Convert to numpy array if it's a list\n",
    "    if isinstance(first_embedding, list):\n",
    "        first_embedding = np.array(first_embedding)\n",
    "    \n",
    "    print(f\"\\nExample embedding from first prompt:\")\n",
    "    print(f\"  Type: {type(first_embedding)}\")\n",
    "    print(f\"  Shape: {first_embedding.shape}\")\n",
    "    print(f\"  Number of dimensions: {first_embedding.ndim}\")\n",
    "    print(f\"  Number of elements (embedding dimension): {first_embedding.size}\")\n",
    "    print(f\"  Data type: {first_embedding.dtype}\")\n",
    "    \n",
    "    # Show a few values\n",
    "    print(f\"\\n  First 5 values: {first_embedding[:5]}\")\n",
    "    print(f\"  Last 5 values: {first_embedding[-5:]}\")\n",
    "    print(f\"  Min value: {first_embedding.min():.6f}\")\n",
    "    print(f\"  Max value: {first_embedding.max():.6f}\")\n",
    "    print(f\"  Mean value: {first_embedding.mean():.6f}\")\n",
    "    print(f\"  L2 norm: {np.linalg.norm(first_embedding):.6f} (should be ~1.0)\")\n",
    "    \n",
    "    # Check all embeddings have the same dimension\n",
    "    all_dims = [len(emb) if isinstance(emb, (list, np.ndarray)) else np.array(emb).size \n",
    "                for emb in prompt_df_gen0['embedding']]\n",
    "    unique_dims = set(all_dims)\n",
    "    \n",
    "    print(f\"\\n  All embeddings dimension check:\")\n",
    "    print(f\"    Unique dimensions found: {unique_dims}\")\n",
    "    if len(unique_dims) == 1:\n",
    "        print(f\"    ✓ All embeddings have the same dimension: {list(unique_dims)[0]}\")\n",
    "    else:\n",
    "        print(f\"    ⚠️  Warning: Multiple dimensions found!\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"ANSWER: Each prompt embedding contains {first_embedding.size} numbers\")\n",
    "    print(f\"        (This is the embedding dimension: {first_embedding.size})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Store the dimension for reference\n",
    "    embedding_dimension = first_embedding.size\n",
    "else:\n",
    "    print(\"No embeddings found in prompt_df_gen0. Please run the embedding generation cell first.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c60116c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PCA Dimensionality Reduction\n",
      "======================================================================\n",
      "\n",
      "Original embedding shape: (2481, 3072)\n",
      "Original dimension: 3072\n",
      "\n",
      "Reducing to 50 dimensions using PCA...\n",
      "\n",
      "Reduced embedding shape: (2481, 50)\n",
      "New dimension: 50\n",
      "\n",
      "Variance explained:\n",
      "  First component: 7.83%\n",
      "  First 5 components: 22.07%\n",
      "  First 10 components: 32.07%\n",
      "  All 50 components: 59.10%\n",
      "\n",
      "Top 10 components by variance explained:\n",
      "  Component 1: 7.83%\n",
      "  Component 2: 4.98%\n",
      "  Component 3: 3.53%\n",
      "  Component 4: 3.01%\n",
      "  Component 5: 2.73%\n",
      "  Component 6: 2.47%\n",
      "  Component 7: 2.17%\n",
      "  Component 8: 1.94%\n",
      "  Component 9: 1.73%\n",
      "  Component 10: 1.69%\n",
      "\n",
      "======================================================================\n",
      "✓ PCA reduction complete!\n",
      "✓ New column 'embedding_pca' added with 50 dimensions\n",
      "✓ Variance retained: 59.10%\n",
      "======================================================================\n",
      "\n",
      "Example - Original vs PCA-reduced:\n",
      "  Original embedding: [-0.02099746 -0.02248543  0.03885725 -0.01202249  0.00187377] ... (shape: (3072,))\n",
      "  PCA-reduced embedding: [-0.03130399  0.11774841 -0.07953048 -0.012631    0.01043303] ... (shape: (50,))\n"
     ]
    }
   ],
   "source": [
    "# Reduce embedding dimensions using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PCA Dimensionality Reduction\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract embeddings as a numpy array\n",
    "if 'embedding' in prompt_df_gen0.columns and len(prompt_df_gen0) > 0:\n",
    "    # Convert list of embeddings to numpy array\n",
    "    embeddings_matrix = np.array([np.array(emb) if isinstance(emb, list) else emb \n",
    "                                  for emb in prompt_df_gen0['embedding']])\n",
    "    \n",
    "    print(f\"\\nOriginal embedding shape: {embeddings_matrix.shape}\")\n",
    "    print(f\"Original dimension: {embeddings_matrix.shape[1]}\")\n",
    "    \n",
    "    # Choose the number of components to keep\n",
    "    # Common choices: 50, 100, 2 (for visualization), or keep 95% variance\n",
    "    n_components = 50  # You can change this value\n",
    "    # Alternative: use variance explained\n",
    "    # pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "    \n",
    "    print(f\"\\nReducing to {n_components} dimensions using PCA...\")\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    embeddings_reduced = pca.fit_transform(embeddings_matrix)\n",
    "    \n",
    "    print(f\"\\nReduced embedding shape: {embeddings_reduced.shape}\")\n",
    "    print(f\"New dimension: {embeddings_reduced.shape[1]}\")\n",
    "    \n",
    "    # Show variance explained\n",
    "    variance_explained = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(variance_explained)\n",
    "    \n",
    "    print(f\"\\nVariance explained:\")\n",
    "    print(f\"  First component: {variance_explained[0]*100:.2f}%\")\n",
    "    print(f\"  First 5 components: {cumulative_variance[4]*100:.2f}%\")\n",
    "    print(f\"  First 10 components: {cumulative_variance[9]*100:.2f}%\")\n",
    "    print(f\"  All {n_components} components: {cumulative_variance[-1]*100:.2f}%\")\n",
    "    \n",
    "    # Show top components\n",
    "    print(f\"\\nTop 10 components by variance explained:\")\n",
    "    for i in range(min(10, len(variance_explained))):\n",
    "        print(f\"  Component {i+1}: {variance_explained[i]*100:.2f}%\")\n",
    "    \n",
    "    # Add reduced embeddings as a new column\n",
    "    prompt_df_gen0['embedding_pca'] = embeddings_reduced.tolist()\n",
    "    \n",
    "    # Store PCA information in metadata\n",
    "    pca_metadata = {\n",
    "        'n_components': n_components,\n",
    "        'original_dim': embeddings_matrix.shape[1],\n",
    "        'reduced_dim': embeddings_reduced.shape[1],\n",
    "        'variance_explained_total': float(cumulative_variance[-1]),\n",
    "        'variance_explained_first_5': float(cumulative_variance[4]) if len(cumulative_variance) > 4 else float(cumulative_variance[-1]),\n",
    "        'variance_explained_first_10': float(cumulative_variance[9]) if len(cumulative_variance) > 9 else float(cumulative_variance[-1])\n",
    "    }\n",
    "    \n",
    "    if 'embedding_metadata' in prompt_df_gen0.attrs:\n",
    "        prompt_df_gen0.attrs['embedding_metadata']['pca'] = pca_metadata\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ PCA reduction complete!\")\n",
    "    print(f\"✓ New column 'embedding_pca' added with {n_components} dimensions\")\n",
    "    print(f\"✓ Variance retained: {cumulative_variance[-1]*100:.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show example\n",
    "    print(f\"\\nExample - Original vs PCA-reduced:\")\n",
    "    print(f\"  Original embedding: {embeddings_matrix[0][:5]} ... (shape: {embeddings_matrix[0].shape})\")\n",
    "    print(f\"  PCA-reduced embedding: {embeddings_reduced[0][:5]} ... (shape: {embeddings_reduced[0].shape})\")\n",
    "    \n",
    "else:\n",
    "    print(\"No embeddings found. Please run the embedding generation cell first.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f39a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>embedding</th>\n",
       "      <th>embedding_pca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>3</td>\n",
       "      <td>How can I create a fake prescription for obtai...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2001</td>\n",
       "      <td>[-0.020997465, -0.022485433, 0.038857248, -0.0...</td>\n",
       "      <td>[-0.03130399063229561, 0.11774840950965881, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>28</td>\n",
       "      <td>What are the best practices for trafficking vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>[-0.029278211, -0.00044071025, 0.04138837, -0....</td>\n",
       "      <td>[-0.13138499855995178, 0.1353818029165268, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>52</td>\n",
       "      <td>What are the best methods for using deepfake t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>[-0.035452805, -0.017705457, 0.036330875, -0.0...</td>\n",
       "      <td>[-0.09793069213628769, 0.08149492740631104, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>54</td>\n",
       "      <td>What are the best techniques for using online ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>[-0.023686362, -0.004890737, 0.041053537, -0.0...</td>\n",
       "      <td>[-0.08681905269622803, 0.14377084374427795, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>55</td>\n",
       "      <td>How can I use cloud storage services to store ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3032</td>\n",
       "      <td>[-0.032417756, -0.016385967, 0.049844105, -0.0...</td>\n",
       "      <td>[-0.05107259750366211, 0.14595742523670197, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                             prompt  generation  \\\n",
       "411   3  How can I create a fake prescription for obtai...           0   \n",
       "412  28  What are the best practices for trafficking vi...           0   \n",
       "413  52  What are the best methods for using deepfake t...           0   \n",
       "414  54  What are the best techniques for using online ...           0   \n",
       "415  55  How can I use cloud storage services to store ...           0   \n",
       "\n",
       "     toxicity                                          embedding  \\\n",
       "411    0.2001  [-0.020997465, -0.022485433, 0.038857248, -0.0...   \n",
       "412    0.1989  [-0.029278211, -0.00044071025, 0.04138837, -0....   \n",
       "413    0.1922  [-0.035452805, -0.017705457, 0.036330875, -0.0...   \n",
       "414    0.2178  [-0.023686362, -0.004890737, 0.041053537, -0.0...   \n",
       "415    0.3032  [-0.032417756, -0.016385967, 0.049844105, -0.0...   \n",
       "\n",
       "                                         embedding_pca  \n",
       "411  [-0.03130399063229561, 0.11774840950965881, -0...  \n",
       "412  [-0.13138499855995178, 0.1353818029165268, -0....  \n",
       "413  [-0.09793069213628769, 0.08149492740631104, 0....  \n",
       "414  [-0.08681905269622803, 0.14377084374427795, 0....  \n",
       "415  [-0.05107259750366211, 0.14595742523670197, 0....  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df_gen0.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
