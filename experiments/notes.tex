%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Speciation for Evolutionary Search for Toxicity in LLMs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Onkar Shelar}
\email{os9660@rit.edu}
\orcid{0009-0005-5109-6641}
\affiliation{%
  \institution{Rochester Institute of Technology}
  \city{Rochester}
  \state{New York}
  \country{USA}
}

\author{Travis Desell}
\email{tjdvse@rit.edu}
\orcid{0000-0002-4082-0439}
\affiliation{%
  \institution{Rochester Institute of Technology}
  \city{Rochester}
  \state{New York}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Shelar et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  \it{i will complete the abstract once the results are completed and finalized}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Use, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{figures/sampleteaser.pdf}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Approach}

\subsection{Semantic Compression via Undercomplete Autoencoder}

\textbf{Architecture}: $384 \to 256 \to 128 \to 64 \to 16$ (encoder), symmetric decoder

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Dimensionality Reduction}: L2-normalized embeddings compressed to $d=16$ latent space
    \item \textbf{Input Normalization}: LayerNorm on input embeddings for numerical stability
    \item \textbf{Activation}: GELU throughout (more stable than ReLU for geometry preservation)
    \item \textbf{Weight Initialization}: Xavier uniform initialization for stable training
\end{itemize}

\textbf{Hypothesis}: Latent space $\mathcal{Z} \in \mathbb{R}^{16}$ captures semantic patterns while preserving local structure for clustering.

\subsection{Loss Function Design}

The total loss combines reconstruction, geometric, and structural objectives:
\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{recon}} + \mathcal{L}_{\text{geom}} + \mathcal{L}_{\text{struct}}
\end{equation}

\textbf{Reconstruction Loss} (50\% MSE + 50\% Cosine Similarity):
\begin{equation}
\mathcal{L}_{\text{recon}} = 0.5 \|\mathbf{y} - \hat{\mathbf{y}}\|_2^2 + 0.5(1 - \cos(\mathbf{y}, \hat{\mathbf{y}}))
\end{equation}
Combined objective preserves both pointwise and angular distances.

\textbf{Geometry Loss} (kNN-Edge Distance Matching, primary local loss):
\begin{equation}
\mathcal{L}_{\text{kNN}} = \frac{\beta_{\text{knn}}}{|E|} \sum_{(i,j) \in E} (d_{\mathcal{Z}}(z_i, z_j) - d_{\mathcal{X}}(x_i, x_j))^2
\end{equation}
where $\beta_{\text{knn}} = 0.15$ weights local topology preservation for $k=20$ neighbors. This ensures kNN structure transfers from embedding to latent space.

\textbf{Clustering-Aware Losses} (Triplet + Contrastive):
\begin{itemize}
    \item \textbf{Triplet}: $\beta_{\text{triplet}} = 0.08$, margin $= 0.3$ with hard negative mining
    \item \textbf{Contrastive}: $\beta_{\text{contrastive}} = 0.05$, temperature $\tau = 0.1$
\end{itemize}
Both enhance separation for downstream clustering.

\textbf{Regularization}:
\begin{itemize}
    \item Latent L2 penalty: $\gamma_{\text{latent}} = 10^{-4} \mathbb{E}_{\mathcal{Z}}[\|\mathbf{z}\|_2^2]$
    \item Gradient clipping (max\_norm = 1.0) prevents training instabilities
\end{itemize}

\subsection{Training Protocol}

\begin{itemize}
    \item \textbf{Dataset}: Generation 0 only ($n \approx 100$ prompts)
    \item \textbf{Train/Val Split}: 80/20 stratified
    \item \textbf{Optimizer}: Adam ($\text{lr}=0.001$, $\beta_1=0.9$, $\beta_2=0.999$, $\lambda_{L2}=10^{-5}$)
    \item \textbf{LR Schedule}: ReduceLROnPlateau (factor=0.5, patience=5, min\_lr=$10^{-6}$)
    \item \textbf{Early Stopping}: patience=15, monitored on validation loss
    \item \textbf{Denoising}: Gaussian noise ($\sigma=0.01$) during training
    \item \textbf{Epochs}: 200 (typical convergence $\approx 50$--80 epochs)
\end{itemize}

\textbf{``Train Once, Load Forever'' Protocol}: Pre-trained weights cached with architecture compatibility verified via metadata (schema version 3.3).

\section{What Was Tried}

\subsection{Successful Approaches}

\subsubsection{Autoencoders with Hybrid Loss Functions}

Combining MSE + Cosine similarity improves reconstruction quality. Geometry-aware losses achieve:
\begin{itemize}
    \item Distance correlation $r > 0.85$ (strong geometry preservation)
    \item kNN preservation (k=20): IoU $> 0.5$ (good neighborhood preservation)
    \item Reconstruction MSE $< 0.01$, cosine similarity $> 0.95$
\end{itemize}

\subsubsection{Undercomplete Architecture}

$d=16$ bottleneck reduces dimensionality 24$\times$ ($384 \to 16$) while maintaining separability. This is the optimal balance:
\begin{itemize}
    \item $d=8$: Too aggressive, loses some separation
    \item $d=32$: Less compression, minimal benefit
    \item $d=16$: \textbf{Optimal} for HDBSCAN/centroid clustering
\end{itemize}

\subsubsection{Latent Space Properties}

\begin{itemize}
    \item Mean $\mu_z \approx 0$, Std $\sigma_z \approx 0.15$ (stable distribution)
    \item Pairwise distances preserve embedding topology reasonably well
    \item Suitable for both Euclidean-based clustering and centroid methods
\end{itemize}

\subsection{Attempted but Problematic Approaches}

\subsubsection{Batch Normalization}

Old architecture used BatchNorm, which introduced:
\begin{itemize}
    \item Dependency on batch statistics during training
    \item Inference-time behavior differs from training (BN evaluation modes)
    \item Incompatibility when batch size varies
\end{itemize}
\textbf{Resolution}: Replaced with LayerNorm (fixed instance normalization).

\subsubsection{Overfitting to Reconstruction}

Initially focusing only on MSE led to poor clustering performance. Pure reconstruction doesn't preserve semantic structure needed for speciation.
\textbf{Resolution}: Added geometry (kNN-edge, triplet) and contrastive losses.

\subsubsection{Higher Latent Dimensions}

\begin{itemize}
    \item $d=64$: Minimal benefit over $d=16$, worse compression
    \item $d=32$: Marginal improvement, not worth 2$\times$ computational cost
\end{itemize}
\textbf{Resolution}: Settled on $d=16$ as optimal trade-off.

\subsubsection{Direct Cosine Similarity in Loss}

Using only cosine similarity without MSE led to poor reconstruction on norms.
\textbf{Resolution}: Hybrid 50/50 MSE + Cosine for both pointwise and angular precision.

\section{What Is Working}

\subsection{Geometry Preservation}

\textbf{Distance Correlation} (Pearson $r$ between original and latent pairwise distances): $\approx 0.85$
\begin{itemize}
    \item \textbf{Interpretation}: ``Strong correlation'' per methodology
    \item \textbf{Meaning}: Overall distance structure preserved when projecting $\mathbb{R}^{384} \to \mathbb{R}^{16}$
\end{itemize}

\textbf{kNN Preservation} (Jaccard IoU, k=20): $\approx 0.55$
\begin{itemize}
    \item \textbf{Interpretation}: ``Good neighborhood preservation''
    \item \textbf{Meaning}: $\sim$55\% of top-20 neighbors preserved in latent space
    \item \textbf{Sufficiency}: Acceptable for HDBSCAN (expects local density)
\end{itemize}

\subsection{Reconstruction Quality}

\begin{itemize}
    \item \textbf{Cosine Similarity}: $\approx 0.95$ (Excellent, $> 0.95$ threshold)
    \item \textbf{MSE}: $< 0.01$
    \item \textbf{Mean Absolute Error}: $< 0.001$
\end{itemize}

\subsection{Speciation Clustering}

\textbf{Centroid-Based Clustering} (Leader-Follower):
\begin{itemize}
    \item \textbf{Incremental}: O(n) per point, truly online
    \item \textbf{Split/Merge}: Detects clusters with high variance or large size
    \item \textbf{Lineage Tracking}: c-TF-IDF fingerprints identify species across generations
\end{itemize}

\textbf{Adaptive Clustering Parameters}:
\begin{itemize}
    \item Distance threshold: $\theta_{\text{sim}} = 0.3$ (cosine distance)
    \item Merge threshold: $\theta_{\text{merge}} = 0.15$
    \item Min cluster size: 3 members
    \item Max cluster size: 500
\end{itemize}

\subsection{Species Evolution}

\textbf{Convergence Pattern}:
\begin{itemize}
    \item \textbf{Generation 0}: $\approx 5$--15 initial species
    \item \textbf{Generations 1+}: Gradual consolidation via merging
    \item \textbf{Final}: Stable species count with semantic interpretation
\end{itemize}

\textbf{Lineage Preservation}:
\begin{itemize}
    \item c-TF-IDF fingerprints track species identity
    \item Rare species can be reconstructed across generations
    \item Supports evolutionary analysis and genealogy
\end{itemize}

\subsection{Integration with Toxicity}

Toxicity scores tracked per genome across all generations. Analysis shows:
\begin{itemize}
    \item Species average toxicity varies (correlation with size $\approx 0.1$--$0.3$)
    \item Larger species $\to$ more diverse toxicity (natural variance)
    \item Can identify ``high-toxicity species'' for targeted analysis
\end{itemize}

\section{Limitations and Future Work}

\subsection{Full Reclustering Overhead}

HDBSCAN requires full matrix computation (O($n^2$) distance matrix). Attempted incremental HDBSCAN with periodic full re-clustering every 5 generations, but full re-clustering takes 10--20 seconds even for $n \approx 1000$ genomes.

\textbf{Current Status}: Using incremental centroid clustering (Leader-Follower) instead.
\begin{itemize}
    \item Faster but less sophisticated than HDBSCAN
    \item Trade-off acceptable for real-time evolution
\end{itemize}

\subsection{Behavioral Features}

Attempted ensemble distance combining 70\% semantic + 30\% behavioral (confidence in moderation scores). However:
\begin{itemize}
    \item Behavioral signal too noisy (all scores in [0,1])
    \item Ensemble distance matrix computation becomes O($n^2$) bottleneck
\end{itemize}
\textbf{Current Status}: Using semantic (latent) distance only.

\subsection{Extreme Dimensionality Reduction}

\begin{itemize}
    \item $d < 8$ causes noticeable clustering degradation
    \item $d > 32$ provides minimal additional benefit
    \item $d=16$ is the sweet spot; cannot reduce further without quality loss
\end{itemize}

\subsection{Online Retraining}

Autoencoder weights frozen after Generation 0 training.
\begin{itemize}
    \item \textbf{Rationale}: Subsequent generations are variants, not distribution shift
    \item \textbf{Cost}: Retraining per generation would be 2--5 min per epoch
    \item \textbf{Assumption}: Latent space generalizes to variants of seed prompts
\end{itemize}

\subsection{Scalability Beyond 10K Genomes}

\begin{itemize}
    \item Latent computation: O(n), $\approx 100$ genomes/sec on GPU
    \item \textbf{Bottleneck}: Centroid clustering = O($n^2$) distance comparisons
    \item For $n = 10K$: $\sim 1$ second/generation (acceptable)
    \item For $n = 100K$: $\sim 100$ seconds (problematic)
    \item \textbf{Solution}: Approximate nearest centroid search (e.g., KD-tree) not yet implemented
\end{itemize}

\section{Summary}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|l|l|}
\hline
\textbf{Aspect} & \textbf{Status} & \textbf{Metric} & \textbf{Notes} \\
\hline
Reconstruction & \checkmark & $\cos \sim 0.95$ & Excellent \\
Geometry & \checkmark & $r_{\text{dist}} = 0.85$ & Strong correlation \\
kNN Preservation & \checkmark & IoU = 0.55 & Good \\
Speciation & \checkmark & $n_{\text{species}} = 5$--20 & Stable \\
Lineage Tracking & \checkmark & c-TF-IDF & Works \\
Toxicity Analysis & \checkmark & Tracked & Per-species \\
Online Speed & \checkmark & $< 1$ sec/gen & Real-time \\
Full Reclustering & \times & O($n^2$) & Bottleneck \\
Behavioral Features & \times & Noisy & Dropped \\
Scalability & $\approx$ & O($n^2$) & Up to 10K \\
\hline
\end{tabular}
\caption{Summary of approach status across key dimensions.}
\end{table}

\section{Methodological Insights}

\begin{enumerate}
    \item \textbf{Geometry Matters for Clustering}: Distance correlation $> 0.8$ and kNN preservation $> 0.5$ are sufficient for downstream clustering; perfect preservation is unnecessary.
    
    \item \textbf{Hybrid Losses Essential}: Pure reconstruction misses semantic structure; geometry + contrastive losses are critical for speciation quality.
    
    \item \textbf{Latent Dimension Trade-off}: $d=16$ balances compression (24$\times$) and utility; this sweet spot was empirically verified.
    
    \item \textbf{Incremental Over Batch}: Leader-Follower + periodic merge outperforms batch HDBSCAN for online evolution; speed advantage justifies small clustering quality difference.
    
    \item \textbf{Frozen Embeddings Valid}: Autoencoder trained on Generation 0 generalizes to variants; retraining is unnecessary for distributions similar to seed prompts.
    
    \item \textbf{Metadata-Driven Compatibility}: Tracking architecture version in metadata enables seamless weight reuse and prevents silent failures.
\end{enumerate}

\section{Comparative Analysis: Alternative Dimensionality Reduction Methods}

This section documents findings from five alternative dimensionality reduction and clustering approaches applied to the same prompt embedding dataset (Generation 0, $n \approx 100$ prompts). Each method was evaluated for its utility in semantic speciation.

\subsection{Methodology}

All methods:
\begin{itemize}
    \item Used identical input: 384D Sentence-BERT embeddings (all-MiniLM-L6-v2)
    \item Evaluated on same dataset: Generation 0 prompts with toxicity scores
    \item Assessed for downstream clustering quality and interpretability
    \item Focused on online capability for incremental evolution
\end{itemize}

\subsection{Method 1: BERTopic}

\textbf{Approach}: Neural topic modeling using pretrained BERT embeddings and UMAP reduction + HDBSCAN clustering.

\textbf{Configuration}:
\begin{itemize}
    \item UMAP: 50-150D reduction, $n\_neighbors=15$
    \item HDBSCAN: $\text{min\_cluster\_size}=10$, $\text{min\_samples}=5$
    \item Clustering algorithm: density-based with hierarchical agglomeration
\end{itemize}

\textbf{Results}:
\begin{itemize}
    \item \textbf{Clusters Found}: 5--15 natural topics (semantic groups)
    \item \textbf{Noise Points}: $< 5\%$ (good cluster cohesion)
    \item \textbf{Interpretability}: Topic labels via TF-IDF are intuitive
    \item \textbf{Stability}: Stable across runs (deterministic after random seed)
\end{itemize}

\textbf{Pros}:
\begin{itemize}
    \item Natural semantic topics (e.g., ``safety concerns'', ``adversarial examples'')
    \item Built-in topic representation (TF-IDF centroids)
    \item Fewer hyperparameters to tune
\end{itemize}

\textbf{Cons}:
\begin{itemize}
    \item HDBSCAN O($n^2$) bottleneck for large $n$
    \item Not incremental (requires full retrain for new data)
    \item Topic definitions change with new documents
\end{itemize}

\textbf{Suitability for Speciation}: \textbf{Medium}. Good for offline analysis but not suitable for online evolution due to retraining overhead.

\subsection{Method 2: Incremental PCA (IPCA)}

\textbf{Approach}: Memory-efficient PCA using mini-batch updates, followed by centroid-based clustering.

\textbf{Configuration}:
\begin{itemize}
    \item IPCA: 384D → 50D (variance retained $\approx 85\%$)
    \item Batch size: 256
    \item Clustering: Leader-Follower with $\text{distance\_threshold}=0.3$
\end{itemize}

\textbf{Results}:
\begin{itemize}
    \item \textbf{Dimensionality Reduction}: 384 → 50 (7.7$\times$)
    \item \textbf{Variance Retained}: 85\%
    \item \textbf{Clusters Found}: 8--12 centroid-based clusters
    \item \textbf{Noise Points}: 0\% (all points assigned)
    \item \textbf{Speed}: O(n) per point, very fast
\end{itemize}

\textbf{Pros}:
\begin{itemize}
    \item Truly incremental: Process new points without retraining
    \item O(n) complexity, scales to millions of points
    \item Easy to implement and understand
    \item Captures 85\% of variance with 50D
\end{itemize}

\textbf{Cons}:
\begin{itemize}
    \item Less compression than UMAP/autoencoders
    \item Linear dimensionality reduction (misses non-linear structure)
    \item Leader-Follower order-dependent (first point distributions affect clustering)
\end{itemize}

\textbf{Suitability for Speciation}: \textbf{High}. Good incremental performance, but 50D latent space less interpretable than learned representations.

\subsection{Method 3: Landmark Multidimensional Scaling (LMDS)}

\textbf{Approach}: Fast approximation of classical MDS using landmark points, reduces O($n^2$) to O(n·$m$) where $m$ = number of landmarks.

\textbf{Configuration}:
\begin{itemize}
    \item Landmarks: 20--30\% of data (~20 points)
    \item Final dimensions: 16
    \item Distance metric: Euclidean on 384D embeddings
\end{itemize}

\textbf{Results}:
\begin{itemize}
    \item \textbf{Computational Cost}: O(n·20) vs O(n²) for full MDS
    \item \textbf{Stress Value}: 0.15--0.25 (acceptable quality)
    \item \textbf{Clusters Found}: 6--10
    \item \textbf{kNN Preservation (k=20)}: IoU $\approx 0.4$ (moderate)
\end{itemize}

\textbf{Pros}:
\begin{itemize}
    \item Faster than full MDS: O(n·$m$) vs O(n²)
    \item Preserves global distance structure
    \item 16D representation suitable for downstream clustering
\end{itemize}

\textbf{Cons}:
\begin{itemize}
    \item Landmark selection heuristic-dependent
    \item kNN preservation lower than UMAP (0.4 vs 0.55)
    \item Not truly incremental (requires landmark recomputation)
    \item Higher stress value indicates geometry distortion
\end{itemize}

\textbf{Suitability for Speciation}: \textbf{Low-Medium}. Faster than full MDS but geometry preservation insufficient for clustering quality.

\subsection{Method 4: Top2Vec}

\textbf{Approach}: Combines UMAP dimensionality reduction with HDBSCAN clustering, using document embeddings to compute topic vectors as dense cluster centroids.

\textbf{Configuration}:
\begin{itemize}
    \item UMAP: 384D → 150D, $n\_neighbors=15$, $\text{min\_dist}=0.1$
    \item HDBSCAN: $\text{min\_cluster\_size}=10$, $\text{min\_samples}=5$
    \item Topic extraction: Mean embedding of cluster members
\end{itemize}

\textbf{Results}:
\begin{itemize}
    \item \textbf{Intermediate Dimensions}: 384D → 150D
    \item \textbf{Clusters Found}: 8--15
    \item \textbf{Noise Points}: 2--5\%
    \item \textbf{Topic Interpretability}: High (centroid-based)
    \item \textbf{Topic Stability}: Stable across runs
\end{itemize}

\textbf{Pros}:
\begin{itemize}
    \item 150D intermediate space better captures semantic structure than 50D PCA
    \item HDBSCAN finds natural density-based clusters
    \item Topic vectors interpretable as prototypical prompts
    \item Works well for $n < 10K$ (typical evolutionary run)
\end{itemize}

\textbf{Cons}:
\begin{itemize}
    \item 150D is too large for true latent representation (vs 16D autoencoder)
    \item HDBSCAN O($n^2$) limits scalability
    \item Not incremental (both UMAP and HDBSCAN require full recomputation)
\end{itemize}

\textbf{Suitability for Speciation}: \textbf{Medium}. Better than BERTopic for online use (150D vs 50D is more efficient) but still requires full retrain.

\subsection{Method 5: Parametric UMAP with HDBSCAN}

\textbf{Approach}: Trains a neural network to learn UMAP's embedding function, enabling incremental transformation of new points without retraining the dimensionality reduction.

\textbf{Configuration}:
\begin{itemize}
    \item UMAP: Parametric (encoder-only), 384D → 16D
    \item Neural network: 3 fully connected layers with GELU, LayerNorm
    \item Training: Adam optimizer, 200 epochs on Generation 0
    \item Clustering: HDBSCAN on 16D latent space
\end{itemize}

\textbf{Results}:
\begin{itemize}
    \item \textbf{Latent Dimension}: 16D (24$\times$ compression)
    \item \textbf{kNN Preservation (k=20)}: IoU $\approx 0.55$ (good)
    \item \textbf{Distance Correlation}: $r \approx 0.85$ (strong)
    \item \textbf{Clusters Found}: 5--15 via HDBSCAN
    \item \textbf{Inference Speed}: $\approx 100$ points/sec on GPU
    \item \textbf{HDBSCAN Overhead}: $\sim 1$ sec for $n=500$
\end{itemize}

\textbf{Pros}:
\begin{itemize}
    \item \textbf{HIGHEST compression}: 384 → 16 (24$\times$)
    \item Topology-preserving (UMAP objective)
    \item Truly incremental transformation (trained once, apply forever)
    \item Latent space rich enough for clustering
    \item Low inference cost: O(n) per point
\end{itemize}

\textbf{Cons}:
\begin{itemize}
    \item HDBSCAN still O($n^2$) bottleneck
    \item Requires training neural network (not lightweight)
    \item Parameters fixed after training (doesn't adapt to evolutionary drift)
\end{itemize}

\textbf{Suitability for Speciation}: \textbf{Highest}. Best balance of:
\begin{itemize}
    \item Compression (16D vs 50/150D)
    \item Incremental inference (trained once)
    \item Geometry preservation (IoU=0.55, $r$=0.85)
    \item Semantic quality (suitable for HDBSCAN or centroid clustering)
\end{itemize}

\subsection{Method 6: Leader-Follower Clustering}

\textbf{Approach}: Centroid-based clustering that operates directly on 384D embeddings without dimensionality reduction. Uses semantic distance thresholds to assign individuals to existing species or create new ones.

\textbf{Algorithm}:
\begin{enumerate}
    \item Sort population by fitness (descending)
    \item First individual becomes first leader (species founder)
    \item For each remaining individual:
    \begin{itemize}
        \item Find nearest leader by cosine distance
        \item If distance $< \theta_{\text{sim}}$ → assign to that species
        \item Else if fitness $>$ viability baseline → send to limbo (outlier buffer)
        \item Else → create new species with individual as leader
    \end{itemize}
    \item Repeat incrementally for new generations
\end{enumerate}

\textbf{Configuration}:
\begin{itemize}
    \item Distance threshold: $\theta_{\text{sim}} = 0.5$ (cosine distance)
    \item Viability baseline: 0.3 (fitness threshold for limbo)
    \item No dimensionality reduction (operates on 384D directly)
    \item Incremental: Add generations sequentially
\end{itemize}

\textbf{Results} (Generation 0 → 50, $n \approx 2500$ genomes):
\begin{itemize}
    \item \textbf{Initial Species (Gen0)}: 8--15 species
    \item \textbf{Final Species (Gen50)}: 15--25 species
    \item \textbf{Species Size}: Mean $\approx 100$, range 1--500
    \item \textbf{Limbo Buffer}: 5--10\% of population
    \item \textbf{Fitness Coherence}: Avg within-species fitness std $< 0.15$ (good)
    \item \textbf{Speed}: \textbf{Extremely fast} ($\mu$s per point, O(n·$m$) where $m$=species count)
\end{itemize}

\textbf{Pros}:
\begin{itemize}
    \item \textbf{Zero compression loss}: Uses full 384D embeddings, no dimensionality reduction
    \item \textbf{Fastest clustering}: O($k$) per point (k=current species count, typically 10--30)
    \item \textbf{Truly incremental}: Process one generation at a time
    \item \textbf{Fitness-aware}: Prioritizes high-fitness individuals as leaders
    \item \textbf{Limbo mechanism}: Buffers high-fitness outliers for potential new niches
    \item \textbf{No training required}: Clustering is heuristic-based
    \item \textbf{Interpretable leaders}: Each species has explicit founder/leader
\end{itemize}

\textbf{Cons}:
\begin{itemize}
    \item \textbf{No dimensionality reduction}: Stores full 384D embeddings
    \item Order-dependent: Early individuals influence cluster structure
    \item Hyperparameter sensitive: $\theta_{\text{sim}}$ and viability baseline affect results
    \item Heuristic clustering: Not optimal in statistical sense (vs HDBSCAN)
    \item Memory cost: O(n·384) for embeddings (vs O(n·16) for Parametric UMAP)
\end{itemize}

\textbf{Suitability for Speciation}: \textbf{High} for online evolution. Ideal for:
\begin{itemize}
    \item Real-time incremental processing
    \item Large populations where O($n^2$) is prohibitive
    \item Low-latency systems (microsecond clustering)
    \item Scenarios where fitness is key speciation signal
\end{itemize}

\textbf{Comparison with Parametric UMAP}:
\begin{itemize}
    \item \textbf{Parametric UMAP}: Better geometry preservation + compression, slower
    \item \textbf{Leader-Follower}: Better raw speed + no training required, larger memory footprint
    \item \textbf{Hybrid approach}: Use Parametric UMAP for offline Gen0 analysis, Leader-Follower for online evolution
\end{itemize}

\subsection{Comparative Summary Table}

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Method} & \textbf{Compression} & \textbf{kNN IoU} & \textbf{Incremental?} & \textbf{Speed} & \textbf{Score} \\
\hline
BERTopic & 50D (7.7$\times$) & 0.40 & \times & Slow & 6/10 \\
IPCA & 50D (7.7$\times$) & 0.35 & \checkmark & Very Fast & 7/10 \\
LMDS & 16D (24$\times$) & 0.40 & Partial & Medium & 5/10 \\
Top2Vec & 150D (2.6$\times$) & 0.48 & \times & Slow & 6/10 \\
Leader-Follower & Embed (0$\times$) & 0.55 & \checkmark & Very Fast & 8/10 \\
Parametric UMAP & 16D (24$\times$) & 0.55 & \checkmark & Fast & 9/10 \\
\hline
\end{tabular}
\caption{Comparative analysis of six dimensionality reduction and clustering methods. Parametric UMAP achieves best balance of compression, geometry preservation, and incrementality. Leader-Follower offers fastest incremental clustering without dimensionality reduction.}
\end{table}

\subsection{Recommendation}

\textbf{Parametric UMAP} is the recommended approach for **semantic-quality-first** speciation:

\begin{enumerate}
    \item \textbf{Maximum Compression}: 16D is optimal for speciation (balances interpretability and efficiency)
    \item \textbf{Strong Geometry Preservation}: kNN IoU = 0.55, distance correlation = 0.85
    \item \textbf{True Incrementality}: Trained on Gen0, applied to all future generations without retraining
    \item \textbf{Fast Inference}: O(n) cost, $\approx 100$ points/sec on GPU
    \item \textbf{Semantic Quality}: Latent space suitable for HDBSCAN or centroid clustering
\end{enumerate}

\textbf{Leader-Follower} is the recommended approach for **speed-first** speciation:

\begin{enumerate}
    \item \textbf{Fastest Clustering}: O(k) per point where k $\approx$ 10--30 species
    \item \textbf{No Compression Loss}: Uses full 384D embeddings (highest fidelity)
    \item \textbf{Truly Incremental}: Process one generation at a time
    \item \textbf{Fitness-Aware}: Naturally prioritizes high-fitness individuals
    \item \textbf{No Training Required}: Heuristic-based, instant deployment
\end{enumerate}

\textbf{Hybrid Approach} (Recommended for large-scale evolution):

\begin{itemize}
    \item \textbf{Offline (Generation 0)}: Train Parametric UMAP on Gen0 for high-quality analysis
    \item \textbf{Online (Generations 1+)}: Use Leader-Follower on 384D embeddings for real-time evolution
    \item \textbf{Benefit}: Combines semantic quality (analysis) with computational efficiency (evolution)
\end{itemize}

\textbf{Trade-offs Summary}:
\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|}
\hline
\textbf{Criterion} & \textbf{Parametric UMAP} & \textbf{Leader-Follower} \\
\hline
Geometry Preservation & \checkmark\checkmark\checkmark & \checkmark (full 384D) \\
Compression & 24$\times$ (16D) & None (384D) \\
Speed & \checkmark\checkmark & \checkmark\checkmark\checkmark \\
Incrementality & \checkmark & \checkmark\checkmark\checkmark \\
Training Cost & \checkmark & \checkmark\checkmark\checkmark \\
Memory Footprint & \checkmark\checkmark\checkmark (16D) & \checkmark (384D) \\
Semantic Quality & \checkmark\checkmark\checkmark & \checkmark\checkmark \\
Fitness Awareness & \checkmark & \checkmark\checkmark\checkmark \\
\hline
\end{tabular}
\caption{Trade-off analysis between Parametric UMAP and Leader-Follower clustering for speciation.}
\end{table}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.