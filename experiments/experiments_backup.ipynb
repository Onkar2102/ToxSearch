{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea7a371",
   "metadata": {},
   "source": [
    "# Load the JSON and Convert it into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720585f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Notebook configuration complete\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Setup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # Suppress seaborn warnings\n",
    "\n",
    "from pathlib import Path \n",
    "import json, string, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Set up matplotlib and seaborn for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.1)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR = Path(\"../experiments\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Notebook configuration complete\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Schema Information\n",
    "\n",
    "The current data uses OpenAI moderation scores with the following structure:\n",
    "- **Primary metric**: `score_violence` (north star for optimization)\n",
    "- **Other scores**: `score_hate`, `score_harassment`, `score_sexual`, etc.\n",
    "- **Response field**: `generated_text` (newer) or `generated_response` (legacy)\n",
    "- **Status values**: `complete`, `pending_evolution`, `pending_evaluation`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06920de",
   "metadata": {},
   "source": [
    "# Load Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42801310",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprimary_column\u001b[39m\u001b[33m\"\u001b[39m: primary_col,\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmissing_count\u001b[39m\u001b[33m\"\u001b[39m: df[primary_col].isna().sum(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_score\u001b[39m\u001b[33m\"\u001b[39m: df[primary_col].max()\n\u001b[32m     38\u001b[39m     }\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Analyze current dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m tox_analysis = safe_toxicity_analysis(\u001b[43mdf\u001b[49m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Toxicity Score Analysis ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrimary column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtox_analysis[\u001b[33m'\u001b[39m\u001b[33mprimary_column\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper functions for robust data analysis\n",
    "def get_toxicity_columns(df):\n",
    "    \"\"\"Get all available toxicity score columns\"\"\"\n",
    "    return [col for col in df.columns if col.startswith(\"score_\")]\n",
    "\n",
    "def get_primary_toxicity_column(df, preferred=\"score_violence\"):\n",
    "    \"\"\"Get the primary toxicity column, with fallback options\"\"\"\n",
    "    tox_cols = get_toxicity_columns(df)\n",
    "    \n",
    "    if preferred in tox_cols:\n",
    "        return preferred\n",
    "    elif \"score_violence\" in tox_cols:\n",
    "        return \"score_violence\"\n",
    "    elif tox_cols:\n",
    "        return tox_cols[0]  # Return first available\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def safe_toxicity_analysis(df):\n",
    "    \"\"\"Safely analyze toxicity scores with fallbacks\"\"\"\n",
    "    primary_col = get_primary_toxicity_column(df)\n",
    "    \n",
    "    if primary_col is None:\n",
    "        return {\n",
    "            \"primary_column\": None,\n",
    "            \"missing_count\": len(df),\n",
    "            \"has_scores\": False,\n",
    "            \"available_columns\": []\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"primary_column\": primary_col,\n",
    "        \"missing_count\": df[primary_col].isna().sum(),\n",
    "        \"has_scores\": True,\n",
    "        \"available_columns\": get_toxicity_columns(df),\n",
    "        \"mean_score\": df[primary_col].mean(),\n",
    "        \"max_score\": df[primary_col].max()\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Helper functions loaded - will analyze data after loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_population_to_dataframe(json_path=None):\n",
    "    \"\"\"Load population data with fallback paths and error handling\"\"\"\n",
    "    if json_path is None:\n",
    "        # Try multiple fallback paths\n",
    "        possible_paths = [\n",
    "            Path(\"../outputs/Population.json\"),\n",
    "            Path(\"outputs/Population.json\"), \n",
    "            Path(\"../outputs/population_11.json\")\n",
    "        ]\n",
    "        \n",
    "        json_path = None\n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                json_path = path\n",
    "                break\n",
    "        \n",
    "        if json_path is None:\n",
    "            raise FileNotFoundError(f\"No population file found. Tried: {[str(p) for p in possible_paths]}\")\n",
    "    \n",
    "    print(f\"Loading population from: {json_path}\")\n",
    "    \n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for genome in data:\n",
    "        # Handle missing data gracefully\n",
    "        moderation = genome.get(\"moderation_result\") or {}\n",
    "        creation_info = genome.get(\"creation_info\") or {}\n",
    "        scores = moderation.get(\"scores\") or {}\n",
    "\n",
    "        record = {\n",
    "            \"id\": genome.get(\"id\"),\n",
    "            \"prompt_id\": genome.get(\"prompt_id\"),\n",
    "            \"prompt\": genome.get(\"prompt\", \"\"),\n",
    "            \"generated_text\": genome.get(\"generated_text\"),  # Updated field name\n",
    "            \"generated_response\": genome.get(\"generated_response\"),  # Legacy field\n",
    "            \"operator\": genome.get(\"operator\"),\n",
    "            \"parents\": genome.get(\"parents\"),\n",
    "            \"generation\": genome.get(\"generation\", 0),\n",
    "            \"status\": genome.get(\"status\", \"unknown\"),\n",
    "            \"flagged\": moderation.get(\"flagged\"),\n",
    "            \"type\": creation_info.get(\"type\"),\n",
    "            \"operator_source_generation\": creation_info.get(\"source_generation\")\n",
    "        }\n",
    "\n",
    "        # Flatten all toxicity scores (e.g., violence, hate, etc.)\n",
    "        for score_key, score_value in scores.items():\n",
    "            record[f\"score_{score_key}\"] = score_value\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Data validation\n",
    "    print(f\"âœ“ Loaded {len(df)} genomes\")\n",
    "    print(f\"âœ“ Generations: {df['generation'].min()} â†’ {df['generation'].max()}\")\n",
    "    print(f\"âœ“ Operators: {df['operator'].nunique()} unique\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_population_to_dataframe()\n",
    "\n",
    "# Now analyze toxicity scores\n",
    "tox_analysis = safe_toxicity_analysis(df)\n",
    "print(\"\\n=== Toxicity Score Analysis ===\")\n",
    "print(f\"Primary column: {tox_analysis['primary_column']}\")\n",
    "print(f\"Available score columns: {tox_analysis['available_columns']}\")\n",
    "if tox_analysis['has_scores']:\n",
    "    print(f\"Missing values: {tox_analysis['missing_count']}\")\n",
    "    print(f\"Mean score: {tox_analysis['mean_score']:.4f}\")\n",
    "    print(f\"Max score: {tox_analysis['max_score']:.4f}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No toxicity score columns found!\")\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3499eb5",
   "metadata": {},
   "source": [
    "# Build Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90502e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"operator\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Check multiple status possibilities (complete, pending_evolution, etc.)\n",
    "pending = df[df[\"status\"] != \"complete\"]\n",
    "\n",
    "# Check for missing violence scores (primary toxicity metric)\n",
    "if \"score_violence\" in df.columns:\n",
    "    missing_violence = df[\"score_violence\"].isna()\n",
    "    violence_col_exists = True\n",
    "else:\n",
    "    missing_violence = pd.Series([True] * len(df))  # All missing if column doesn't exist\n",
    "    violence_col_exists = False\n",
    "\n",
    "# Check for missing responses (both field names)\n",
    "missing_response = (df[\"generated_response\"].isna() & df.get(\"generated_text\", pd.Series([True] * len(df))).isna())\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\", \n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending/incomplete genomes\",\n",
    "        f\"Rows missing violence scores{'' if violence_col_exists else ' (column missing)'}\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_violence.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Updated Dataset Summary ===\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Complete Operator Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70500722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete operator breakdown with all details visible\n",
    "print(\"=== ALL OPERATORS AND THEIR COUNTS ===\")\n",
    "\n",
    "# Get operator counts including None (original genomes)\n",
    "operator_counts_full = df[\"operator\"].value_counts(dropna=False)\n",
    "\n",
    "# Create comprehensive operator table\n",
    "operator_analysis = pd.DataFrame({\n",
    "    'Operator': operator_counts_full.index,\n",
    "    'Count': operator_counts_full.values,\n",
    "    'Percentage': (operator_counts_full.values / len(df) * 100).round(2),\n",
    "    'Cumulative %': (operator_counts_full.values / len(df) * 100).cumsum().round(2)\n",
    "})\n",
    "\n",
    "# Replace NaN operator with descriptive name\n",
    "operator_analysis['Operator'] = operator_analysis['Operator'].fillna('Original (Generation 0)')\n",
    "\n",
    "display(operator_analysis)\n",
    "\n",
    "# Summary insights\n",
    "print(f\"\\nðŸ“Š OPERATOR INSIGHTS:\")\n",
    "print(f\"â€¢ Total unique operators: {operator_counts_full.dropna().shape[0]}\")\n",
    "print(f\"â€¢ Most common: {operator_analysis.iloc[0]['Operator']} ({operator_analysis.iloc[0]['Count']} genomes, {operator_analysis.iloc[0]['Percentage']}%)\")\n",
    "print(f\"â€¢ Least common: {operator_analysis.iloc[-1]['Operator']} ({operator_analysis.iloc[-1]['Count']} genomes, {operator_analysis.iloc[-1]['Percentage']}%)\")\n",
    "print(f\"â€¢ Top 3 operators represent {operator_analysis.head(3)['Percentage'].sum():.1f}% of all genomes\")\n",
    "\n",
    "# Show all operator names in a clean list\n",
    "all_operators = [op for op in operator_analysis['Operator'].tolist() if op != 'Original (Generation 0)']\n",
    "print(f\"\\nðŸ”§ ALL EVOLUTION OPERATORS:\")\n",
    "for i, op in enumerate(all_operators, 1):\n",
    "    count = operator_analysis[operator_analysis['Operator'] == op]['Count'].iloc[0]\n",
    "    print(f\"  {i}. {op}: {count} genomes\")\n",
    "\n",
    "print(f\"\\nâœ¨ Total evolution variants: {len(df) - operator_analysis[operator_analysis['Operator'] == 'Original (Generation 0)']['Count'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize operator distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart of operator counts\n",
    "operator_viz = operator_analysis[operator_analysis['Operator'] != 'Original (Generation 0)'].copy()\n",
    "ax1.bar(range(len(operator_viz)), operator_viz['Count'], color='skyblue', edgecolor='navy')\n",
    "ax1.set_xticks(range(len(operator_viz)))\n",
    "ax1.set_xticklabels(operator_viz['Operator'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Number of Genomes')\n",
    "ax1.set_title('Distribution of Evolution Operators')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate(operator_viz['Count']):\n",
    "    ax1.text(i, count + 1, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Pie chart for better proportional view\n",
    "colors = plt.cm.Set3(range(len(operator_analysis)))\n",
    "wedges, texts, autotexts = ax2.pie(operator_analysis['Count'], \n",
    "                                   labels=operator_analysis['Operator'],\n",
    "                                   autopct='%1.1f%%', \n",
    "                                   colors=colors,\n",
    "                                   startangle=90)\n",
    "\n",
    "# Improve readability\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    \n",
    "ax2.set_title('Operator Distribution (Proportional View)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"operator_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ Operator distribution visualization saved to experiments/operator_distribution.png\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Operator Performance by Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee079907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed generation Ã— operator analysis\n",
    "print(\"=== OPERATOR USAGE BY GENERATION ===\")\n",
    "\n",
    "# Create crosstab of generation vs operator\n",
    "gen_operator_crosstab = pd.crosstab(df['generation'], df['operator'], margins=True, margins_name=\"Total\")\n",
    "display(gen_operator_crosstab)\n",
    "\n",
    "# Calculate percentages within each generation\n",
    "print(\"\\n=== OPERATOR PERCENTAGES BY GENERATION ===\")\n",
    "gen_operator_pct = pd.crosstab(df['generation'], df['operator'], normalize='index') * 100\n",
    "gen_operator_pct = gen_operator_pct.round(1)\n",
    "display(gen_operator_pct)\n",
    "\n",
    "# Show which operators were most/least active in each generation\n",
    "print(\"\\nðŸŽ¯ GENERATION INSIGHTS:\")\n",
    "for gen in sorted(df['generation'].unique()):\n",
    "    gen_data = df[df['generation'] == gen]\n",
    "    if len(gen_data) > 1:  # Skip generation 0 (usually just 1 original)\n",
    "        top_ops = gen_data['operator'].value_counts()\n",
    "        if len(top_ops) > 0:\n",
    "            print(f\"â€¢ Generation {gen}: {top_ops.iloc[0]} most active ({top_ops.iloc[0]} variants)\")\n",
    "            if len(top_ops) > 1:\n",
    "                print(f\"  â””â”€ Runner-up: {top_ops.index[1]} ({top_ops.iloc[1]} variants)\")\n",
    "\n",
    "# Evolution trajectory summary\n",
    "print(f\"\\nðŸ“ˆ EVOLUTION TRAJECTORY:\")\n",
    "print(f\"â€¢ Started with: {len(df[df['generation'] == 0])} original genome(s)\")\n",
    "print(f\"â€¢ Peak generation: {df['generation'].max()} (latest)\")\n",
    "print(f\"â€¢ Most productive generation: {df[df['generation'] > 0].groupby('generation').size().idxmax()} ({df[df['generation'] > 0].groupby('generation').size().max()} variants)\")\n",
    "print(f\"â€¢ Total variants created: {len(df[df['generation'] > 0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51940b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Check multiple status possibilities (complete, pending_evolution, etc.)\n",
    "pending = df[df[\"status\"] != \"complete\"]\n",
    "\n",
    "# Check for missing violence scores (primary toxicity metric)\n",
    "if \"score_violence\" in df.columns:\n",
    "    missing_violence = df[\"score_violence\"].isna()\n",
    "    violence_col_exists = True\n",
    "else:\n",
    "    missing_violence = pd.Series([True] * len(df))  # All missing if column doesn't exist\n",
    "    violence_col_exists = False\n",
    "\n",
    "# Check for missing responses (both field names)\n",
    "missing_response = (df[\"generated_response\"].isna() & df.get(\"generated_text\", pd.Series([True] * len(df))).isna())\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\", \n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending/incomplete genomes\",\n",
    "        f\"Rows missing violence scores{'' if violence_col_exists else ' (column missing)'}\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_violence.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Updated Dataset Summary ===\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Check multiple status possibilities (complete, pending_evolution, etc.)\n",
    "pending = df[df[\"status\"] != \"complete\"]\n",
    "\n",
    "# Check for missing violence scores (primary toxicity metric)\n",
    "if \"score_violence\" in df.columns:\n",
    "    missing_violence = df[\"score_violence\"].isna()\n",
    "    violence_col_exists = True\n",
    "else:\n",
    "    missing_violence = pd.Series([True] * len(df))  # All missing if column doesn't exist\n",
    "    violence_col_exists = False\n",
    "\n",
    "# Check for missing responses (both field names)\n",
    "missing_response = (df[\"generated_response\"].isna() & df.get(\"generated_text\", pd.Series([True] * len(df))).isna())\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\", \n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending/incomplete genomes\",\n",
    "        f\"Rows missing violence scores{'' if violence_col_exists else ' (column missing)'}\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_violence.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Updated Dataset Summary ===\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has been replaced with the updated operator analysis above.\n",
    "# Please use the \"Complete Operator Analysis\" section instead.\n",
    "print(\"âš ï¸  This cell is deprecated - see the Complete Operator Analysis section above for detailed breakdowns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fa05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8660268",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38900631",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops     = operator_counts.index.tolist()\n",
    "n_ops          = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Pending = status not \"scored\"\n",
    "pending = df.loc[df[\"status\"].ne(\"scored\")]\n",
    "\n",
    "# Missing toxicity = null toxicity_score\n",
    "missing_tox = df[\"toxicity_score\"].isna()\n",
    "missing_response = df[\"generated_response\"].isna()\n",
    "\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Operatorâ†’count map\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        operator_counts.to_dict(),\n",
    "        (int(gen_min), int(gen_max)),\n",
    "        len(pending),\n",
    "        missing_tox.sum(),\n",
    "        missing_response.sum(),\n",
    "        duplicate_prompts\n",
    "    ]\n",
    "})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics with better handling\n",
    "total_genomes = len(df)\n",
    "operator_counts = df[\"operator\"].value_counts()\n",
    "unique_ops = operator_counts.index.tolist()\n",
    "n_ops = operator_counts.size\n",
    "gen_min, gen_max = df[\"generation\"].min(), df[\"generation\"].max()\n",
    "\n",
    "# Check multiple status possibilities (scored, complete, pending_evolution, etc.)\n",
    "scored_genomes = df[df[\"status\"] == \"complete\"].shape[0]\n",
    "pending_genomes = df[df[\"status\"].isin([\"pending_generation\", \"pending_evaluation\", \"pending_evolution\"])].shape[0]\n",
    "\n",
    "# Missing response or moderation (check both old and new field names)\n",
    "missing_response = (df[\"generated_response\"].isna() & df[\"generated_text\"].isna()).sum()\n",
    "missing_moderation = df[\"flagged\"].isna().sum()\n",
    "\n",
    "# Count duplicate prompts\n",
    "duplicate_prompts = df.duplicated(\"prompt\").sum()\n",
    "\n",
    "# Get all toxicity score columns (score_*)\n",
    "tox_columns = [col for col in df.columns if col.startswith(\"score_\")]\n",
    "missing_scores = df[tox_columns].isna().any(axis=1).sum() if tox_columns else df.shape[0]\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total genomes\",\n",
    "        \"Unique operators\",\n",
    "        \"Generation range (min,max)\",\n",
    "        \"Scored/complete genomes\",\n",
    "        \"Pending genomes\",\n",
    "        \"Rows missing any moderation score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\",\n",
    "        \"Top 3 operators\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        total_genomes,\n",
    "        n_ops,\n",
    "        f\"{int(gen_min)} â†’ {int(gen_max)}\",\n",
    "        scored_genomes,\n",
    "        pending_genomes,\n",
    "        missing_scores,\n",
    "        missing_response,\n",
    "        duplicate_prompts,\n",
    "        \" | \".join([f\"{op}: {count}\" for op, count in operator_counts.head(3).items()])\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== Dataset Overview ===\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29388af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def style_summary(summary_df,\n",
    "                  caption=\"Dataset Summary\",\n",
    "                  save_latex=None,\n",
    "                  save_html=None):\n",
    "    \"\"\"\n",
    "    Pretty table of the summary metrics.\n",
    "    Highlights nonâ€‘zero 'problem' rows in light red.\n",
    "    \"\"\"\n",
    "    alert_rows = {\n",
    "        \"Pending genomes (statusâ‰ scored)\",\n",
    "        \"Rows missing toxicity_score\",\n",
    "        \"Rows missing response\",\n",
    "        \"Duplicate prompts\",\n",
    "    }\n",
    "\n",
    "    def highlight(row):\n",
    "        # row is a Series  (Metric, Value)\n",
    "        metric = row[\"Metric\"]\n",
    "        value  = row[\"Value\"]\n",
    "        if metric in alert_rows and value:\n",
    "            return [\"\", \"background-color:#ffe6e6\"]\n",
    "        return [\"\", \"\"]\n",
    "\n",
    "    sty = (\n",
    "        summary_df.style\n",
    "                 .apply(highlight, axis=1)\n",
    "                 .set_properties(subset=[\"Metric\"], **{\"text-align\": \"left\"})\n",
    "                 .format(na_rep=\"-\")\n",
    "                 .set_caption(caption)\n",
    "    )\n",
    "\n",
    "    # Hide index in a versionâ€‘agnostic way\n",
    "    if hasattr(sty, \"hide_index\"):\n",
    "        sty = sty.hide_index()\n",
    "    else:\n",
    "        sty = sty.hide(axis=\"index\")\n",
    "\n",
    "    if save_latex:\n",
    "        sty.to_latex(Path(save_latex), position=\"h!\")\n",
    "    if save_html:\n",
    "        sty.to_html(Path(save_html), doctype_html=True)\n",
    "\n",
    "    return sty\n",
    "\n",
    "styled = style_summary(\n",
    "    summary,\n",
    "    caption=\"Genomeâ€‘Pool Overview\",\n",
    "    save_latex=\"../experiments/summary_table.tex\",\n",
    "    save_html=\"../experiments/summary_table.html\"\n",
    ")\n",
    "styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88493049",
   "metadata": {},
   "source": [
    "# Detailed Operator Skew Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_df = operator_counts.reset_index()\n",
    "op_df.columns = [\"operator\", \"count\"]\n",
    "op_df[\"percent\"] = 100 * op_df[\"count\"] / total_genomes\n",
    "display(op_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_df = operator_counts.reset_index()\n",
    "op_df.columns = [\"operator\", \"count\"]\n",
    "op_df[\"percent\"] = 100 * op_df[\"count\"] / total_genomes\n",
    "display(op_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1291fc",
   "metadata": {},
   "source": [
    "# Key Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac529b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Quick Insights ===\")\n",
    "print(f\"- You have {total_genomes} genomes covering {n_ops} operators.\")\n",
    "print(f\"- Generations present: {gen_min} â†’ {gen_max}.\")\n",
    "print(f\"- Top 3 operators by frequency:\\n{op_df.head(3).to_string(index=False)}\")\n",
    "if len(pending):\n",
    "    print(f\"- WARNING: {len(pending)} genomes are still marked 'pending'.\")\n",
    "if missing_tox.sum():\n",
    "    print(f\"- NOTE: {missing_tox.sum()} genomes lack toxicity_score (need scoring).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Quick Insights ===\")\n",
    "print(f\"- You have {total_genomes} genomes covering {n_ops} operators.\")\n",
    "print(f\"- Generations present: {gen_min} â†’ {gen_max}.\")\n",
    "print(f\"- Top 3 operators by frequency:\\n{op_df.head(3).to_string(index=False)}\")\n",
    "if len(pending):\n",
    "    print(f\"- WARNING: {len(pending)} genomes are still marked 'pending'.\")\n",
    "if missing_tox.sum():\n",
    "    print(f\"- NOTE: {missing_tox.sum()} genomes lack toxicity_score (need scoring).\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Evolution Progress Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2823aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze EvolutionTracker.json for progress insights\n",
    "try:\n",
    "    with open(\"../outputs/EvolutionTracker.json\", 'r') as f:\n",
    "        evolution_tracker = json.load(f)\n",
    "    \n",
    "    print(\"=== Evolution Progress Analysis ===\")\n",
    "    \n",
    "    for entry in evolution_tracker:\n",
    "        prompt_id = entry[\"prompt_id\"]\n",
    "        status = entry[\"status\"]\n",
    "        total_gens = entry[\"total_generations\"]\n",
    "        \n",
    "        # Get best score across generations\n",
    "        best_gen = max(entry[\"generations\"], key=lambda x: x.get(\"max_score\", 0))\n",
    "        best_score = best_gen.get(\"max_score\", 0)\n",
    "        best_gen_num = best_gen.get(\"generation_number\", 0)\n",
    "        \n",
    "        print(f\"Prompt {prompt_id}: {status} | {total_gens} generations | \"\n",
    "              f\"Best: {best_score:.4f} (gen {best_gen_num})\")\n",
    "        \n",
    "        # Show evolution trajectory\n",
    "        scores = [gen.get(\"max_score\", 0) for gen in entry[\"generations\"]]\n",
    "        if len(scores) > 1:\n",
    "            improvement = scores[-1] - scores[0]\n",
    "            print(f\"  â†’ Score trajectory: {scores[0]:.4f} â†’ {scores[-1]:.4f} (Î”{improvement:+.4f})\")\n",
    "        print()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"EvolutionTracker.json not found - skipping evolution analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e09bf9",
   "metadata": {},
   "source": [
    "# Deduplicate Exact-String Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882848f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_only = df[df.duplicated(\"prompt\", keep='first')]\n",
    "num_duplicates = df.duplicated(\"prompt\", keep=False).sum()\n",
    "print(f\"Total duplicates in 'prompt' column: {num_duplicates}\")\n",
    "display(duplicates_only[[\"id\", \"prompt\"]])\n",
    "display(duplicates_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_counts = duplicates_only[\"prompt\"].value_counts()\n",
    "duplicate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d766b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates_only[duplicates_only[\"prompt\"] == \"Why should some nations bow down to others?\"].sort_values(\"parents\", ascending=True)\n",
    "duplicates_only.groupby([\"operator\",\"generation\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03087589",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupe_mask   = df.duplicated(\"prompt\")\n",
    "duplicates  = df[dupe_mask]\n",
    "dedup_df    = df[~dupe_mask].reset_index(drop=True)\n",
    "\n",
    "# Log counts\n",
    "n_dupes = len(duplicates)\n",
    "print(f\"Removed {n_dupes} duplicates \"\n",
    "      f\"({n_dupes/len(df):.1%} of total).\")\n",
    "\n",
    "dupe_counts = duplicates[\"operator\"].value_counts()\n",
    "print(\"\\nDuplicates per operator:\\n\", dupe_counts.to_string())\n",
    "\n",
    "# Save duplicates for future analysis\n",
    "duplicates.to_csv(\"../experiments/duplicate_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_tbl = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def tokenize(txt, strip_punct=False):\n",
    "    txt = txt.lower()\n",
    "    if strip_punct:\n",
    "        txt = txt.translate(punct_tbl)\n",
    "    return txt.split()\n",
    "\n",
    "# Create token + length columns only if absent\n",
    "if \"tokens_clean\" not in dedup_df.columns:\n",
    "    dedup_df[\"tokens_clean\"] = dedup_df[\"prompt\"].astype(str).apply(lambda t: tokenize(t, True))\n",
    "    dedup_df[\"len_clean\"]    = dedup_df[\"tokens_clean\"].str.len()\n",
    "\n",
    "if \"tokens_raw\" not in dedup_df.columns:\n",
    "    dedup_df[\"tokens_raw\"]   = dedup_df[\"prompt\"].astype(str).apply(lambda t: tokenize(t, False))\n",
    "    dedup_df[\"len_raw\"]      = dedup_df[\"tokens_raw\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05974023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_length_distribution = dedup_df.groupby(\"operator\")[\"len_clean\"].describe()\n",
    "prompt_length_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1faee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_tok(series):\n",
    "    return len({tok for toks in series for tok in toks})\n",
    "\n",
    "def hapax_ratio(series):\n",
    "    c = Counter(tok for toks in series for tok in toks)\n",
    "    return sum(v == 1 for v in c.values()) / sum(c.values()) if c else 0\n",
    "\n",
    "def shannon_entropy(series):\n",
    "    c = Counter(tok for toks in series for tok in toks)\n",
    "    if not c: \n",
    "        return 0\n",
    "    probs = np.array(list(c.values())) / sum(c.values())\n",
    "    return -(probs * np.log2(probs)).sum()\n",
    "\n",
    "metrics_tables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for view in [\"tokens_clean\", \"tokens_raw\"]:\n",
    "    TOK_COL = view\n",
    "    LEN_COL = \"len_clean\" if view == \"tokens_clean\" else \"len_raw\"\n",
    "\n",
    "    agg = (dedup_df.groupby(\"operator\")\n",
    "             .agg(total_tokens   = (LEN_COL,  \"sum\"),\n",
    "                  unique_tokens  = (TOK_COL,  uniq_tok),\n",
    "                  n_prompts      = (\"prompt\",\"size\"))\n",
    "          )\n",
    "    agg[\"TTR\"]             = agg[\"unique_tokens\"] / agg[\"total_tokens\"]\n",
    "    agg[\"uniq_per_prompt\"] = agg[\"unique_tokens\"] / agg[\"n_prompts\"]\n",
    "    agg[\"hapax_ratio\"]     = dedup_df.groupby(\"operator\")[TOK_COL].apply(hapax_ratio)\n",
    "    agg[\"entropy\"]         = dedup_df.groupby(\"operator\")[TOK_COL].apply(shannon_entropy)\n",
    "    agg[\"mean_prompt_len\"] = agg[\"total_tokens\"] / agg[\"n_prompts\"]\n",
    "    metrics_tables[view]   = agg.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat(metrics_tables, axis=1)\n",
    "display(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a630edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the secondâ€‘level \"TTR\" metric for both views\n",
    "ttr_df = combined.xs(\"TTR\", level=1, axis=1)   # DataFrame with columns tokens_clean / tokens_raw\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ttr_df.plot(kind=\"bar\")\n",
    "plt.ylabel(\"Typeâ€“Token Ratio\")\n",
    "plt.title(\"TTR per Operator (clean vs. raw punctuation)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"../experiments/lexical_diversity_clean_vs_raw.csv\")\n",
    "print(\"Saved metrics to experiments/lexical_diversity_clean_vs_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa240b",
   "metadata": {},
   "source": [
    "# Operator Dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dedup_df.groupby([\"generation\",\"operator\"]).size().unstack(fill_value=0)\n",
    "counts.plot(kind=\"barh\", stacked=True, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25f698",
   "metadata": {},
   "source": [
    "# Semantic Drift per Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746300bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 0.  Setup\n",
    "# -----------------------------------------------------------\n",
    "import json, pandas as pd, torch, datetime as dt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# I/O paths\n",
    "POP_PATH  = Path(\"../outputs/Population.json\")\n",
    "OUT_DIR   = Path(\"../experiments\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load population\n",
    "# df = pd.read_json(POP_PATH)\n",
    "df = dedup_df.copy() \n",
    "\n",
    "# Device & model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id    # safety for older HF versions\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    output_hidden_states=True,\n",
    "    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n",
    ").to(device).eval()\n",
    "\n",
    "# Helper: meanâ€‘pool last hidden state\n",
    "def llama_embed(texts, batch_size=32):\n",
    "    \"\"\"Return tensor of shape (N, hidden_dim)\"\"\"\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding with LLaMA\"):\n",
    "        batch = texts[i : i+batch_size]\n",
    "        toks  = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**toks)\n",
    "        last_hidden = out.hidden_states[-1]           # (B, seq, dim)\n",
    "        mask = toks[\"attention_mask\"].unsqueeze(-1)   # (B, seq, 1)\n",
    "        summed = (last_hidden * mask).sum(dim=1)\n",
    "        lens   = mask.sum(dim=1)                      # (B,1)\n",
    "        vecs   = summed / lens                       # mean pooling\n",
    "        vecs   = torch.nn.functional.normalize(vecs.float(), p=2, dim=1)\n",
    "        embs.append(vecs.cpu())\n",
    "    return torch.cat(embs, dim=0)                     # (N, dim)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1.  Embed all prompts once\n",
    "# -----------------------------------------------------------\n",
    "all_embs = llama_embed(df[\"prompt\"].tolist(), batch_size=256)   # adjust batch_size as memory allows\n",
    "index2emb = {idx: emb for idx, emb in enumerate(all_embs)}\n",
    "\n",
    "# Build idâ€‘>rowâ€‘index map for quick lookup\n",
    "id2idx = df.reset_index().set_index(\"id\")[\"index\"].to_dict()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2.  Cosine similarity to first parent\n",
    "# -----------------------------------------------------------\n",
    "def sim_to_parent_llama(row_idx, row):\n",
    "    parents = row[\"parents\"]\n",
    "    if not parents:\n",
    "        return None\n",
    "    parent_idx = id2idx.get(parents[0])\n",
    "    if parent_idx is None:\n",
    "        return None\n",
    "    child_emb  = index2emb[row_idx]\n",
    "    parent_emb = index2emb[parent_idx]\n",
    "    return float(torch.dot(child_emb, parent_emb))\n",
    "\n",
    "df[\"sim_to_parent_llama\"] = [\n",
    "    sim_to_parent_llama(i, row) for i, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3.  Aggregate per operator\n",
    "# -----------------------------------------------------------\n",
    "# sim_stats = (\n",
    "#     df.dropna(subset=[\"sim_to_parent_llama\"])\n",
    "#       .groupby(\"operator\")[\"sim_to_parent_llama\"]\n",
    "#       .agg([\"count\",\"mean\",\"median\",\"std\"])\n",
    "#       .round(3)\n",
    "# )\n",
    "\n",
    "# display(sim_stats.sort_values(\"mean\", ascending=False))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3A.  Perâ€‘operator Ã— generation stats\n",
    "# -----------------------------------------------------------\n",
    "sim_stats_gen = (\n",
    "    df.dropna(subset=[\"sim_to_parent_llama\"])\n",
    "      .groupby([\"operator\",\"generation\"])[\"sim_to_parent_llama\"]\n",
    "      .agg(count    =\"count\",\n",
    "           mean     =\"mean\",\n",
    "           median   =\"median\",\n",
    "           std      =\"std\",\n",
    "           q25      =lambda x: x.quantile(0.25),\n",
    "           q75      =lambda x: x.quantile(0.75),\n",
    "           min      =\"min\",\n",
    "           max      =\"max\",\n",
    "           pct_lt_0_8 =lambda x: (x < 0.8).mean() )\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "# Optional pivot for heatâ€‘map (mean similarity)\n",
    "pivot_mean = sim_stats_gen.reset_index().pivot(\n",
    "    index=\"operator\", columns=\"generation\", values=\"mean\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3B.  Add a global \"ALL\" row to sim_stats\n",
    "# -----------------------------------------------------------\n",
    "global_row = (df[\"sim_to_parent_llama\"]\n",
    "                .describe(percentiles=[0.25,0.75])\n",
    "                .rename({\"25%\":\"q25\",\"75%\":\"q75\"})\n",
    "                .reindex([\"count\",\"mean\",\"median\",\"std\",\"q25\",\"q75\",\"min\",\"max\"])\n",
    "                .round(3)\n",
    "             )\n",
    "global_row[\"pct_lt_0_8\"] = (df[\"sim_to_parent_llama\"] < 0.8).mean().round(3)\n",
    "sim_stats_full = (sim_stats_gen.groupby(\"operator\")\n",
    "                                    .first()        # grab any row to keep order\n",
    "                                    .drop(columns=sim_stats_gen.columns, errors=\"ignore\")\n",
    "                 )  # placeholder to concatenate\n",
    "sim_stats_full = pd.concat([sim_stats, global_row.to_frame().T.assign(operator=\"ALL\")])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4.  Save with timestamp\n",
    "# -----------------------------------------------------------\n",
    "ts = dt.datetime.now(dt.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv(OUT_DIR / f\"prompt_similarity_llama_{ts}.csv\", index=False)\n",
    "# sim_stats.to_csv(OUT_DIR / f\"operator_similarity_llama_{ts}.csv\")\n",
    "\n",
    "# print(\"âœ“ Saved:\")\n",
    "# print(f\"  perâ€‘prompt â†’ {OUT_DIR / f'prompt_similarity_llama_{ts}.csv'}\")\n",
    "# print(f\"  perâ€‘operator â†’ {OUT_DIR / f'operator_similarity_llama_{ts}.csv'}\")\n",
    "sim_stats_gen.to_csv(OUT_DIR / f\"operator_gen_similarity_{ts}.csv\")\n",
    "pivot_mean.to_csv(OUT_DIR / f\"operator_gen_pivot_mean_{ts}.csv\")\n",
    "sim_stats_full.to_csv(OUT_DIR / f\"operator_similarity_all_{ts}.csv\")\n",
    "\n",
    "print(\"  perâ€‘opÃ—gen  â†’\", OUT_DIR / f\"operator_gen_similarity_{ts}.csv\")\n",
    "print(\"  pivot mean  â†’\", OUT_DIR / f\"operator_gen_pivot_mean_{ts}.csv\")\n",
    "print(\"  ALLâ€‘row     â†’\", OUT_DIR / f\"operator_similarity_all_{ts}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2067bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 5.  Visualisations\n",
    "# -----------------------------------------------------------\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# â€” 5A. Bar chart of mean similarity per operator â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "mean_per_op = (\n",
    "    df.dropna(subset=[\"sim_to_parent_llama\"])\n",
    "      .groupby(\"operator\")[\"sim_to_parent_llama\"].mean()\n",
    "      .sort_values()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "mean_per_op.plot(kind=\"barh\", color=\"steelblue\")\n",
    "plt.xlabel(\"Mean cosine similarity\")\n",
    "plt.title(\"Semantic Drift per Operator (lower = farther)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â€” 5B. Heatâ€‘map: mean similarity by generation â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(pivot_mean, annot=True, cmap=\"rocket_r\", vmin=0, vmax=1)\n",
    "plt.title(\"Mean Similarity (Operator Ã— Generation)\")\n",
    "plt.ylabel(\"Operator\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â€” 5C. Boxâ€‘plot of full similarity distribution (optional) â€”\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(\n",
    "    data=df.dropna(subset=[\"sim_to_parent_llama\"]),\n",
    "    x=\"operator\", y=\"sim_to_parent_llama\", palette=\"pastel\"\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Cosine similarity to parent\")\n",
    "plt.title(\"Distribution of Semantic Drift by Operator\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Build master similarity table  (adds min / max)\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd, datetime as dt\n",
    "\n",
    "# ensure df has sim_to_parent_llama\n",
    "\n",
    "STAT_COLS = dict(\n",
    "    count  =\"count\",\n",
    "    mean   =\"mean\",\n",
    "    median =\"median\",\n",
    "    std    =\"std\",\n",
    "    min    =\"min\",\n",
    "    max    =\"max\",\n",
    ")\n",
    "\n",
    "# 1. Perâ€‘operator Ã— generation ------------------------------------------\n",
    "op_gen = (\n",
    "    df.dropna(subset=[\"sim_to_parent_llama\"])\n",
    "      .groupby([\"operator\",\"generation\"])[\"sim_to_parent_llama\"]\n",
    "      .agg(**STAT_COLS)\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# 2. Perâ€‘operator (ALL generations) --------------------------------------\n",
    "op_total = (\n",
    "    df.dropna(subset=[\"sim_to_parent_llama\"])\n",
    "      .groupby(\"operator\")[\"sim_to_parent_llama\"]\n",
    "      .agg(**STAT_COLS)\n",
    "      .reset_index()\n",
    "      .assign(generation=\"ALL\")\n",
    ")\n",
    "\n",
    "# 3. Global TOTAL row ----------------------------------------------------\n",
    "global_total = (\n",
    "    df[\"sim_to_parent_llama\"].dropna()\n",
    "      .agg(**STAT_COLS)\n",
    "      .to_frame()\n",
    "      .T\n",
    "      .assign(operator=\"ALL\", generation=\"ALL\")\n",
    ")\n",
    "\n",
    "# 4. Combine & round -----------------------------------------------------\n",
    "master_table = (\n",
    "    pd.concat([op_gen, op_total, global_total], ignore_index=True)\n",
    "      .sort_values([\"operator\",\"generation\"])\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "display(master_table)\n",
    "\n",
    "# 5. Save with timestamp -------------------------------------------------\n",
    "ts = dt.datetime.now(dt.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "outfile = OUT_DIR / f\"semantic_similarity_master_{ts}.csv\"\n",
    "master_table.to_csv(outfile, index=False)\n",
    "print(\"âœ“ Master similarity table saved â†’\", outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "# â”€â”€ 1.  Prepare perâ€‘operator totals (generation == \"ALL\") â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plot_df = (master_table\n",
    "           .query(\"generation == 'ALL' and operator != 'ALL'\")\n",
    "           .set_index(\"operator\")\n",
    "           .loc[:, [\"mean\",\"min\",\"max\"]]\n",
    "           .sort_values(\"mean\"))          # ascending so lowest at top\n",
    "\n",
    "ops   = plot_df.index\n",
    "means = plot_df[\"mean\"]\n",
    "mins  = plot_df[\"min\"]\n",
    "maxs  = plot_df[\"max\"]\n",
    "\n",
    "# asymmetric errors\n",
    "errors = [means - mins, maxs - means]\n",
    "\n",
    "# â”€â”€ 2.  Pick a qualitative palette (one colour per bar) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "palette = sns.color_palette(\"Set2\", n_colors=len(ops))  # or \"colorblind\", \"tab10\"\n",
    "colors  = dict(zip(ops, palette))\n",
    "\n",
    "# â”€â”€ 3.  Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(6, 3))   # adjust width/height to taste\n",
    "bars = ax.barh(\n",
    "    ops, means, xerr=errors, align=\"center\",\n",
    "    capsize=4, ecolor=\"black\",\n",
    "    color=[colors[o] for o in ops]\n",
    ")\n",
    "\n",
    "# â”€â”€ 4.  Annotate min / max neatly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for y, mean, lo, hi in zip(ops, means, mins, maxs):\n",
    "    ax.text(lo - 0.02, y, f\"{lo:.3f}\", ha=\"right\", va=\"center\", fontsize=9)\n",
    "    ax.text(hi + 0.02, y, f\"{hi:.3f}\", ha=\"left\",  va=\"center\", fontsize=9)\n",
    "\n",
    "# â”€â”€ 5.  Styling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax.set_xlabel(r\"\\textbf{Cosine similarity to parent}\", labelpad=6)\n",
    "ax.set_title(r\"\\textbf{Semantic Drift per Operator}\", pad=10)\n",
    "ax.invert_yaxis()                           # â€œfartherâ€ operator at top\n",
    "ax.xaxis.grid(True, linestyle=\"--\", alpha=.5)\n",
    "ax.set_xlim(0.0, 1.05)                      # leave space for max labels\n",
    "\n",
    "sns.despine(ax=ax, left=True, top=True, right=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUT_DIR / \"fig_drift_per_operator.pdf\")\n",
    "fig.savefig(OUT_DIR / \"fig_drift_per_operator.png\", dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba12121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------\n",
    "# # Beautiful horizontal rangeâ€‘bar chart  (min / mean / max)\n",
    "# # -----------------------------------------------------------\n",
    "# import matplotlib.pyplot as plt, seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# # 1.  Pull min / mean / max for operator totals\n",
    "# stats = (master_table\n",
    "#          .query(\"generation == 'ALL' and operator != 'ALL'\")\n",
    "#          .set_index(\"operator\")[[\"min\",\"mean\",\"max\"]]\n",
    "#          .sort_values(\"mean\"))\n",
    "\n",
    "# ops   = stats.index.to_list()\n",
    "# mins  = stats[\"min\"].values\n",
    "# means = stats[\"mean\"].values\n",
    "# maxs  = stats[\"max\"].values\n",
    "\n",
    "# # 2.  Build the plot\n",
    "# sns.set_theme(style=\"white\", font_scale=1.2, context=\"paper\")\n",
    "# plt.rcParams.update({\"text.usetex\": True, \"font.family\": \"serif\"})\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5,3))\n",
    "\n",
    "# y = np.arange(len(ops))\n",
    "\n",
    "# # Grey range bars  (min â†’ max)\n",
    "# ax.hlines(y, mins, maxs, color=\"lightgrey\", linewidth=6, zorder=1)\n",
    "\n",
    "# # Coloured bars for mean\n",
    "# palette = sns.color_palette(\"viridis_r\", len(ops))\n",
    "# for i, (m, col) in enumerate(zip(means, palette)):\n",
    "#     ax.barh(y[i], m, color=col, height=0.5, zorder=2)\n",
    "\n",
    "# # Labels & styling\n",
    "# ax.set_yticks(y)\n",
    "# ax.set_yticklabels([rf\"\\textbf{{{op}}}\" for op in ops])\n",
    "# ax.set_xlabel(r\"\\textbf{Cosine similarity to parent}\")\n",
    "# ax.set_title(r\"\\textbf{Semantic Drift per Operator}\", pad=10)\n",
    "# ax.set_xlim(0, 1)\n",
    "# sns.despine(ax=ax, left=True)\n",
    "# ax.grid(axis=\"x\", linestyle=\"--\", alpha=.3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "# # 3.  Save\n",
    "# fig.savefig(OUT_DIR / \"fig_drift_rangebar.pdf\")\n",
    "# fig.savefig(OUT_DIR / \"fig_drift_rangebar.png\", dpi=600, transparent=True)\n",
    "\n",
    "# print(\"Saved rangeâ€‘bar figure â†’\", OUT_DIR / \"fig_drift_rangebar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0773641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Betterâ€‘scaled heatâ€‘map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "# Compute dynamic limits\n",
    "vmin = pivot_mean.min().min() - 0.005   # min minus a small margin\n",
    "vmax = pivot_mean.max().max()\n",
    "\n",
    "# Choose a palette that has contrast near the top end\n",
    "cmap = sns.color_palette(\"crest\", as_cmap=True)   # or \"flare_r\", \"crest\", \"mako_r\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 2.8))\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_mean,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=cmap,\n",
    "    vmin=vmin, vmax=vmax,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={\"label\": \"Mean cosine similarity\"},\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"\\textbf{Generation}\", labelpad=6)\n",
    "ax.set_ylabel(r\"\\textbf{Operator}\",  labelpad=6)\n",
    "ax.set_title(r\"\\textbf{Semantic Drift Across Generations}\", pad=10)\n",
    "\n",
    "for spine in (\"top\", \"right\", \"left\", \"bottom\"):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUT_DIR / \"fig_drift_heatmap_scaled.pdf\")\n",
    "fig.savefig(OUT_DIR / \"fig_drift_heatmap_scaled.png\", dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceca5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, seaborn as sns, numpy as np\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\")  # â€˜paperâ€™ keeps fonts small\n",
    "\n",
    "plot_df = df.dropna(subset=[\"sim_to_parent_llama\"]).copy()\n",
    "op_order = (plot_df.groupby(\"operator\")[\"sim_to_parent_llama\"]\n",
    "                      .median()\n",
    "                      .sort_values()\n",
    "                      .index)\n",
    "\n",
    "palette = sns.color_palette(\"Set2\", len(op_order))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 2.8))           # slightly shorter height\n",
    "\n",
    "# Violin layer\n",
    "sns.violinplot(\n",
    "    data=plot_df, x=\"operator\", y=\"sim_to_parent_llama\",\n",
    "    order=op_order, palette=palette, cut=0, inner=None, ax=ax\n",
    ")\n",
    "\n",
    "# Strip layer (lighter + smaller)\n",
    "sns.stripplot(\n",
    "    data=plot_df.sample(frac=0.15, random_state=42),\n",
    "    x=\"operator\", y=\"sim_to_parent_llama\",\n",
    "    order=op_order, color=\"k\", alpha=0.25, size=1, jitter=0.25, ax=ax\n",
    ")\n",
    "\n",
    "# Median markers\n",
    "med = plot_df.groupby(\"operator\")[\"sim_to_parent_llama\"].median().reindex(op_order)\n",
    "ax.scatter(np.arange(len(op_order)), med.values,\n",
    "           marker=\"s\", color=\"white\", edgecolor=\"black\", s=30, zorder=3,\n",
    "           label=None)\n",
    "\n",
    "# â”€â”€ FONT & LAYOUT TWEAKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SMALL  = 9\n",
    "MEDIUM = 10\n",
    "ax.set_xlabel(r\"\\textbf{Operator}\",           fontsize=SMALL, labelpad=6)\n",
    "ax.set_ylabel(r\"\\textbf{Cosine similarity}\",  fontsize=SMALL, labelpad=6)\n",
    "ax.set_title(r\"\\textbf{Semantic Drift Distribution}\", fontsize=11, pad=8)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=25, ha=\"right\", fontsize=SMALL)\n",
    "ax.tick_params(axis=\"y\", labelsize=SMALL)\n",
    "\n",
    "ax.set_ylim(0.8, 1.01)\n",
    "ax.legend(frameon=False, fontsize=SMALL, loc=\"lower right\")\n",
    "\n",
    "sns.despine(ax=ax, left=False)\n",
    "\n",
    "plt.tight_layout(pad=0.3)\n",
    "fig.savefig(OUT_DIR / \"fig_drift_distribution_tight.pdf\")\n",
    "fig.savefig(OUT_DIR / \"fig_drift_distribution_tight.png\", dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2526aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2, context=\"paper\")\n",
    "\n",
    "# Use LaTeX fonts if texlive is available\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"figure.dpi\": 300,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(4.5,3))\n",
    "# mean_per_op = (\n",
    "#     master_table.query(\"generation == 'ALL' and operator != 'ALL'\")\n",
    "#                 .set_index(\"operator\")[\"mean\"]\n",
    "#                 .sort_values()\n",
    "# )\n",
    "# sns.barplot(x=mean_per_op.values, y=mean_per_op.index,\n",
    "#             palette=\"rocket_r\", ax=ax)\n",
    "\n",
    "# ax.set_xlabel(r\"\\textbf{Mean cosine similarity}\")\n",
    "# ax.set_ylabel(r\"\\textbf{Operator}\")\n",
    "# ax.set_title(r\"\\textbf{Semantic Drift (lower $\\rightarrow$ farther)}\", pad=12)\n",
    "# sns.despine(ax=ax, left=True)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(OUT_DIR / \"fig_mean_drift.pdf\")\n",
    "# fig.savefig(OUT_DIR / \"fig_mean_drift.png\", dpi=600, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(4.5,3))\n",
    "# sns.heatmap(pivot_mean, annot=True, cmap=\"rocket_r\",\n",
    "#             vmin=0, vmax=1, linewidths=.5, cbar_kws={\"label\": \"Mean similarity\"},\n",
    "#             fmt=\".2f\", ax=ax)\n",
    "\n",
    "# ax.set_xlabel(r\"\\textbf{Generation}\")\n",
    "# ax.set_ylabel(r\"\\textbf{Operator}\")\n",
    "# ax.set_title(r\"\\textbf{Drift Across Generations}\", pad=12)\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(OUT_DIR / \"fig_heatmap_drift.pdf\")\n",
    "# fig.savefig(OUT_DIR / \"fig_heatmap_drift.png\", dpi=600, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.5,3))\n",
    "sns.boxplot(data=df.dropna(subset=[\"sim_to_parent_llama\"]),\n",
    "            x=\"operator\", y=\"sim_to_parent_llama\",\n",
    "            palette=\"pastel\", fliersize=1, ax=ax)\n",
    "\n",
    "ax.set_xlabel(r\"\\textbf{Operator}\")\n",
    "ax.set_ylabel(r\"\\textbf{Cosine similarity to parent}\")\n",
    "ax.set_title(r\"\\textbf{Distribution of Semantic Drift}\", pad=12)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "sns.despine(ax=ax)\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_DIR / \"fig_box_drift.pdf\")\n",
    "fig.savefig(OUT_DIR / \"fig_box_drift.png\", dpi=600, transparent=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Performance & Quality Metrics Dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics dashboard\n",
    "def create_performance_dashboard(df):\n",
    "    \"\"\"Create a comprehensive performance dashboard\"\"\"\n",
    "    \n",
    "    # 1. Operator Performance Comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Violence scores by operator\n",
    "    if 'score_violence' in df.columns:\n",
    "        df.boxplot(column='score_violence', by='operator', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Violence Scores by Operator')\n",
    "        axes[0,0].set_xlabel('Operator')\n",
    "        \n",
    "    # Generation distribution\n",
    "    gen_counts = df.groupby(['operator', 'generation']).size().unstack(fill_value=0)\n",
    "    gen_counts.plot(kind='bar', stacked=True, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Generation Distribution by Operator')\n",
    "    axes[0,1].legend(title='Generation', bbox_to_anchor=(1.05, 1))\n",
    "    \n",
    "    # Success rate (completed vs pending)\n",
    "    status_counts = df.groupby('operator')['status'].apply(lambda x: (x == 'complete').mean())\n",
    "    status_counts.plot(kind='bar', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Completion Rate by Operator')\n",
    "    axes[1,0].set_ylabel('Completion Rate')\n",
    "    \n",
    "    # Prompt length distribution\n",
    "    if 'len_clean' in df.columns:\n",
    "        for op in df['operator'].unique():\n",
    "            subset = df[df['operator'] == op]['len_clean']\n",
    "            axes[1,1].hist(subset, alpha=0.6, label=op, bins=20)\n",
    "        axes[1,1].set_title('Prompt Length Distribution')\n",
    "        axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"performance_dashboard.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run the dashboard if we have the required columns\n",
    "if not df.empty:\n",
    "    try:\n",
    "        create_performance_dashboard(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Dashboard creation failed: {e}\")\n",
    "        print(\"This might be due to missing columns in the dataset\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Export and Reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis report\n",
    "def generate_analysis_report():\n",
    "    \"\"\"Generate a comprehensive analysis report\"\"\"\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# Evolution Experiment Analysis Report\n",
    "Generated: {pd.Timestamp.now()}\n",
    "\n",
    "## Dataset Summary\n",
    "- Total Genomes: {len(df):,}\n",
    "- Unique Operators: {df['operator'].nunique()}\n",
    "- Generation Range: {df['generation'].min()} â†’ {df['generation'].max()}\n",
    "- Completion Rate: {(df['status'] == 'complete').mean():.2%}\n",
    "\n",
    "## Operator Performance\n",
    "\"\"\"\n",
    "    \n",
    "    if 'score_violence' in df.columns:\n",
    "        operator_stats = df.groupby('operator')['score_violence'].agg(['count', 'mean', 'std', 'max'])\n",
    "        report += operator_stats.to_string()\n",
    "        \n",
    "    # Save detailed CSV exports\n",
    "    exports = {\n",
    "        'full_dataset': df,\n",
    "        'operator_summary': df.groupby('operator').agg({\n",
    "            'id': 'count',\n",
    "            'generation': ['min', 'max', 'mean'],\n",
    "            'score_violence': ['mean', 'std', 'max'] if 'score_violence' in df.columns else None\n",
    "        }).round(4),\n",
    "        'generation_summary': df.groupby('generation').agg({\n",
    "            'id': 'count',\n",
    "            'operator': 'nunique',\n",
    "            'score_violence': ['mean', 'std'] if 'score_violence' in df.columns else None\n",
    "        }).round(4)\n",
    "    }\n",
    "    \n",
    "    for name, data in exports.items():\n",
    "        if data is not None:\n",
    "            filename = OUTPUT_DIR / f\"{name}_{timestamp}.csv\"\n",
    "            data.to_csv(filename)\n",
    "            print(f\"âœ“ Exported {name} to {filename}\")\n",
    "    \n",
    "    # Save report\n",
    "    report_file = OUTPUT_DIR / f\"analysis_report_{timestamp}.md\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"âœ“ Generated comprehensive report: {report_file}\")\n",
    "    return report\n",
    "\n",
    "# Generate the report\n",
    "if not df.empty:\n",
    "    try:\n",
    "        report = generate_analysis_report()\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(report[:500] + \"...\" if len(report) > 500 else report)\n",
    "    except Exception as e:\n",
    "        print(f\"Report generation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = (master_table\n",
    "               .query(\"generation == 'ALL' and operator != 'ALL'\")\n",
    "               .drop(columns=\"generation\")\n",
    "               .set_index(\"operator\")\n",
    "               .to_latex(column_format=\"lrrrrrrr\",\n",
    "                         bold_rows=True,\n",
    "                         float_format=\"%.3f\"))\n",
    "with open(OUT_DIR / \"table_similarity.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved â†’\", OUT_DIR / \"table_similarity.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def autotune_bs(texts, start=256, max_bs=1024, step=128):\n",
    "#     import time, torch\n",
    "#     sizes, times = [], []\n",
    "#     for bs in range(start, max_bs+1, step):\n",
    "#         t0 = time.time()\n",
    "#         _ = llama_embed(texts[:bs], batch_size=bs)   # function from earlier\n",
    "#         torch.mps.empty_cache()\n",
    "#         times.append(time.time()-t0)\n",
    "#         sizes.append(bs)\n",
    "#         if len(times) > 1 and times[-1] > times[-2] * 1.1:   # no speedâ€‘up\n",
    "#             return sizes[-2]          # previous bs was sweet spot\n",
    "#     return sizes[-1]\n",
    "\n",
    "# best_bs = autotune_bs(df[\"prompt\"].tolist())\n",
    "# print(\"Chosen batch size:\", best_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------\n",
    "# # Pick a variant row to inspect  (e.g. DataFrame index 1234)\n",
    "# # -----------------------------------------------------------\n",
    "# row_idx   = 1234                       # <-- change to any index you like\n",
    "# row       = dedup_df.loc[row_idx]\n",
    "\n",
    "# parent_id = row[\"parents\"][0] if row[\"parents\"] else None\n",
    "# if parent_id is None:\n",
    "#     raise ValueError(\"Selected row is an original prompt; choose a variant row.\")\n",
    "\n",
    "# # Find parent row index\n",
    "# parent_idx = dedup_df.index[dedup_df[\"id\"] == parent_id][0]\n",
    "# parent_row = dedup_df.loc[parent_idx]\n",
    "\n",
    "# print(\"Variant prompt:\", row[\"prompt\"])\n",
    "# print(\"Parent  prompt:\", parent_row[\"prompt\"])\n",
    "# print(\"Operator that produced variant:\", row[\"operator\"])\n",
    "\n",
    "# # -----------------------------------------------------------\n",
    "# # Compute / fetch embeddings\n",
    "# # -----------------------------------------------------------\n",
    "# # If you've already built `index2emb` from llama_embed earlier:\n",
    "# child_emb  = index2emb[row_idx]\n",
    "# parent_emb = index2emb[parent_idx]\n",
    "\n",
    "# # If you haven't, call embed() directly:\n",
    "# # child_emb  = embed([row[\"prompt\"]])[0]\n",
    "# # parent_emb = embed([parent_row[\"prompt\"]])[0]\n",
    "\n",
    "# # -----------------------------------------------------------\n",
    "# # Print first 10 dimensions for sanity\n",
    "# # -----------------------------------------------------------\n",
    "# print(\"\\nFirst 10 dims of parent embedding:\", parent_emb.numpy())\n",
    "# print(\"First 10 dims of child  embedding:\", child_emb.numpy())\n",
    "\n",
    "# # Cosine similarity\n",
    "# cos_sim = float(torch.dot(child_emb, parent_emb))\n",
    "# print(f\"\\nCosine similarity = {cos_sim:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
