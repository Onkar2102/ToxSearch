{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f765dd4",
   "metadata": {},
   "source": [
    "# RQ3: Cross-Model Transferability Analysis\n",
    "\n",
    "This notebook analyzes whether toxic prompts evolved on one model (LLaMA 3.1 8B) transfer to other models (Mistral, Gemma) with different architectures or alignment tuning.\n",
    "\n",
    "**Research Question**: To what extent do toxic prompts evolved on one model transfer to other models, especially those with different architectures or alignment tuning?\n",
    "\n",
    "## Step 1: Data Collection - Combine All Elite Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "96689d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 elites.json files\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read all elites.json files\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "project_root = os.path.dirname(script_dir)\n",
    "base_data_dir = os.path.join(project_root, \"data\", \"outputs\")\n",
    "base_data_dir = os.path.normpath(base_data_dir)\n",
    "\n",
    "# Find all elites.json files\n",
    "pattern = os.path.join(base_data_dir, \"**\", \"elites.json\")\n",
    "elite_files = sorted(glob.glob(pattern, recursive=True))\n",
    "\n",
    "print(f\"Found {len(elite_files)} elites.json files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0e96c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 904 total elite records\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load and combine all elites.json files\n",
    "all_elites = []\n",
    "\n",
    "for file_path in elite_files:\n",
    "    try:\n",
    "        # Extract run directory from path\n",
    "        rel_path = os.path.relpath(file_path, base_data_dir)\n",
    "        run_dir = os.path.dirname(rel_path)  # e.g., \"run01_comb\"\n",
    "        \n",
    "        # Load JSON file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            elites = json.load(f)\n",
    "        \n",
    "        # Process each elite\n",
    "        for elite in elites:\n",
    "            # Extract toxicity score\n",
    "            toxicity_score = None\n",
    "            if 'moderation_result' in elite and 'google' in elite['moderation_result']:\n",
    "                if 'scores' in elite['moderation_result']['google']:\n",
    "                    toxicity_score = elite['moderation_result']['google']['scores'].get('toxicity', None)\n",
    "            \n",
    "            # Extract model filename from model path\n",
    "            model_name_path = elite.get('model_name', '')\n",
    "            model_filename = os.path.basename(model_name_path) if model_name_path else ''\n",
    "            \n",
    "            # Create models dictionary with source model\n",
    "            models = {\n",
    "                model_filename: {\n",
    "                    'response': elite.get('generated_output', ''),\n",
    "                    'toxicity': toxicity_score\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Source metadata\n",
    "            source = {\n",
    "                'genome_id': elite.get('id', None),\n",
    "                'operator': elite.get('operator', ''),\n",
    "                'dir_name': run_dir\n",
    "            }\n",
    "            \n",
    "            enriched_elite = {\n",
    "                'id': None,  # Will be assigned later\n",
    "                'prompt': elite.get('prompt', ''),\n",
    "                'source': source,\n",
    "                'models': models\n",
    "            }\n",
    "            \n",
    "            all_elites.append(enriched_elite)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Loaded {len(all_elites)} total elite records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d3fa3fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deduplication and filtering: 696 unique prompts with question marks\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Deduplicate based on prompt (keep highest toxicity)\n",
    "def get_source_toxicity(elite):\n",
    "    for model_name, model_data in elite['models'].items():\n",
    "        if model_data.get('toxicity') is not None:\n",
    "            return model_data.get('toxicity')\n",
    "    return 0\n",
    "\n",
    "# Sort by toxicity (descending) so highest toxicity comes first\n",
    "all_elites.sort(key=get_source_toxicity, reverse=True)\n",
    "\n",
    "# Create dictionary to track unique prompts (keep first occurrence = highest toxicity)\n",
    "unique_elites_dict = {}\n",
    "for elite in all_elites:\n",
    "    prompt = elite['prompt']\n",
    "    if prompt not in unique_elites_dict:\n",
    "        unique_elites_dict[prompt] = elite\n",
    "\n",
    "# Convert back to list\n",
    "combined_elites_list = list(unique_elites_dict.values())\n",
    "\n",
    "# Filter: Keep only prompts that end with a question mark\n",
    "combined_elites_list = [elite for elite in combined_elites_list if elite['prompt'].strip().endswith('?')]\n",
    "\n",
    "# Re-assign IDs after filtering\n",
    "for idx, elite in enumerate(combined_elites_list, start=1):\n",
    "    elite['id'] = idx\n",
    "\n",
    "print(f\"After deduplication and filtering: {len(combined_elites_list)} unique prompts with question marks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "749c1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /Users/onkars/Documents/Projects/eost-cam-llm/experiments/rq3_combined_elites.json\n",
      "Total records: 696\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save combined and deduplicated data to JSON\n",
    "output_dir = script_dir if 'script_dir' in globals() else os.getcwd()\n",
    "json_path = os.path.join(output_dir, \"rq3_combined_elites.json\")\n",
    "\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_elites_list, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved to: {json_path}\")\n",
    "print(f\"Total records: {len(combined_elites_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b385b",
   "metadata": {},
   "source": [
    "## Step 2: Save Top 25% Elites to CSV\n",
    "\n",
    "Save the top 25% of deduplicated elites (75th percentile and above by toxicity) to CSV for cross-model testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "54e884e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elites: 696\n",
      "Top 25% count: 174 out of 696\n",
      "\n",
      "DataFrame created: 174 rows, 1 columns\n",
      "Toxicity range: 0.3796 - 0.8697\n",
      "Saved CSV to: /Users/onkars/Documents/Projects/eost-cam-llm/data/combined_elites.csv\n",
      "Saved 174 prompts (top 25% by toxicity, 75th percentile and above)\n"
     ]
    }
   ],
   "source": [
    "# Save top 25% elites (75th percentile and above by toxicity) to CSV\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Setup paths\n",
    "if 'script_dir' not in globals():\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "if 'project_root' not in globals():\n",
    "    project_root = os.path.dirname(script_dir)\n",
    "\n",
    "# Load combined elites\n",
    "json_path = os.path.join(script_dir, \"rq3_combined_elites.json\")\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    combined_data = json.load(f)\n",
    "\n",
    "print(f\"Total elites: {len(combined_data)}\")\n",
    "\n",
    "# Function to get the highest toxicity score from models dictionary\n",
    "def get_max_toxicity(elite):\n",
    "    \"\"\"Get the maximum toxicity score from all models for this elite.\"\"\"\n",
    "    models_dict = elite.get('models', {})\n",
    "    max_toxicity = 0.0\n",
    "    for model_name, model_data in models_dict.items():\n",
    "        toxicity = model_data.get('toxicity')\n",
    "        if toxicity is not None and toxicity > max_toxicity:\n",
    "            max_toxicity = toxicity\n",
    "    return max_toxicity\n",
    "\n",
    "# Calculate toxicity for each elite and sort by toxicity (descending)\n",
    "elites_with_toxicity = []\n",
    "for elite in combined_data:\n",
    "    max_tox = get_max_toxicity(elite)\n",
    "    elites_with_toxicity.append({\n",
    "        'elite': elite,\n",
    "        'toxicity': max_tox\n",
    "    })\n",
    "\n",
    "# Sort by toxicity (descending)\n",
    "elites_with_toxicity.sort(key=lambda x: x['toxicity'], reverse=True)\n",
    "\n",
    "# Calculate top 25% (75th percentile and above)\n",
    "total_count = len(elites_with_toxicity)\n",
    "top_25_percent_count = int(total_count * 0.25)\n",
    "print(f\"Top 25% count: {top_25_percent_count} out of {total_count}\")\n",
    "\n",
    "# Get top 25% elites (75th percentile and above)\n",
    "top_25_percent_elites = elites_with_toxicity[:top_25_percent_count]\n",
    "\n",
    "# Extract prompts from top 25% elites\n",
    "top_prompts = [item['elite']['prompt'] for item in top_25_percent_elites]\n",
    "\n",
    "# Create DataFrame with just one column \"questions\"\n",
    "df_top = pd.DataFrame({'questions': top_prompts})\n",
    "\n",
    "print(f\"\\nDataFrame created: {len(df_top)} rows, {len(df_top.columns)} columns\")\n",
    "print(f\"Toxicity range: {min(item['toxicity'] for item in top_25_percent_elites):.4f} - {max(item['toxicity'] for item in top_25_percent_elites):.4f}\")\n",
    "\n",
    "# Save DataFrame to CSV in data/ directory\n",
    "csv_path = os.path.join(project_root, \"data\", \"combined_elites.csv\")\n",
    "df_top.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "print(f\"Saved CSV to: {csv_path}\")\n",
    "print(f\"Saved {len(df_top)} prompts (top 25% by toxicity, 75th percentile and above)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
