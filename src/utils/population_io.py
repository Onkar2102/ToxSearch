"""
Comprehensive Population I/O Utility

This module provides unified population loading, saving, and management functionality
that automatically handles both monolithic and split file formats for optimal memory efficiency.
"""

from typing import List, Dict, Any, Optional, Union
import os
import json
from pathlib import Path
from collections import defaultdict
import time
from utils import get_custom_logging
from utils.constants import EvolutionConstants, FileConstants
from utils import get_population_io
from gne import get_ResponseGenerator, get_PromptGenerator
from datetime import datetime

import pandas as pd

get_logger, _, _, PerformanceLogger = get_custom_logging()



def get_project_root():
    """Get the absolute path to the project root directory"""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    return project_root.resolve()

def get_config_path():
    """Get the absolute path to the config directory"""
    return get_project_root() / "config"

def get_data_path():
    """Get the absolute path to the data directory"""
    return get_project_root() / "data" / "prompt.csv"

# Global variable to store the outputs path for the current run
_current_outputs_path = None

def get_outputs_path():
    """Get the absolute path to the outputs directory"""
    global _current_outputs_path
    
    if _current_outputs_path is not None:
        return _current_outputs_path
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    outputs_dir = get_project_root() / "data" / "outputs" / timestamp
    
    outputs_dir.mkdir(parents=True, exist_ok=True)
    
    _current_outputs_path = outputs_dir
    
    return outputs_dir

def _extract_north_star_score(genome, metric="toxicity"):
    """Extract the north star metric score from a genome.
    
    Priority order:
    1. moderation_result.google.scores[metric] (standard format)
    2. moderation_result.scores[metric] (legacy format)
    3. Direct field: metric (e.g., "toxicity" for top_10.json slimmed format)
    4. scores[metric] (nested field)
    
    Returns minimum 0.0001 for consistency across the project.
    """
    if not genome:
        return 0.0001
    
    # Priority 1: moderation_result.google.scores[metric] (standard format)
    if "moderation_result" in genome:
        moderation_result = genome["moderation_result"]
        
        # Skip if moderation_result is None
        if moderation_result is None:
            pass  # Fall through to other priorities
        elif "google" in moderation_result:
            google_scores = moderation_result["google"]
            if google_scores is not None and "scores" in google_scores:
                score = google_scores["scores"].get(metric, 0.0001)
                if score is not None and score > 0:
                    return round(float(score), 4)
        
        # Priority 2: moderation_result.scores[metric] (legacy format)
        if moderation_result is not None and "scores" in moderation_result:
            score = moderation_result["scores"].get(metric, 0.0001)
            if score is not None and score > 0:
                return round(float(score), 4)
    
    # Priority 3: Direct field (e.g., "toxicity" for top_10.json slimmed format)
    if metric in genome:
        score = genome.get(metric)
        if score is not None and score > 0:
            return round(float(score), 4)
    
    # Priority 4: scores[metric] (nested field)
    if "scores" in genome and isinstance(genome["scores"], dict):
        score = genome["scores"].get(metric, 0.0001)
        if score is not None and score > 0:
            return round(float(score), 4)
    
    return 0.0001



def initialize_system(logger, log_file, seed_file="data/prompt.csv"):
    """Initialize the system components and create gen0 if needed
    
    Args:
        logger: Logger instance
        log_file: Log file path
        seed_file: Path to CSV file with seed prompts (must have 'questions' column).
                   Default: data/prompt.csv
    """
    from utils.device_utils import device_manager
    device = device_manager.get_optimal_device()
    
    logger.debug("Initializing pipeline for device: %s", device)
    
    population_io_functions = get_population_io()
    
    load_and_initialize_population, get_population_files_info, load_population, save_population, sort_population_json, load_genome_by_id, consolidate_generations_to_single_file, migrate_from_split_to_single, sort_population_by_elite_criteria, load_elites, save_elites, get_population_stats_steady_state = get_population_io()
    
    ResponseGenerator = get_ResponseGenerator()
    response_generator = ResponseGenerator(model_key="response_generator", config_path="config/RGConfig.yaml", log_file=log_file)
    logger.debug("Response generator initialized")
    
    PromptGenerator = get_PromptGenerator()
    prompt_generator = PromptGenerator(model_key="prompt_generator", config_path="config/PGConfig.yaml", log_file=log_file)
    logger.debug("Prompt generator initialized")
    
    from ea.evolution_engine import set_global_generators
    set_global_generators(response_generator, prompt_generator)
    logger.debug("Global generators set")
    
    population_file = get_outputs_path() / "elites.json"
    
    population_content = None
    if not population_file.exists():
        should_initialize = True
    else:
        try:
            with open(population_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            should_initialize = len(content) == 0 or content == '[]'
            if not should_initialize:
                import json
                population_content = json.loads(content)
        except Exception:
            should_initialize = True

    if should_initialize:
        try:
            seed_path = Path(seed_file)
            if not seed_path.is_absolute():
                seed_path = get_project_root() / seed_path
            input_path = str(seed_path)
            logger.info("Initializing population from seed file: %s", input_path)
            
            load_and_initialize_population(
                input_path=input_path,
                output_path=str(get_outputs_path()),
                log_file=log_file
            )
            logger.debug("Population successfully initialized and saved.")
        except Exception as e:
            logger.error("Failed to initialize population: %s", e, exc_info=True)
            raise
    else:
        logger.info("Existing elites file found. Skipping initialization.")
        try:
            population = population_content if population_content is not None else []
            logger.info("Loaded %d genomes from existing elites.json", len(population))
            generations = set(g.get("generation", 0) for g in population if g)
            logger.debug("Available generations: %s", sorted(generations))
        except Exception as e:
            logger.warning("Could not read existing population info: %s", e)
    
    return response_generator, prompt_generator


def clean_population(population: List[Dict[str, Any]], *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """Remove None genomes and invalid entries from population.
    
    Parameters
    ----------
    population : List[Dict[str, Any]]
        Population to clean.
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created.
    log_file : str | None
        Optional log-file path when a new logger is created.
        
    Returns
    -------
    List[Dict[str, Any]]
        Cleaned population with None genomes removed.
    """
    _logger = logger or get_logger("population_io", log_file)
    
    original_count = len(population)
    cleaned_population = [g for g in population if g is not None]
    removed_count = original_count - len(cleaned_population)
    
    if removed_count > 0:
        _logger.warning("Removed %d None genomes from population", removed_count)
    
    _logger.info("Population cleaned: %d → %d genomes", original_count, len(cleaned_population))
    return cleaned_population



def get_population_files_info(base_dir: str = "outputs") -> Dict[str, Any]:
    """Get information about population files including elites.json and reserves.json
    
    Note: Active population = elites.json + reserves.json (archive.json is NOT part of population).
    """
    
    base_path = Path(base_dir).resolve()
    # Use reserves.json as default population file
    # Note: We only maintain elites (in species or reserves). Active population = elites + reserves.
    population_file = base_path / "reserves.json"
    elites_file = base_path / "elites.json"
    evolution_tracker_file = base_path / "EvolutionTracker.json"
    
    info = {
        "total_generations": 0,
        "generation_counts": {},
    }
    
    # Try to get metadata from EvolutionTracker.json first
    if evolution_tracker_file.exists():
        try:
            with open(evolution_tracker_file, 'r', encoding='utf-8') as f:
                tracker = json.load(f)
            
            # Calculate total_generations from the actual generations array
            # This ensures it's always up-to-date with the actual generation count
            if "generations" in tracker and tracker["generations"]:
                # Get the maximum generation number from the generations array
                max_gen_num = max(gen.get("generation_number", 0) for gen in tracker["generations"])
                info["total_generations"] = max_gen_num + 1  # +1 because generation 0 counts as 1 generation
            else:
                # Fallback: use tracker value or 0 if no generations exist
                info["total_generations"] = tracker.get("total_generations", 0)
            
            return info
                
        except Exception as e:
            # Fall back to file scanning if tracker read fails
            pass
    
    # Count genomes in reserves.json (fallback if EvolutionTracker not available)
    if population_file.exists():
        try:
            with open(population_file, 'r', encoding='utf-8') as f:
                population = json.load(f)
            
            
            # Count genomes by generation
            for genome in population:
                if genome and "generation" in genome:
                    gen_num = str(genome["generation"])
                    info["generation_counts"][gen_num] = info["generation_counts"].get(gen_num, 0) + 1
            
        except Exception as e:
            # Silently fail if we can't read the file
            pass
    
    # Count genomes in elites.json
    if elites_file.exists():
        try:
            with open(elites_file, 'r', encoding='utf-8') as f:
                elites = json.load(f)
            
            
            # Add elite genomes to generation counts
            for genome in elites:
                if genome and "generation" in genome:
                    gen_num = str(genome["generation"])
                    info["generation_counts"][gen_num] = info["generation_counts"].get(gen_num, 0) + 1
            
        except Exception as e:
            # Silently fail if we can't read the file
            pass
    
    # Calculate total genomes and generations
    
    # Ensure all generations from 0 to max are represented, even if they have 0 variants
    if info["generation_counts"]:
        max_generation = max(int(k) for k in info["generation_counts"].keys())
        # Fill in missing generations with 0 counts
        for gen_num in range(max_generation + 1):
            if str(gen_num) not in info["generation_counts"]:
                info["generation_counts"][str(gen_num)] = 0
        info["total_generations"] = max_generation + 1
    else:
        info["total_generations"] = 0
    
    return info


def update_population_index_single_file(base_dir: str, total_genomes: int, *, logger=None, log_file: Optional[str] = None):
    """Update the population metadata in EvolutionTracker.json for single file mode"""
    
    _logger = logger or get_logger("update_population_index", log_file)
    
    try:
        base_dir = str(Path(base_dir).resolve())
        info = get_population_files_info(base_dir)
        evolution_tracker_file = Path(base_dir) / "EvolutionTracker.json"
        
        if evolution_tracker_file.exists():
            try:
                with open(evolution_tracker_file, 'r', encoding='utf-8') as f:
                    tracker = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                tracker = {
                    "status": "not_complete",
                    "total_generations": 1,
                    "generations_since_improvement": 0,
                    "avg_fitness_history": [],
                    "slope_of_avg_fitness": 0.0,
                    "selection_mode": "default",
                    "generations": []
                }
        else:
                tracker = {
                    "status": "not_complete",
                    "total_generations": 1,
                    "generations_since_improvement": 0,
                    "avg_fitness_history": [],
                    "slope_of_avg_fitness": 0.0,
                    "selection_mode": "default",
                    "generations": []
                }
        
        
        tracker["total_generations"] = info["total_generations"]
        
        with open(evolution_tracker_file, 'w', encoding='utf-8') as f:
            json.dump(tracker, f, indent=2)
        
        _logger.debug("Updated EvolutionTracker population metadata: single file mode, "
                     "total_generations: %d", info['total_generations'])
        
    except Exception as e:
        _logger.warning("Failed to update EvolutionTracker population metadata: %s", e)


def load_population_generation(generation: int, base_dir: str = "outputs", 
                              *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """Load genomes for a specific generation from the single non_elites.json file"""
    
    _logger = logger or get_logger("population_io", log_file)
    
    with PerformanceLogger(_logger, f"Load Generation {generation} from Single File"):
        try:
            # Load entire population
            all_genomes = load_population(base_dir, logger=_logger, log_file=log_file)
            
            # Filter by generation
            generation_genomes = [g for g in all_genomes if g and g.get("generation") == generation]
            
            _logger.info("Loaded generation %d: %d genomes from non_elites.json", generation, len(generation_genomes))
            return generation_genomes
            
        except Exception as e:
            _logger.error("Failed to load generation %d: %s", generation, e, exc_info=True)
            return []


def load_population_range(start_gen: int, end_gen: int, base_dir: str = "outputs",
                         *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """Load multiple generations from the single non_elites.json file"""
    
    _logger = logger or get_logger("population_io", log_file)
    
    with PerformanceLogger(_logger, f"Load Generations {start_gen}-{end_gen} from Single File"):
        try:
            all_genomes = load_population(base_dir, logger=_logger, log_file=log_file)
            
            range_genomes = [g for g in all_genomes if g and start_gen <= g.get("generation", 0) <= end_gen]
            
            _logger.info("Loaded generations %d-%d: %d genomes from non_elites.json", start_gen, end_gen, len(range_genomes))
            return range_genomes
            
        except Exception as e:
            _logger.error("Failed to load generation range: %s", e, exc_info=True)
            return []


def load_population_lazy(base_dir: str = "outputs", max_gens: Optional[int] = None,
                        *, logger=None, log_file: Optional[str] = None):
    """Generator that yields genomes from the single non_elites.json file"""
    
    _logger = logger or get_logger("population_io", log_file)
    
    try:
        # Load entire population
        all_genomes = load_population(base_dir, logger=_logger, log_file=log_file)
        
        # Apply generation limit if specified
        if max_gens is not None:
            all_genomes = [g for g in all_genomes if g and g.get("generation", 0) < max_gens]
        
        _logger.info("Lazy loading %d genomes from non_elites.json", len(all_genomes))
        
        for genome in all_genomes:
            yield genome
            
    except Exception as e:
        _logger.error("Failed to load population lazily: %s", e, exc_info=True)
        return


def save_population_generation(genomes: List[Dict[str, Any]], generation: int, 
                              base_dir: str = "outputs", *, logger=None, log_file: Optional[str] = None):
    """Save genomes to the single non_elites.json file (updates the single file)"""
    
    _logger = logger or get_logger("population_io", log_file)
    
    with PerformanceLogger(_logger, f"Save Generation {generation} to Single File"):
        try:
            existing_population = load_population(base_dir, logger=_logger, log_file=log_file)
            
            filtered_population = [g for g in existing_population if g and g.get("generation") != generation]
            
            filtered_population.extend(genomes)
            
            save_population(filtered_population, base_dir, logger=_logger, log_file=log_file)
            
            _logger.info("Updated non_elites.json with generation %d: %d genomes", generation, len(genomes))
            
        except Exception as e:
            _logger.error("Failed to save generation %d: %s", generation, e, exc_info=True)
            raise


def get_latest_generation(base_dir: str = "outputs") -> int:
    """Get the highest generation number available from the single non_elites.json file"""
    info = get_population_files_info(base_dir)
    return info["total_generations"] - 1 if info["total_generations"] > 0 else 0


def load_population(pop_path: str = "data/outputs/reserves.json", *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Load population with automatic detection of split vs monolithic format
    
    If pop_path points to a JSON file and it exists, use it directly.
    Otherwise, fall back to split files if they exist.
    
    Parameters
    ----------
    pop_path : str
        Path to the population file.
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created

    Returns
    -------
    list[dict]
        Parsed population.
    """
    _logger = logger or get_logger("population_io", log_file)

    with PerformanceLogger(_logger, "Load Population", file_path=pop_path):
        try:
            pop_path_obj = Path(pop_path)
            
            if pop_path_obj.is_dir():
                base_dir = pop_path_obj
                population_file = base_dir / "reserves.json"
            else:
                population_file = pop_path_obj
                base_dir = pop_path_obj.parent
            if population_file.exists():
                if pop_path_obj.is_dir():
                    _logger.info("Using monolithic population file (preferred)")
                else:
                    _logger.info("Using specified population file: %s", pop_path)
                try:
                    with open(population_file, "r", encoding="utf-8") as f:
                        population = json.load(f)

                    population = clean_population(population, logger=_logger, log_file=log_file)
                    if pop_path_obj.is_dir():
                        _logger.info("Successfully loaded population with %d genomes", len(population))
                    else:
                        _logger.info("Successfully loaded population with %d genomes from %s", len(population), pop_path)
                    return population
                except Exception as e:
                    _logger.warning("Failed to load population file: %s, falling back to split files", e)
            
            info = get_population_files_info(str(base_dir))
            
            if info["generation_counts"]:
                _logger.info("Using single file mode with generation counts")
                all_genomes = load_population(str(base_dir), logger=_logger, log_file=log_file)
                
                all_genomes = clean_population(all_genomes, logger=_logger, log_file=log_file)
                _logger.info("Successfully loaded population with %d genomes from split files", len(all_genomes))
                return all_genomes
            else:
                if not os.path.exists(pop_path):
                    _logger.error("No population files found: neither reserves.json nor split files exist")
                    raise FileNotFoundError(f"No population files found in {base_dir}")
                else:
                    _logger.info("Using fallback population file: %s", pop_path)
                    with open(pop_path, "r", encoding="utf-8") as f:
                        population = json.load(f)

                    population = clean_population(population, logger=_logger, log_file=log_file)
                    _logger.info("Successfully loaded population with %d genomes from fallback file", len(population))
                    return population

        except Exception as e:
            _logger.error("Failed to load population: %s", e, exc_info=True)
            raise


def save_population(population: List[Dict[str, Any]], pop_path: str = "data/outputs/reserves.json", 
                   *, logger=None, log_file: Optional[str] = None, preserve_sort_order: bool = False) -> None:
    """
    Save entire population to a JSON file
    
    Parameters
    ----------
    population : List[Dict[str, Any]]
        Population to save.
    pop_path : str
        Path where to save the population (can be file or directory path).
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created.
    log_file : str | None
        Optional log-file path when a new logger is created.
    """
    _logger = logger or get_logger("population_io", log_file)

    with PerformanceLogger(_logger, "Save Population", file_path=pop_path, genome_count=len(population)):
        try:
            cleaned_population = clean_population(population, logger=_logger, log_file=log_file)
            
            pop_path_obj = Path(pop_path)
            output_file = pop_path_obj if pop_path_obj.suffix else pop_path_obj / "reserves.json"
            
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            if not preserve_sort_order:
                cleaned_population.sort(key=lambda g: (
                    g.get("generation", 0),
                    g.get("id", "0")
                ))
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_population, f, indent=2, ensure_ascii=False)
            
            size_mb = output_file.stat().st_size / (1024 * 1024)
            _logger.info("Successfully saved population to %s: %d genomes, %.2f MB", 
                        output_file.name, len(cleaned_population), size_mb)
            
            update_population_index_single_file(str(output_file.parent), len(cleaned_population), logger=_logger, log_file=log_file)
                        
        except Exception as e:
            _logger.error("Failed to save population: %s", e, exc_info=True)
            raise



def load_and_initialize_population(
    input_path: str,
    output_path: str,
    *,
    log_file: Optional[str] = None,
) -> None:
    """Load prompts from Excel file and initialize population as single non_elites.json file"""

    get_logger, _, _, PerformanceLogger = get_custom_logging()
    logger = get_logger("initialize_population", log_file)

    with PerformanceLogger(
        logger, "Initialize Population", input_path=input_path, output_path=output_path
    ):
        try:
            logger.info("Starting population initialization")
            logger.info("Input file: %s", input_path)
            logger.info("Output directory: %s", output_path)

            if not os.path.exists(input_path):
                logger.error("Input file not found: %s", input_path)
                raise FileNotFoundError(f"Input file not found: {input_path}")

            # ---------------------------- Load CSV File -----------------------
            with PerformanceLogger(logger, "Load CSV File"):
                df = pd.read_csv(input_path)
                logger.info(
                    "Successfully loaded CSV file with %d rows and %d columns",
                    len(df),
                    len(df.columns),
                )

            # -------------------------- Extract prompts --------------------
            # Only expect a "questions" column in the CSV file
            if "questions" not in df.columns:
                raise ValueError("Required 'questions' column not found in CSV file")
            
            prompt_column = "questions"
            prompts = (
                df[prompt_column].dropna().drop_duplicates().astype(str).str.strip().tolist()
            )
            logger.info("Extracted %d unique prompts from 'questions' column", len(prompts))

            # -------------------------- Create genomes ---------------------
            # Get prompt generator name if available (lazy import to avoid circular dependency)
            prompt_generator_name = None
            try:
                from ea.evolution_engine import get_prompt_generator
                pg = get_prompt_generator()
                if pg and hasattr(pg, 'model_cfg'):
                    prompt_generator_name = pg.model_cfg.get("name", "")
            except Exception:
                # Prompt generator not initialized yet, will be None
                pass

            population: List[Dict[str, Any]] = []
            for i, prompt in enumerate(prompts):
                population.append(
                    {
                        "id": i + 1,
                        "prompt": prompt,
                        "model_name": None,
                        "prompt_generator_name": prompt_generator_name,
                        "moderation_result": None,
                        "operator": None,
                        "parents": [],
                        "parent_score": None,  # null for initial genomes (no parents)
                        "generation": 0,
                        "status": "pending_generation",
                        "variant_type": "initial",  # Moved to top-level
                        "creation_info": {
                            "type": "initial",
                            "operator": "excel_import"
                        }
                    }
                )

            logger.info("Created %d genomes", len(population))

            # ----------------------------- Initialize temp.json (staging) ----------------------------
            with PerformanceLogger(logger, "Initialize temp.json (staging)"):
                temp_path = Path(output_path) / "temp.json"
                temp_path.parent.mkdir(parents=True, exist_ok=True)
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(population, f, indent=2, ensure_ascii=False)
                logger.info("Initialized temp.json with %d genomes (staging)", len(population))

            # ----------------------------- Initialize empty elites.json ----------------------------
            with PerformanceLogger(logger, "Initialize empty elites.json"):
                # elites.json starts empty
                empty_elites = []
                elites_path = Path(output_path) / "elites.json"
                elites_path.parent.mkdir(parents=True, exist_ok=True)
                with open(elites_path, 'w', encoding='utf-8') as f:
                    json.dump(empty_elites, f, indent=2, ensure_ascii=False)
                logger.info("Initialized empty elites.json")

            # ----------------------------- Initialize empty reserves.json ----------------------------
            with PerformanceLogger(logger, "Initialize empty reserves.json"):
                # reserves.json starts empty - buffer for high-fitness outliers that don't fit species
                empty_reserves = []
                reserves_path = Path(output_path) / "reserves.json"
                reserves_path.parent.mkdir(parents=True, exist_ok=True)
                with open(reserves_path, 'w', encoding='utf-8') as f:
                    json.dump(empty_reserves, f, indent=2, ensure_ascii=False)
                logger.info("Initialized empty reserves.json")
            
            # ----------------------------- Initialize empty archive.json ----------------------------
            with PerformanceLogger(logger, "Initialize empty archive.json"):
                # archive.json starts empty - stores genomes removed due to capacity limits
                empty_archive = []
                archive_path = Path(output_path) / "archive.json"
                archive_path.parent.mkdir(parents=True, exist_ok=True)
                with open(archive_path, 'w', encoding='utf-8') as f:
                    json.dump(empty_archive, f, indent=2, ensure_ascii=False)
                logger.info("Initialized empty archive.json")

            # ----------------------------- Initialize empty parents.json ----------------------------
            with PerformanceLogger(logger, "Initialize empty parents.json"):
                # parents.json starts empty
                empty_parents = []
                parents_path = Path(output_path) / "parents.json"
                parents_path.parent.mkdir(parents=True, exist_ok=True)
                with open(parents_path, 'w', encoding='utf-8') as f:
                    json.dump(empty_parents, f, indent=2, ensure_ascii=False)
                logger.info("Initialized empty parents.json")

            # ----------------------------- Initialize empty top_10.json ----------------------------
            with PerformanceLogger(logger, "Initialize empty top_10.json"):
                # top_10.json starts empty
                empty_top_10 = []
                top_10_path = Path(output_path) / "top_10.json"
                top_10_path.parent.mkdir(parents=True, exist_ok=True)
                with open(top_10_path, 'w', encoding='utf-8') as f:
                    json.dump(empty_top_10, f, indent=2, ensure_ascii=False)
                logger.info("Initialized empty top_10.json")

            # ----------------------------- Initialize EvolutionTracker ----------------------------
            with PerformanceLogger(logger, "Initialize EvolutionTracker"):
                evolution_tracker = {
                    "status": "not_complete",
                    "total_generations": 1,  # Generation 0 exists
                    "generations_since_improvement": 0,
                    "avg_fitness_history": [],
                    "slope_of_avg_fitness": 0.0,
                    "selection_mode": "default",
                    "generations": [
                        {
                            "generation_number": 0,
                            "genome_id": "1",  # Will be updated with actual best genome during threshold check
                            "max_score_variants": 0.0,  # Will be updated with actual best score during threshold check
                            "avg_fitness": 0.0,  # Will be calculated and updated
                            "parents": None,
                            "top_10": None,
                            "variants_created": None,
                            "mutation_variants": None,
                            "crossover_variants": None,
                            "operator_statistics": {}
                        }
                    ]
                }
                
                # Save EvolutionTracker
                evolution_tracker_path = Path(output_path) / "EvolutionTracker.json"
                evolution_tracker_path.parent.mkdir(parents=True, exist_ok=True)
                with open(evolution_tracker_path, 'w', encoding='utf-8') as f:
                    json.dump(evolution_tracker, f, indent=2)
                
                logger.info("Initialized global EvolutionTracker with %d genomes", len(prompts))

            logger.info("Population initialization completed successfully")

        except Exception:
            logger.exception("Population initialization failed")
            raise


def validate_population_file(population_path: str, *, log_file: Optional[str] = None) -> Dict[str, Any]:
    """Run sanity checks on a population JSON and return aggregate statistics."""

    get_logger, _, _, PerformanceLogger = get_custom_logging()
    logger = get_logger("validate_population", log_file)

    with PerformanceLogger(logger, "Validate Population File", file_path=population_path):
        population = load_population(population_path, logger=logger)

        stats: Dict[str, Any] = {
            "generations": set(),
            "statuses": {},
            "prompt_lengths": [],
            "errors": [],
        }

        for genome in population:
            for field in ("id", "prompt", "generation", "status"):
                if field not in genome:
                    stats["errors"].append(
                        f"Missing required field '{field}' in genome {genome.get('id', '?')}"
                    )

            stats["generations"].add(genome.get("generation", -1))
            status = genome.get("status", "unknown")
            stats["statuses"][status] = stats["statuses"].get(status, 0) + 1
            stats["prompt_lengths"].append(len(genome.get("prompt", "")))

        if stats["prompt_lengths"]:
            stats["avg_prompt_length"] = sum(stats["prompt_lengths"]) / len(
                stats["prompt_lengths"]
            )
            stats["min_prompt_length"] = min(stats["prompt_lengths"])
            stats["max_prompt_length"] = max(stats["prompt_lengths"])

        stats["generations"] = sorted(stats["generations"])

        logger.info("Validation complete – %d genomes analysed", len(population))
        if stats["errors"]:
            logger.warning("Found %d schema issues", len(stats["errors"]))

        return stats


def sort_population_json(
    population: Union[str, List[Dict[str, Any]]],
    sort_keys: List,
    *,
    reverse_flags: Optional[List[bool]] = None,
    output_path: Optional[str] = None,
    log_file: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """Sort population by multiple keys.

    • *population* may be a list or a path to a JSON file.
    • *sort_keys* items can be strings (direct keys) or callables.
    """

    import collections.abc as _abc

    get_logger, _, _, PerformanceLogger = get_custom_logging()
    logger = get_logger("sort_population", log_file)

    with PerformanceLogger(logger, "Sort Population JSON"):
        if isinstance(population, str):
            pop_list = load_population(population, logger=logger)
            input_is_file = True
        elif isinstance(population, _abc.Sequence):
            pop_list = list(population)
            input_is_file = False
        else:
            raise ValueError("population must be a file path or a list of genomes")

        if reverse_flags is None:
            reverse_flags = [False] * len(sort_keys)
        if len(reverse_flags) != len(sort_keys):
            raise ValueError("reverse_flags must match sort_keys in length")

        def compound_sort_key(genome):
            values = []
            for i, key in enumerate(sort_keys):
                if callable(key):
                    value = key(genome)
                else:
                    value = genome.get(key) if genome is not None else None
                
                if value is None:
                    value = float("-inf") if reverse_flags[i] else float("inf")
                
                if reverse_flags[i] and isinstance(value, (int, float)):
                    value = -value
                elif reverse_flags[i] and isinstance(value, str):
                    try:
                        int_value = int(value)
                        value = -int_value
                        logger.debug("Negated string ID %s -> %d", value, int_value)
                    except ValueError:
                        pass
                
                values.append(value)
            return tuple(values)
        
        logger.debug("Sorting population with %d keys and reverse flags: %s", len(sort_keys), reverse_flags)
        
        pop_list.sort(key=compound_sort_key)

        if output_path:
            dest = output_path
        elif isinstance(population, str):
            dest = population
        else:
            dest = None
            
        if dest:
            logger.debug("Saving sorted population to: %s", dest)
            save_population(pop_list, dest, logger=logger, preserve_sort_order=True)
            logger.info("Successfully saved sorted population to: %s", dest)

        return pop_list


def load_genome_by_id(genome_id: str, generation: int, base_dir: str = "outputs", 
                      *, logger=None, log_file: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """
    Load a specific genome by ID from a specific generation file
    
    Parameters
    ----------
    genome_id : str
        The ID of the genome to load
    generation : int
        The generation number where the genome is stored
    base_dir : str
        Base directory containing generation files
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    Dict[str, Any] | None
        The genome if found, None otherwise
    """
    _logger = logger or get_logger("population_io", log_file)
    
    with PerformanceLogger(_logger, f"Load Genome by ID", genome_id=genome_id, generation=generation):
        try:
            genomes = load_population_generation(generation, base_dir, logger=_logger, log_file=log_file)
            
            for genome in genomes:
                if genome.get("id") == genome_id:
                    _logger.info(f"Found genome {genome_id} in generation {generation}")
                    return genome
            
            _logger.warning(f"Genome {genome_id} not found in generation {generation}")
            return None
            
        except Exception as e:
            _logger.error(f"Failed to load genome {genome_id} from generation {generation}: {e}", exc_info=True)
            return None


def load_genomes_by_ids(genome_ids: List[str], generations: List[int], base_dir: str = "outputs",
                        *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Load multiple genomes by their IDs and generation numbers
    
    Parameters
    ----------
    genome_ids : List[str]
        List of genome IDs to load
    generations : List[int]
        List of generation numbers corresponding to each genome ID
    base_dir : str
        Base directory containing generation files
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    List[Dict[str, Any]]
        List of found genomes (may be shorter than input if some not found)
    """
    _logger = logger or get_logger("population_io", log_file)
    
    with PerformanceLogger(_logger, f"Load Genomes by IDs", count=len(genome_ids)):
        try:
            generation_groups = {}
            for genome_id, generation in zip(genome_ids, generations):
                if generation not in generation_groups:
                    generation_groups[generation] = []
                generation_groups[generation].append(genome_id)
            
            found_genomes = []
            
            for generation, ids_in_gen in generation_groups.items():
                genomes = load_population_generation(generation, base_dir, logger=_logger, log_file=log_file)
                
                genome_lookup = {g.get("id"): g for g in genomes}
                
                for genome_id in ids_in_gen:
                    if genome_id in genome_lookup:
                        found_genomes.append(genome_lookup[genome_id])
                    else:
                        _logger.warning(f"Genome {genome_id} not found in generation {generation}")
            
            _logger.info(f"Loaded {len(found_genomes)} out of {len(genome_ids)} requested genomes")
            return found_genomes
            
        except Exception as e:
            _logger.error(f"Failed to load genomes by IDs: {e}", exc_info=True)
            return []


def consolidate_generations_to_single_file(base_dir: str = "outputs", 
                                         output_file: str = "non_elites.json",
                                         *, logger=None, log_file: Optional[str] = None) -> bool:
    """
    Consolidate all split generation files back into a single non_elites.json file.
    
    This function merges all gen*.json files into a single non_elites.json file,
    effectively reverting from the split file architecture back to the monolithic approach.
    
    Parameters
    ----------
    base_dir : str
        Base directory containing generation files
    output_file : str
        Name of the output non_elites.json file
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    bool
        True if consolidation was successful, False otherwise
    """
    _logger = logger or get_logger("consolidate_generations", log_file)
    
    with PerformanceLogger(_logger, "Consolidate Generations to Single File"):
        try:
            base_path = Path(base_dir).resolve()
            output_path = base_path / output_file
            
            info = get_population_files_info(str(base_path))
            
            if not info["generation_counts"]:
                _logger.warning("No generation counts found to consolidate")
                return False
            
            _logger.info(f"Found {len(info['generation_counts'])} generations to consolidate")
            _logger.info(f"Population metadata updated for {len(info['generation_counts'])} generations")
            
            generation_order = sorted(info['generation_counts'].keys())
            
            all_genomes = load_population(str(base_path), logger=_logger, log_file=log_file)
            
            if not all_genomes:
                _logger.error("No genomes loaded from non_elites.json")
                return False
            
            all_genomes = clean_population(all_genomes, logger=_logger, log_file=log_file)
            
            all_genomes.sort(key=lambda g: (
                g.get("generation", 0),
                g.get("id", "0")
            ))
            
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(all_genomes, f, indent=2, ensure_ascii=False)
                
                size_mb = output_path.stat().st_size / (1024 * 1024)
                _logger.info(f"Successfully consolidated {len(all_genomes)} genomes into {output_file}")
                _logger.info(f"File size: {size_mb:.2f} MB")
                
                backup_dir = base_path / "generations_backup"
                backup_dir.mkdir(exist_ok=True)
                
                for gen_num in generation_order:
                    gen_file = base_path / f"gen{gen_num}.json"
                    if gen_file.exists():
                        backup_file = backup_dir / f"gen{gen_num}.json"
                        import shutil
                        shutil.copy2(gen_file, backup_file)
                
                _logger.info(f"Backed up original generation files to {backup_dir}")
                
                return True
                
            except Exception as e:
                _logger.error(f"Failed to save consolidated non_elites.json: {e}")
                return False
                
        except Exception as e:
            _logger.error(f"Failed to consolidate generations: {e}", exc_info=True)
            return False


def migrate_from_split_to_single(base_dir: str = "outputs", 
                                *, logger=None, log_file: Optional[str] = None) -> bool:
    """
    Complete migration from split file architecture back to single non_elites.json.
    
    This function:
    1. Consolidates all generation files into non_elites.json
    2. Updates the population loading logic to use the single file
    3. Provides a clean migration path
    
    Parameters
    ----------
    base_dir : str
        Base directory containing generation files
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    bool
        True if migration was successful, False otherwise
    """
    _logger = logger or get_logger("migrate_to_single", log_file)
    
    with PerformanceLogger(_logger, "Migrate from Split to Single File"):
        try:
            if not consolidate_generations_to_single_file(base_dir, "reserves.json", logger=_logger, log_file=log_file):
                _logger.error("Failed to consolidate generation files")
                return False
            
            base_path = Path(base_dir).resolve()
            evolution_tracker_file = base_path / "EvolutionTracker.json"
            
            if evolution_tracker_file.exists():
                try:
                    with open(evolution_tracker_file, 'r', encoding='utf-8') as f:
                        tracker = json.load(f)
                    
                    tracker["population_metadata"] = {
                        "single_file_mode": True,
                        "population_file": "reserves.json",
                        "elites_file": "elites.json",
                        "migration_note": "Migrated from split files to single reserves.json"
                    }
                    
                    with open(evolution_tracker_file, 'w', encoding='utf-8') as f:
                        json.dump(tracker, f, indent=2)
                    _logger.info("Updated EvolutionTracker for single file architecture")
                    
                except Exception as e:
                    _logger.warning(f"Failed to update EvolutionTracker: {e}")
            
            # Check for consolidated file (now uses reserves.json instead of non_elites.json)
            population_file = base_path / "reserves.json"
            if population_file.exists():
                try:
                    with open(population_file, 'r', encoding='utf-8') as f:
                        consolidated_genomes = json.load(f)
                    
                    _logger.info(f"Migration successful! reserves.json contains {len(consolidated_genomes)} genomes")
                    _logger.info("You can now use the single file approach")
                    
                    return True
                    
                except Exception as e:
                    _logger.error(f"Failed to verify consolidated file: {e}")
                    return False
            else:
                _logger.error("reserves.json was not created during consolidation")
                return False
                
        except Exception as e:
            _logger.error(f"Migration failed: {e}", exc_info=True)
            return False



def _extract_score(genome: Dict[str, Any], north_star_metric: str = "toxicity") -> float:
    """Extract score from genome using north star metric."""
    return _extract_north_star_score(genome, north_star_metric)


def sort_population_by_elite_criteria(population: List[Dict[str, Any]], north_star_metric: str = "toxicity", 
                                     *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Sort population by elite criteria: north star metric score descending, generation descending, genome id descending.
    
    Parameters
    ----------
    population : List[Dict[str, Any]]
        Population to sort
    north_star_metric : str
        Metric to use for scoring (default: "toxicity")
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    List[Dict[str, Any]]
        Sorted population
    """
    _logger = logger or get_logger("population_io", log_file)
    
    with PerformanceLogger(_logger, "Sort Population by Elite Criteria", north_star_metric=north_star_metric):
        def sort_key(genome: Dict[str, Any]) -> tuple:
            score = _extract_score(genome, north_star_metric)
            generation = genome.get("generation", 0)
            genome_id = genome.get("id", 0)
            # Genome IDs are always integers
            return (-score, -generation, -genome_id)
        
        sorted_population = sorted(population, key=sort_key)
        _logger.info(f"Sorted {len(sorted_population)} genomes by elite criteria")
        return sorted_population


from .constants import EvolutionConstants, FileConstants




def load_elites(elites_file_path: str = "data/outputs/elites.json", 
                *, logger=None, log_file: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Load elites from elites.json file.
    
    Parameters
    ----------
    elites_file_path : str
        Path to the elites.json file
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    List[Dict[str, Any]]
        List of elite genomes
    """
    _logger = logger or get_logger("population_io", log_file)
    
    try:
        elites_path = Path(elites_file_path)
        if elites_path.exists():
            with open(elites_path, 'r', encoding='utf-8') as f:
                elites = json.load(f)
            _logger.info(f"Loaded {len(elites)} elites from {elites_file_path}")
            return elites
        else:
            _logger.info(f"Elites file not found: {elites_file_path}, returning empty list")
            return []
    except Exception as e:
        _logger.error(f"Failed to load elites: {e}")
        return []


def save_elites(elites: List[Dict[str, Any]], elites_file_path: str = "data/outputs/elites.json",
                *, logger=None, log_file: Optional[str] = None) -> None:
    """
    Save elites to elites.json file.
    
    Parameters
    ----------
    elites : List[Dict[str, Any]]
        List of elite genomes to save
    elites_file_path : str
        Path to the elites.json file
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
    """
    _logger = logger or get_logger("population_io", log_file)
    
    try:
        elites_path = Path(elites_file_path)
        elites_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(elites_path, 'w', encoding='utf-8') as f:
            json.dump(elites, f, indent=2, ensure_ascii=False)
        
        _logger.info(f"Saved {len(elites)} elites to {elites_file_path}")
    except Exception as e:
        _logger.error(f"Failed to save elites: {e}")
        raise





def get_population_stats_steady_state(population_file_path: str = FileConstants.DEFAULT_RESERVES_FILE,
                                     elites_file_path: str = FileConstants.DEFAULT_ELITES_FILE,
                                     *, logger=None, log_file: Optional[str] = None) -> Dict[str, Any]:
    """
    Get population statistics for steady state mode.
    
    Note: Active population = elites.json + reserves.json (archive.json is NOT part of population).
    
    Parameters
    ----------
    population_file_path : str
        Path to the reserves.json file (Cluster 0 outliers)
    elites_file_path : str
        Path to the elites.json file
    logger : logging.Logger | None
        Existing logger to reuse; if *None* a new one is created
    log_file : str | None
        Optional log-file path when a new logger is created
        
    Returns
    -------
    Dict[str, Any]
        Population statistics
    """
    _logger = logger or get_logger("population_io", log_file)
    
    try:
        # Load population
        population = load_population(population_file_path, logger=_logger, log_file=log_file)
        
        # Load elites
        elites = load_elites(elites_file_path, logger=_logger, log_file=log_file)
        
        return {
            "elites_count": len(elites),
            "steady_state_mode": True
        }
    except Exception as e:
        _logger.error(f"Failed to get population stats: {e}")
        return {
            "steady_state_mode": True,
            "error": str(e)
        }


# Note: remove_worse_performing_genomes_from_all_files() has been removed.
# After speciation integration, low-fitness genomes are handled by speciation's
# capacity enforcement, not by removal_threshold percentage.


def calculate_average_fitness(
    outputs_path: str, 
    north_star_metric: str = "toxicity", 
    include_temp: bool = False,
    logger=None, 
    log_file: Optional[str] = None
) -> float:
    """
    Calculate the average fitness of genomes.
    
    Two modes controlled by include_temp:
    - include_temp=False (default): AFTER speciation - elites.json + reserves.json only
    - include_temp=True: BEFORE speciation - elites.json + reserves.json + temp.json
    
    This distinction is important:
    - avg_fitness: calculated BEFORE speciation (includes temp.json with new variants)
    - avg_fitness_generation: calculated AFTER speciation (elites + reserves only)
    
    Files:
    - elites.json: Elites assigned to species (species_id > 0)
    - reserves.json: Elite outliers that don't fit existing species (species_id == 0)
    - temp.json: New variants before speciation (included only if include_temp=True)
    - archive.json: Archived/removed genomes (NOT part of population, never included)
    
    Args:
        outputs_path: Path to outputs directory
        north_star_metric: Metric to use for scoring (default: "toxicity")
        include_temp: If True, include temp.json (for BEFORE speciation calculation)
        logger: Logger instance
        log_file: Log file path
        
    Returns:
        Average fitness score across selected genomes
    """
    _logger = logger or get_logger("calculate_average_fitness", log_file)
    
    try:
        outputs_dir = Path(outputs_path)
        elites_path = outputs_dir / "elites.json"
        reserves_path = outputs_dir / "reserves.json"
        temp_path = outputs_dir / "temp.json"
        
        total_score = 0.0
        total_count = 0
        
        # Process elites.json (elites assigned to species, species_id > 0)
        if elites_path.exists():
            elites_genomes = load_population(str(elites_path), logger=_logger, log_file=log_file)
            for genome in elites_genomes:
                score = _extract_north_star_score(genome, north_star_metric)
                total_score += score
                total_count += 1
            _logger.debug(f"Processed {len(elites_genomes)} genomes from elites.json")
        
        # Process reserves.json (elite outliers, species_id == 0)
        if reserves_path.exists():
            reserves_genomes = load_population(str(reserves_path), logger=_logger, log_file=log_file)
            for genome in reserves_genomes:
                score = _extract_north_star_score(genome, north_star_metric)
                total_score += score
                total_count += 1
            _logger.debug(f"Processed {len(reserves_genomes)} genomes from reserves.json")
        
        # Process temp.json only if include_temp=True (BEFORE speciation)
        if include_temp and temp_path.exists():
            try:
                with open(temp_path, 'r', encoding='utf-8') as f:
                    temp_genomes = json.load(f)
                for genome in temp_genomes:
                    if genome:
                        score = _extract_north_star_score(genome, north_star_metric)
                        total_score += score
                        total_count += 1
                _logger.debug(f"Processed {len(temp_genomes)} genomes from temp.json")
            except Exception as e:
                _logger.warning(f"Failed to load temp.json for avg_fitness: {e}")
        
        if total_count == 0:
            # This is expected before distribution (generation 0) or if files are empty
            # Only warn if we're not in generation 0 or if files should exist
            if not include_temp:
                # After distribution, elites.json and reserves.json should have genomes
                # If they're empty, this might indicate an issue
                _logger.debug("No genomes found for average fitness calculation (after speciation)")
            else:
                # Before distribution, temp.json might be empty if already processed
                _logger.debug("No genomes found for average fitness calculation (before speciation)")
            return 0.0
        
        avg_fitness = total_score / total_count
        avg_fitness = round(avg_fitness, 4)
        mode = "before speciation (elites+reserves+temp)" if include_temp else "after speciation (elites+reserves)"
        _logger.info(f"Calculated average fitness: {avg_fitness:.4f} from {total_count} genomes ({mode})")
        
        return avg_fitness
        
    except Exception as e:
        _logger.error(f"Failed to calculate average fitness: {e}", exc_info=True)
        return 0.0


def calculate_budget_metrics(
    elites_genomes: List[Dict[str, Any]],
    reserves_genomes: List[Dict[str, Any]],
    temp_genomes: List[Dict[str, Any]],
    current_generation: int,
    logger=None
) -> Dict[str, Any]:
    """
    Calculate evaluation budget metrics for a generation.
    
    Budget metrics track computational cost:
    - llm_calls: Number of LLM calls (response generation) in this generation
    - api_calls: Number of moderation API calls (Perspective API) in this generation
    - total_response_time: Total LLM response generation time (seconds)
    - total_evaluation_time: Total moderation API evaluation time (seconds)
    
    These are counted from genomes created in the current generation.
    
    Args:
        elites_genomes: List of elite genomes
        reserves_genomes: List of reserves genomes
        temp_genomes: List of temp genomes (current generation variants)
        current_generation: Current generation number
        logger: Optional logger instance
        
    Returns:
        Dictionary with budget metrics
    """
    _logger = logger or get_logger("BudgetMetrics")
    
    budget = {
        "llm_calls": 0,
        "api_calls": 0,
        "total_response_time": 0.0,
        "total_evaluation_time": 0.0
    }
    
    try:
        # Combine all genomes and filter by current generation
        all_genomes = (elites_genomes or []) + (reserves_genomes or []) + (temp_genomes or [])
        current_gen_genomes = [g for g in all_genomes if g and g.get("generation") == current_generation]
        
        for genome in current_gen_genomes:
            # Count LLM calls (each genome with response_duration had an LLM call)
            if genome.get("response_duration") is not None or genome.get("generated_output"):
                budget["llm_calls"] += 1
                if genome.get("response_duration"):
                    budget["total_response_time"] += float(genome.get("response_duration", 0))
            
            # Count API calls (each genome with evaluation_duration had an API call)
            if genome.get("evaluation_duration") is not None or genome.get("moderation_result"):
                budget["api_calls"] += 1
                if genome.get("evaluation_duration"):
                    budget["total_evaluation_time"] += float(genome.get("evaluation_duration", 0))
        
        # Round times to 2 decimal places
        budget["total_response_time"] = round(budget["total_response_time"], 2)
        budget["total_evaluation_time"] = round(budget["total_evaluation_time"], 2)
        
        _logger.debug(
            f"Gen {current_generation} budget: {budget['llm_calls']} LLM calls ({budget['total_response_time']}s), "
            f"{budget['api_calls']} API calls ({budget['total_evaluation_time']}s)"
        )
        
        return budget
        
    except Exception as e:
        _logger.warning(f"Failed to calculate budget metrics: {e}")
        return budget


def update_generation_avg_fitness(generation_number: int, avg_fitness: float, evolution_tracker_path: str, logger=None, log_file: Optional[str] = None) -> None:
    """
    Update the avg_fitness field for a specific generation in EvolutionTracker.json.
    
    Args:
        generation_number: The generation number to update
        avg_fitness: The calculated average fitness for this generation
        evolution_tracker_path: Path to EvolutionTracker.json file
        logger: Logger instance
        log_file: Log file path
    """
    _logger = logger or get_logger("update_generation_avg_fitness", log_file)
    
    try:
        # Load EvolutionTracker
        with open(evolution_tracker_path, 'r', encoding='utf-8') as f:
            tracker = json.load(f)
        
        # Find and update the generation
        generation_updated = False
        for gen in tracker.get("generations", []):
            if gen["generation_number"] == generation_number:
                gen["avg_fitness"] = round(avg_fitness, 4)
                generation_updated = True
                _logger.info(f"Updated generation {generation_number} avg_fitness to {avg_fitness:.4f}")
                break
        
        if not generation_updated:
            _logger.warning(f"Generation {generation_number} not found in EvolutionTracker")
            return
        
        # Save updated tracker
        with open(evolution_tracker_path, 'w', encoding='utf-8') as f:
            json.dump(tracker, f, indent=4, ensure_ascii=False)
            
    except Exception as e:
        _logger.error(f"Failed to update generation avg_fitness: {e}", exc_info=True)
        raise


def calculate_slope(values: List[float]) -> float:
    """
    Calculate the slope of a list of values using linear regression.
    
    Args:
        values: List of numeric values
        
    Returns:
        Slope of the linear regression line
    """
    if len(values) < 2:
        return 0.0
    
    try:
        import numpy as np
        x = np.arange(len(values))
        y = np.array(values)
        
        # Calculate slope using least squares
        slope = np.polyfit(x, y, 1)[0]
        return round(float(slope), 4)
        
    except ImportError:
        # Fallback calculation without numpy
        n = len(values)
        sum_x = sum(range(n))
        sum_y = sum(values)
        sum_xy = sum(i * values[i] for i in range(n))
        sum_x2 = sum(i * i for i in range(n))
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        return round(slope, 4)
        
    except Exception:
        return 0.0


def update_adaptive_selection_logic(
    outputs_path: str,
    current_max_toxicity: float,
    previous_max_toxicity: float,
    stagnation_limit: int = 5,
    north_star_metric: str = "toxicity",
    logger=None,
    log_file: Optional[str] = None
) -> Dict[str, Any]:
    """
    Update the adaptive selection logic based on stagnation and fitness trends.
    
    Args:
        outputs_path: Path to outputs directory
        current_max_toxicity: Current maximum toxicity score
        previous_max_toxicity: Previous maximum toxicity score (for comparison)
        stagnation_limit: Number of generations without improvement before switching to explore mode
        north_star_metric: Metric to use for scoring (default: "toxicity")
        logger: Logger instance
        log_file: Log file path
        
    Returns:
        Dict containing updated selection parameters:
        - selection_mode: "default", "explore", or "exploit"
        - generations_since_improvement: Updated count
        - current_avg_fitness: Current average fitness
        - slope_of_avg_fitness: Slope of fitness history
    """
    _logger = logger or get_logger("update_adaptive_selection_logic", log_file)
    
    try:
        evolution_tracker_path = Path(outputs_path) / "EvolutionTracker.json"
        
        # Load EvolutionTracker
        if not evolution_tracker_path.exists():
            _logger.error("EvolutionTracker.json not found")
            return {"selection_mode": "default", "generations_since_improvement": 0, "current_avg_fitness": 0.0, "slope_of_avg_fitness": 0.0}
        
        with open(evolution_tracker_path, 'r', encoding='utf-8') as f:
            tracker = json.load(f)
        
        # Use the passed previous_max_toxicity instead of reading from tracker
        _logger.info(f"Adaptive selection comparison: current_max_toxicity={current_max_toxicity:.4f}, previous_max_toxicity={previous_max_toxicity:.4f}")
        
        # Update generations_since_improvement
        if current_max_toxicity > previous_max_toxicity:
            tracker["generations_since_improvement"] = 0
            _logger.info(f"Improvement detected! Max toxicity increased from {previous_max_toxicity:.4f} to {current_max_toxicity:.4f}")
        else:
            tracker["generations_since_improvement"] = tracker.get("generations_since_improvement", 0) + 1
            _logger.info(f"No improvement. Generations since improvement: {tracker['generations_since_improvement']}")
        
        # Calculate current average fitness
        current_avg_fitness = calculate_average_fitness(outputs_path, north_star_metric, logger=_logger, log_file=log_file)
        
        # Update avg_fitness_history using sliding window from generations
        avg_fitness_history = tracker.get("avg_fitness_history", [])
        
        # Get current generation number - should be the latest generation
        generations = tracker.get("generations", [])
        if generations:
            current_generation = max(gen.get("generation_number", 0) for gen in generations)
        else:
            current_generation = 0
        
        # Update the current generation's avg_fitness in the tracker
        generation_updated = False
        for gen in tracker.get("generations", []):
            if gen["generation_number"] == current_generation:
                gen["avg_fitness"] = round(current_avg_fitness, 4)
                generation_updated = True
                break
        
        if not generation_updated:
            _logger.warning(f"Generation {current_generation} not found in EvolutionTracker for avg_fitness update")
        
        # Build avg_fitness_history from the last m generations
        # IMPORTANT: Re-fetch generations after update to ensure we have the latest values
        generations = tracker.get("generations", [])
        # Filter for generations with valid avg_fitness (exclude None)
        # Only include generations where avg_fitness was actually calculated (not the initial 0.0 placeholder)
        generations_with_avg_fitness = []
        for gen in generations:
            if "avg_fitness" in gen and gen["avg_fitness"] is not None:
                # Skip initial placeholder 0.0 values that haven't been updated yet
                # If avg_fitness is 0.0 and this is the first time we're calculating, 
                # it means the calculation hasn't happened yet or returned 0.0 legitimately
                # We include it only if it's not the initial placeholder (i.e., if current_avg_fitness was calculated)
                if gen["generation_number"] == current_generation:
                    # For the current generation, use the calculated value we just computed
                    generations_with_avg_fitness.append(gen)
                elif gen["avg_fitness"] > 0.0 or gen.get("elites_count", 0) > 0 or gen.get("reserves_count", 0) > 0:
                    # For past generations, include if avg_fitness > 0 or if population exists (indicating it was calculated)
                    generations_with_avg_fitness.append(gen)
                # Otherwise, skip 0.0 values that are likely placeholders
        
        # Sort by generation number and take the last m generations (sliding window)
        generations_with_avg_fitness.sort(key=lambda x: x["generation_number"])
        # Take the last stagnation_limit generations (or all if fewer than stagnation_limit exist)
        recent_generations = generations_with_avg_fitness[-stagnation_limit:]
        
        # Extract avg_fitness values for the sliding window (round to 4 decimal places)
        avg_fitness_history = [round(gen["avg_fitness"], 4) for gen in recent_generations]
        
        _logger.info(f"Built avg_fitness_history with {len(avg_fitness_history)} entries from {len(generations_with_avg_fitness)} total generations (window size: {stagnation_limit})")
        
        tracker["avg_fitness_history"] = avg_fitness_history
        
        # Calculate slope of avg_fitness_history (already rounded in calculate_slope, but ensure it's 4 decimals)
        slope_of_avg_fitness = calculate_slope(avg_fitness_history)
        slope_of_avg_fitness = round(slope_of_avg_fitness, 4)
        tracker["slope_of_avg_fitness"] = slope_of_avg_fitness
        
        # Determine selection mode
        generations_since_improvement = tracker["generations_since_improvement"]
        total_generations = tracker.get("total_generations", 1)
        
        # For the first m generations (where m = stagnation_limit), always use DEFAULT mode
        if total_generations <= stagnation_limit:
            selection_mode = "default"
            _logger.info(f"Using DEFAULT mode for initial {stagnation_limit} generations (generation {total_generations})")
        elif slope_of_avg_fitness < -0.0001:  # Use small threshold to handle -0.0 case
            # Check EXPLOIT condition first (negative fitness slope)
            # Note: -0.0 == 0.0 in Python, so we use a small threshold
            selection_mode = "exploit"
            _logger.info(f"Switching to EXPLOIT mode (negative fitness slope: {slope_of_avg_fitness:.4f})")
        elif generations_since_improvement >= stagnation_limit:
            # Then check EXPLORE condition (stagnation)
            selection_mode = "explore"
            _logger.info(f"Switching to EXPLORE mode (generations since improvement: {generations_since_improvement} >= {stagnation_limit})")
        else:
            # Finally DEFAULT mode
            selection_mode = "default"
            _logger.info(f"Using DEFAULT mode (generations since improvement: {generations_since_improvement}, slope: {slope_of_avg_fitness:.4f})")
        
        tracker["selection_mode"] = selection_mode
        
        # Save updated tracker
        with open(evolution_tracker_path, 'w', encoding='utf-8') as f:
            json.dump(tracker, f, indent=2)
        
        _logger.info(f"Updated adaptive selection: mode={selection_mode}, avg_fitness={current_avg_fitness:.4f}, slope={slope_of_avg_fitness:.4f}")
        
        return {
            "selection_mode": selection_mode,
            "generations_since_improvement": generations_since_improvement,
            "current_avg_fitness": round(current_avg_fitness, 4),
            "slope_of_avg_fitness": slope_of_avg_fitness
        }
        
    except Exception as e:
        _logger.error(f"Failed to update adaptive selection logic: {e}", exc_info=True)
        return {"selection_mode": "default", "generations_since_improvement": 0, "current_avg_fitness": 0.0, "slope_of_avg_fitness": 0.0}


def calculate_generation_statistics(
    outputs_path: str,
    north_star_metric: str = "toxicity",
    current_generation: int = 0,
    logger=None,
    log_file: Optional[str] = None
) -> Dict[str, Any]:
    """
    Calculate comprehensive statistics for a generation.
    
    Reads elites.json, reserves.json, and temp.json to calculate:
    - Fitness statistics (max, min, avg) for each category
    - Population counts
    - Score distributions
    
    Args:
        outputs_path: Path to the outputs directory
        north_star_metric: Metric to use for fitness (default: "toxicity")
        current_generation: Current generation number
        logger: Optional logger instance
        log_file: Optional log file path
        
    Returns:
        Dictionary with all calculated statistics
    """
    _logger = logger or get_logger("GenerationStatistics", log_file)
    
    outputs_dir = Path(outputs_path)
    stats = {
        "generation_number": current_generation,
        "initial_population_size": 0,
        "elites_count": 0,
        "reserves_count": 0,
        "total_population": 0,
        "max_score_variants": 0.0001,
        "min_score_variants": 0.0001,
        "avg_fitness_variants": 0.0001,
        "avg_fitness_generation": 0.0001,
        "avg_fitness_elites": 0.0001,
        "avg_fitness_reserves": 0.0001,
        "population_max_toxicity": 0.0001,
    }
    
    try:
        # Load elites.json
        elites_path = outputs_dir / "elites.json"
        elites_genomes = []
        if elites_path.exists():
            with open(elites_path, 'r', encoding='utf-8') as f:
                elites_genomes = json.load(f)
        
        # Load reserves.json
        reserves_path = outputs_dir / "reserves.json"
        reserves_genomes = []
        if reserves_path.exists():
            with open(reserves_path, 'r', encoding='utf-8') as f:
                reserves_genomes = json.load(f)
        
        # Load temp.json (current variants being processed)
        temp_path = outputs_dir / "temp.json"
        temp_genomes = []
        if temp_path.exists():
            with open(temp_path, 'r', encoding='utf-8') as f:
                temp_genomes = json.load(f)
        
        # Calculate counts
        stats["elites_count"] = len(elites_genomes)
        stats["reserves_count"] = len(reserves_genomes)
        stats["total_population"] = stats["elites_count"] + stats["reserves_count"]
        
        # Calculate elite fitness statistics
        elite_scores = []
        for g in elites_genomes:
            score = _extract_north_star_score(g, north_star_metric)
            if score > 0.0001:
                elite_scores.append(score)
        
        if elite_scores:
            stats["avg_fitness_elites"] = round(sum(elite_scores) / len(elite_scores), 4)
        
        # Calculate reserves fitness statistics
        reserves_scores = []
        for g in reserves_genomes:
            score = _extract_north_star_score(g, north_star_metric)
            if score > 0.0001:
                reserves_scores.append(score)
        
        if reserves_scores:
            stats["avg_fitness_reserves"] = round(sum(reserves_scores) / len(reserves_scores), 4)
        
        # Calculate temp/variant fitness statistics
        variant_scores = []
        for g in temp_genomes:
            score = _extract_north_star_score(g, north_star_metric)
            if score > 0.0001:
                variant_scores.append(score)
        
        if variant_scores:
            stats["max_score_variants"] = round(max(variant_scores), 4)
            stats["min_score_variants"] = round(min(variant_scores), 4)
            stats["avg_fitness_variants"] = round(sum(variant_scores) / len(variant_scores), 4)
        
        # Calculate overall generation fitness (elites + reserves)
        all_scores = elite_scores + reserves_scores
        if all_scores:
            stats["avg_fitness_generation"] = round(sum(all_scores) / len(all_scores), 4)
            stats["population_max_toxicity"] = round(max(all_scores), 4)
        
        # For generation 0, initial_population_size is the count before distribution
        if current_generation == 0:
            # Use temp_genomes or combined count
            stats["initial_population_size"] = len(temp_genomes) if temp_genomes else stats["total_population"]
        
        # Calculate budget metrics (LLM calls + API calls) for current generation
        budget_metrics = calculate_budget_metrics(
            elites_genomes, reserves_genomes, temp_genomes,
            current_generation, _logger
        )
        stats.update(budget_metrics)
        
        _logger.debug(
            "Gen %d stats: elites=%d (avg=%.4f), reserves=%d (avg=%.4f), total=%d, avg_gen=%.4f, llm_calls=%d, api_calls=%d",
            current_generation, stats["elites_count"], stats["avg_fitness_elites"],
            stats["reserves_count"], stats["avg_fitness_reserves"],
            stats["total_population"], stats["avg_fitness_generation"],
            stats.get("llm_calls", 0), stats.get("api_calls", 0)
        )
        
        return stats
        
    except Exception as e:
        _logger.error(f"Failed to calculate generation statistics: {e}", exc_info=True)
        return stats


def update_evolution_tracker_with_statistics(
    evolution_tracker_path: str,
    current_generation: int,
    statistics: Dict[str, Any],
    operator_statistics: Optional[Dict[str, Any]] = None,
    logger=None,
    log_file: Optional[str] = None
) -> bool:
    """
    Update EvolutionTracker.json with generation statistics.
    
    Args:
        evolution_tracker_path: Path to EvolutionTracker.json
        current_generation: Current generation number
        statistics: Dictionary of statistics from calculate_generation_statistics()
        operator_statistics: Optional operator-level statistics
        logger: Optional logger instance
        log_file: Optional log file path
        
    Returns:
        True if successful, False otherwise
    """
    _logger = logger or get_logger("UpdateEvolutionTracker", log_file)
    
    try:
        tracker_path = Path(evolution_tracker_path)
        if not tracker_path.exists():
            _logger.warning("EvolutionTracker.json not found at %s", evolution_tracker_path)
            return False
        
        with open(tracker_path, 'r', encoding='utf-8') as f:
            tracker = json.load(f)
        
        # Find or create generation entry
        generations = tracker.setdefault("generations", [])
        gen_entry = None
        for gen in generations:
            if gen.get("generation_number") == current_generation:
                gen_entry = gen
                break
        
        if gen_entry is None:
            gen_entry = {"generation_number": current_generation}
            generations.append(gen_entry)
        
        # Update with statistics (round all float values to 4 decimal places)
        gen_entry.update({
            "elites_count": statistics.get("elites_count", 0),
            "reserves_count": statistics.get("reserves_count", 0),
            "total_population": statistics.get("total_population", 0),
            "max_score_variants": round(statistics.get("max_score_variants", 0.0001), 4),
            "min_score_variants": round(statistics.get("min_score_variants", 0.0001), 4),
            "avg_fitness_variants": round(statistics.get("avg_fitness_variants", 0.0001), 4),
            "avg_fitness_generation": round(statistics.get("avg_fitness_generation", 0.0001), 4),
            "avg_fitness": round(statistics.get("avg_fitness_generation", 0.0001), 4),  # Alias for compatibility
            "avg_fitness_elites": round(statistics.get("avg_fitness_elites", 0.0001), 4),
            "avg_fitness_reserves": round(statistics.get("avg_fitness_reserves", 0.0001), 4),
        })
        
        # Add budget metrics if available
        if "llm_calls" in statistics:
            gen_entry["budget"] = {
                "llm_calls": statistics.get("llm_calls", 0),
                "api_calls": statistics.get("api_calls", 0),
                "total_response_time": statistics.get("total_response_time", 0.0),
                "total_evaluation_time": statistics.get("total_evaluation_time", 0.0)
            }
            
            # Update cumulative budget at tracker level
            if "cumulative_budget" not in tracker:
                tracker["cumulative_budget"] = {
                    "total_llm_calls": 0,
                    "total_api_calls": 0,
                    "total_response_time": 0.0,
                    "total_evaluation_time": 0.0
                }
            
            tracker["cumulative_budget"]["total_llm_calls"] += statistics.get("llm_calls", 0)
            tracker["cumulative_budget"]["total_api_calls"] += statistics.get("api_calls", 0)
            tracker["cumulative_budget"]["total_response_time"] = round(
                tracker["cumulative_budget"]["total_response_time"] + statistics.get("total_response_time", 0.0), 2
            )
            tracker["cumulative_budget"]["total_evaluation_time"] = round(
                tracker["cumulative_budget"]["total_evaluation_time"] + statistics.get("total_evaluation_time", 0.0), 2
            )
        
        # Update population max toxicity at tracker level (cumulative max across all generations)
        # This tracks the maximum toxicity score achieved across all generations
        new_max = statistics.get("population_max_toxicity")
        if new_max and new_max > 0.0001:
            # Initialize if not present
            if "population_max_toxicity" not in tracker:
                tracker["population_max_toxicity"] = 0.0001
            # Update to cumulative max (always keep the highest value seen)
            tracker["population_max_toxicity"] = max(
                tracker.get("population_max_toxicity", 0.0001),
                new_max
            )
            _logger.debug(f"Updated cumulative population_max_toxicity to {tracker['population_max_toxicity']:.4f}")
        
        # Also update genome_id and variant counts if provided
        if statistics.get("best_genome_id"):
            gen_entry["genome_id"] = statistics.get("best_genome_id")
        
        if statistics.get("variants_created") is not None:
            gen_entry["variants_created"] = statistics.get("variants_created", 0)
        if statistics.get("mutation_variants") is not None:
            gen_entry["mutation_variants"] = statistics.get("mutation_variants", 0)
        if statistics.get("crossover_variants") is not None:
            gen_entry["crossover_variants"] = statistics.get("crossover_variants", 0)
        
        # Add operator statistics if provided
        if operator_statistics:
            gen_entry["operator_statistics"] = operator_statistics
        
        # Sort generations by number
        tracker["generations"] = sorted(generations, key=lambda x: x.get("generation_number", 0))
        
        # Save updated tracker (use indent=2 to match other JSON files)
        with open(tracker_path, 'w', encoding='utf-8') as f:
            json.dump(tracker, f, indent=2, ensure_ascii=False)
        
        _logger.info(
            "Updated EvolutionTracker gen %d: elites=%d, reserves=%d, avg_fitness=%.4f",
            current_generation, statistics.get("elites_count", 0),
            statistics.get("reserves_count", 0), statistics.get("avg_fitness_generation", 0.0001)
        )
        
        return True
        
    except Exception as e:
        _logger.error(f"Failed to update EvolutionTracker with statistics: {e}", exc_info=True)
        return False


# ============================================================================
# Module Exports
# ============================================================================

__all__ = [
    # Main I/O functions
    "load_population",
    "save_population",
    
    # Split file management
    "get_population_files_info",
    "load_population_generation",
    "load_population_range", 
    "load_population_lazy",
    "save_population_generation",
    "update_population_index_single_file",
    "get_latest_generation",
    
    # Genome-specific loading
    "load_genome_by_id",
    "load_genomes_by_ids",
    
    # Population management
    "load_and_initialize_population",
    "validate_population_file",
    "sort_population_json",
    "clean_population",
    
    # Adaptive selection
    "calculate_average_fitness",
    "update_generation_avg_fitness",
    "calculate_slope",
    "update_adaptive_selection_logic",
    
    # Migration functions
    "consolidate_generations_to_single_file",
    "migrate_from_split_to_single",
    
    # Steady state population management
    "sort_population_by_elite_criteria",
    "load_elites",
    "save_elites",
    "get_population_stats_steady_state",
    
    # Generation statistics
    "calculate_generation_statistics",
    "update_evolution_tracker_with_statistics",
]