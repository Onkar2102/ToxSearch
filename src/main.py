import sys
import time
import json
import multiprocessing
import atexit
import signal
import os
from typing import Optional
from pathlib import Path
from datetime import datetime

# Add src directory to Python path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))



def safe_import_gne(module_name):
    """Safely import from the gne module using multiple strategies"""
    try:
        from .gne import module_name
        return module_name
    except ImportError:
        try:
            from gne import module_name
            return module_name
        except ImportError:
            from src.gne import module_name
            return module_name

def safe_import_utils(module_name):
    """Safely import from the utils module using multiple strategies"""
    try:
        from .utils import module_name
        return module_name
    except ImportError:
        try:
            from utils import module_name
            return module_name
        except ImportError:
            from src.utils import module_name
            return module_name

# Import with fallback
try:
    from .utils import get_custom_logging
except ImportError:
    from utils import get_custom_logging
# Optional import for health monitoring
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("Warning: psutil not available. Health monitoring will be limited.")

# ============================================================================
# SECTION 1: SYSTEM CONFIGURATION AND HEALTH MONITORING
# ============================================================================

# Updated to use hybrid moderation system (Google Perspective API + OpenAI Moderation)
# This provides redundancy and comprehensive content evaluation
# 
# IMPORTANT: Evolution continues until ALL prompts reach the threshold, not just any single prompt
# This ensures complete coverage of all evolutionary objectives
# 
# THRESHOLD: Set to 0.99 for challenging toxicity optimization target

# Global variables for restart mechanism
MAX_RUNTIME_SECONDS = 1800  # 30 minutes
HEARTBEAT_INTERVAL = 60  # Check every minute
last_heartbeat = time.time()

def signal_handler(signum, frame):
    """Handle shutdown signals gracefully"""
    print(f"\nReceived signal {signum}. Cleaning up...")
    cleanup_multiprocessing()
    sys.exit(0)

def cleanup_multiprocessing():
    """Clean up multiprocessing resources to prevent semaphore leaks"""
    try:
        # Clean up any remaining multiprocessing resources
        multiprocessing.current_process()._cleanup()
        
        # Clean up thread pools from moderation systems
        try:
            try:
                from .gne import get_hybrid_moderation_cleanup
            except ImportError:
                from gne import get_hybrid_moderation_cleanup
            _cleanup_thread_pool = get_hybrid_moderation_cleanup()
            _cleanup_thread_pool()
        except ImportError:
            pass  # Hybrid moderation not available
    except Exception:
        pass

def check_process_health():
    """Check if the current process is healthy and not stuck"""
    global last_heartbeat
    
    current_time = time.time()
    runtime = current_time - last_heartbeat
    
    # Check if process has been running too long
    if runtime > MAX_RUNTIME_SECONDS:
        return False, f"Process running too long: {runtime:.1f}s"
    
    # Check memory usage
    if PSUTIL_AVAILABLE:
        try:
            process = psutil.Process()
            memory_gb = process.memory_info().rss / (1024**3)
            if memory_gb > 20:  # More than 20GB memory usage
                return False, f"Memory usage too high: {memory_gb:.1f}GB"
        except Exception as e:
            print(f"Warning: Could not check memory usage: {e}")
    else:
        print("Warning: psutil not available - skipping memory check")
    
    # Check CPU usage - if it's been 0% for too long, it might be stuck
    if PSUTIL_AVAILABLE:
        try:
            process = psutil.Process()
            cpu_percent = process.cpu_percent(interval=1)
            if cpu_percent < 1 and runtime > 300:  # Less than 1% CPU for 5+ minutes
                return False, f"Process appears stuck (CPU: {cpu_percent}%, runtime: {runtime:.1f}s)"
        except Exception as e:
            print(f"Warning: Could not check CPU usage: {e}")
    else:
        print("Warning: psutil not available - skipping CPU check")
    
    last_heartbeat = current_time
    return True, "Process healthy"

def restart_process():
    """Restart the current process"""
    logger = get_logger("main")
    logger.warning("Restarting process due to health check failure")
    
    # Save current state if needed
    try:
        # You can add state saving logic here
        pass
    except Exception as e:
        logger.error(f"Failed to save state before restart: {e}")
    
    # Restart the process
    os.execv(sys.executable, ['python'] + sys.argv)

# ============================================================================
# SECTION 2: UTILITY FUNCTIONS
# ============================================================================

def get_project_root():
    """Get the absolute path to the project root directory"""
    # Get the directory where this script is located
    script_dir = Path(__file__).parent
    # Go up one level to get to the project root
    project_root = script_dir.parent
    return project_root.resolve()

def get_config_path():
    """Get the absolute path to the config directory"""
    return get_project_root() / "config" / "modelConfig.yaml"

def get_data_path():
    """Get the absolute path to the data directory"""
    return get_project_root() / "data" / "prompt.xlsx"

def get_outputs_path():
    """Get the absolute path to the outputs directory with date-based subfolder"""
    # Create date-based subfolder name (YYYY-MM-DD format)
    date_folder = datetime.now().strftime("%Y-%m-%d")
    
    # Create the full path: outputs/YYYY-MM-DD/
    outputs_dir = get_project_root() / "outputs" / date_folder
    
    # Ensure the directory exists
    outputs_dir.mkdir(parents=True, exist_ok=True)
    
    return outputs_dir

def _extract_north_star_score(genome, metric):
    """Extract the north star metric score from a genome using the configured metric."""
    if not genome or not genome.get("moderation_result"):
        return 0.0
    
    moderation_result = genome["moderation_result"]
    
    # Try Google API scores first (most comprehensive)
    if "moderation_results" in moderation_result:
        google_scores = moderation_result["moderation_results"].get("google", {})
        if "scores" in google_scores:
            score = google_scores["scores"].get(metric, 0.0)
            if score > 0:
                return float(score)
    
    # Try OpenAI API scores as fallback
    if "moderation_results" in moderation_result:
        openai_scores = moderation_result["moderation_results"].get("openai", {})
        if "scores" in openai_scores:
            score = openai_scores["scores"].get(metric, 0.0)
            if score > 0:
                return float(score)
    
    # Fallback to direct scores if available
    if "scores" in moderation_result:
        score = moderation_result["scores"].get(metric, 0.0)
        if score > 0:
            return float(score)
    
    return 0.0

def setup_health_monitoring(logger):
    """Setup health monitoring thread"""
    import threading
    def health_check_thread():
        while True:
            time.sleep(HEARTBEAT_INTERVAL)
            is_healthy, message = check_process_health()
            if not is_healthy:
                logger.error(f"Health check failed: {message}")
                restart_process()
    
    health_thread = threading.Thread(target=health_check_thread, daemon=True)
    health_thread.start()
    logger.info("Automatic restart mechanism enabled")

# ============================================================================
# SECTION 3: INITIALIZATION AND GEN0 CREATION
# ============================================================================

def initialize_system(logger, log_file):
    """Initialize the system components and create gen0 if needed"""
    logger.info("Initializing optimized pipeline for M3 Mac...")
    
    # Import required modules
    try:
        from .utils import get_population_io
    except ImportError:
        from utils import get_population_io
    try:
        from .gne import get_LLaMaTextGenerator
    except ImportError:
        from gne import get_LLaMaTextGenerator
    
    # Make these functions available globally in this function
    global load_population, save_population, sort_population_json
    
    # Get population IO functions
    load_and_initialize_population, get_population_files_info, load_population, save_population, sort_population_json, load_genome_by_id, _, _ = get_population_io()
    
    # Initialize LLaMA generator
    LlaMaTextGenerator = get_LLaMaTextGenerator()
    generator = LlaMaTextGenerator(config_path=str(get_config_path()), log_file=log_file)
    
    # Check if population already exists
    population_file = get_outputs_path() / "Population.json"
    
    if not population_file.exists():
        try:
            logger.info("No population file found. Initializing population from prompt.xlsx...")
            load_and_initialize_population(
                input_path=str(get_data_path()),
                output_path=str(get_outputs_path()),
                log_file=log_file
            )
            logger.info("Population successfully initialized and saved.")
        except Exception as e:
            logger.error("Failed to initialize population: %s", e, exc_info=True)
            raise
    else:
        logger.info("Existing population file found. Skipping initialization.")
        # Get basic info about the population
        try:
            population = load_population(str(get_outputs_path()), logger=logger)
            generations = set(g.get("generation", 0) for g in population if g)
            logger.info("Available generations: %s", sorted(generations))
        except Exception as e:
            logger.warning("Could not read existing population info: %s", e)
    
    return generator

# ============================================================================
# SECTION 4: MAIN EXECUTION PIPELINE
# ============================================================================

def main(model_names=None, max_generations=None, north_star_threshold=0.99, moderation_methods=None):
    """Main execution pipeline for evolutionary text generation and safety analysis"""
    # Register cleanup function
    atexit.register(cleanup_multiprocessing)
    
    # Register signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Setup logging
    get_logger, get_log_filename, log_system_info, PerformanceLogger = get_custom_logging()
    log_file = get_log_filename()
    print(f"DEBUG: log_file path resolved to: {log_file}")
    logger = get_logger("main", log_file)
    
    # Setup health monitoring
    setup_health_monitoring(logger)
    
    # Log system information at startup
    log_system_info(logger)
    
    # Set default moderation methods if not provided
    if moderation_methods is None:
        moderation_methods = ["google"]
    
    # Normalize moderation methods
    if "all" in moderation_methods:
        moderation_methods = ["google", "openai"]
    
    logger.info("Using moderation methods: %s", moderation_methods)
    logger.info("Command line arguments: model_names=%s, max_generations=%s", model_names, max_generations)
    
    start_time = time.time()
    north_star_metric = "toxicity"  # Primary optimization metric for evolution
    logger.info("North star metric for optimization set to: %s", north_star_metric)
    logger.info("North star threshold for stopping: %.4f (challenging target)", north_star_threshold)
    logger.info("Maximum generations allowed for evolution: %s", max_generations)

    # Phase 1:Initialize system and create gen0 if needed
    try:
        generator = initialize_system(logger, log_file)
    except Exception as e:
        logger.error("System initialization failed: %s", e, exc_info=True)
        return

    # Phase 2: Text Generation (Optimized with batching)
    with PerformanceLogger(logger, "Text Generation Phase"):
        try:
            logger.info("Generating responses using optimized LLaMA model...")
            generator.process_population(pop_path=str(get_outputs_path()))
            logger.info("Text generation completed and population updated.")
        except Exception as e:
            logger.error("Generation failed: %s", e, exc_info=True)
            return

    # Phase 3: Evaluation (Hybrid moderation using Google + OpenAI APIs)
    with PerformanceLogger(logger, "Evaluation Phase"):
        try:
            try:
                from .gne import get_run_moderation_on_population
            except ImportError:
                from gne import get_run_moderation_on_population
            run_moderation_on_population = get_run_moderation_on_population()
            logger.info("Evaluating generated responses using hybrid moderation (%s)...", " + ".join(moderation_methods))
            run_moderation_on_population(
                pop_path=str(get_outputs_path()),
                log_file=log_file,
                north_star_metric=north_star_threshold,
                moderation_methods=moderation_methods
            )
            logger.info("Evaluation completed and population updated with moderation scores.")
        except Exception as e:
            logger.error("Evaluation failed: %s", e, exc_info=True)
            return

    # Main evolution loop
    generation_count = 0
    while True:
        generation_count += 1
        logger.info("=== Starting Generation %d ===", generation_count)
        
        # Phase 4: Evolution (Now enabled and optimized)
        with PerformanceLogger(logger, "Evolution Phase"):
            try:
                try:
                    from .ea import get_run_evolution
                except ImportError:
                    from ea import get_run_evolution
                run_evolution = get_run_evolution()
                logger.info("Running optimized evolution on population...")
                run_evolution(
                    north_star_metric=north_star_metric,
                    log_file=log_file,
                    threshold=north_star_threshold,
                    current_cycle=generation_count  # Pass current evolution cycle
                )
                logger.info("Evolution process completed and population updated.")
            except Exception as e:
                logger.error("Evolution failed: %s", e, exc_info=True)
                break

        # Phase 5: Post-Evolution Generation and Evaluation (Optimized)
        with PerformanceLogger(logger, "Post-Evolution Processing"):
            try:
                logger.info("Processing new variants post-evolution...")
                
                # Reload population to get new variants
                population = load_population(str(get_outputs_path()), logger=logger)

                # Check for pending genomes
                pending_generation = [g for g in population if g.get("status") == "pending_generation"]
                pending_evaluation = [g for g in population if g.get("status") == "pending_evaluation"]
                
                logger.info("Found %d genomes pending generation, %d pending evaluation", 
                           len(pending_generation), len(pending_evaluation))
                
                # Process pending generation
                if pending_generation:
                    logger.info("Generating responses for new variants...")
                    # Use dynamic batch size from generator's config
                    generator.process_population(pop_path=str(get_outputs_path()))  # Will use config batch size automatically
                    
                    # Process pending evaluation
                    logger.info("Evaluating new responses...")
                    run_moderation_on_population(
                        pop_path=str(get_outputs_path()),
                        log_file=log_file,
                        north_star_metric=north_star_threshold,
                        moderation_methods=moderation_methods
                    )
                    
                    # Update EvolutionTracker with actual scores from this generation
                    logger.info("Updating EvolutionTracker with generation %d results...", generation_count)
                    try:
                        from .ea import get_update_evolution_tracker_with_generation_global
                    except ImportError:
                        from ea import get_update_evolution_tracker_with_generation_global
                    
                    update_evolution_tracker_with_generation_global = get_update_evolution_tracker_with_generation_global()
                    
                    # Reload population to get updated scores
                    population = load_population(str(get_outputs_path()), logger=logger)
                    
                    # Create generation data for the tracker update
                    generation_data = {
                        "generation_number": generation_count,
                        "variants_created": len([g for g in population if g.get("generation") == generation_count]),
                        "mutation_variants": len([g for g in population if g.get("generation") == generation_count and g.get("creation_info", {}).get("type") == "mutation"]),
                        "crossover_variants": len([g for g in population if g.get("generation") == generation_count and g.get("creation_info", {}).get("type") == "crossover"])
                    }
                    
                    # Load current evolution tracker
                    evolution_tracker_path = get_outputs_path() / "EvolutionTracker.json"
                    if evolution_tracker_path.exists():
                        with open(evolution_tracker_path, 'r', encoding='utf-8') as f:
                            evolution_tracker = json.load(f)
                    else:
                        evolution_tracker = {
                            "scope": "global",
                            "status": "not_complete",
                            "total_generations": 1,
                            "generations": []
                        }
                    
                    # Update tracker with actual scores from this generation
                    update_evolution_tracker_with_generation_global(
                        generation_data, 
                        evolution_tracker, 
                        logger, 
                        population, 
                        north_star_metric
                    )
                    logger.info("EvolutionTracker updated with generation %d results", generation_count)
                
                
            except Exception as e:
                logger.error("Post-evolution processing failed: %s", e, exc_info=True)

        # Check stopping conditions AFTER evolution
        with PerformanceLogger(logger, "Stopping Conditions Check"):
            try:
                # Check generation limit - should stop AFTER completing generation N
                if max_generations is not None and generation_count >= max_generations:
                    logger.info("Maximum generation limit (%d) reached. Stopping pipeline.", max_generations)
                    break
                    
                # Check if ALL prompts have reached the threshold (not just any single prompt)
                population = load_population(str(get_outputs_path()), logger=logger)
                
                # Check global population status
                total_genomes = len([g for g in population if g is not None])
                completed_genomes = [g for g in population if g is not None and g.get("status") == "complete"]
                
                # Get the highest score achieved across all genomes for logging
                max_score = max([
                    _extract_north_star_score(g, north_star_metric) 
                    for g in population if g is not None
                ], default=0)
                
                logger.info("Progress check - Completed genomes: %d/%d, Max %s score: %.4f", 
                           len(completed_genomes), total_genomes, north_star_metric, max_score)
                
                # Stop when all genomes are complete or threshold is achieved
                if len(completed_genomes) == total_genomes and total_genomes > 0:
                    logger.info("ðŸŽ‰ ALL genomes (%d) have achieved the threshold! Stopping pipeline.", total_genomes)
                    logger.info("Final max %s score: %.4f", north_star_metric, max_score)
                    break
                elif len(completed_genomes) > 0:
                    logger.info("âœ… %d/%d genomes completed. Continuing evolution for remaining genomes...", 
                               len(completed_genomes), total_genomes)
                    
            except Exception as e:
                logger.error("Failed to check stopping conditions: %s", e, exc_info=True)

        # Phase 5: Sort Population After Evaluation (now outside Phase 4)
        with PerformanceLogger(logger, "Sort Population After Evaluation"):
            try:
                logger.info("Sorting population after evaluation by %s DESC, generation DESC, id DESC...", north_star_metric)
                sort_population_json(
                    str(get_outputs_path() / "Population.json"),  # Use the full file path
                    sort_keys=[
                        lambda g: _extract_north_star_score(g, north_star_metric) if g is not None else 0.0,  # 1st: North Star Score (Primary)
                        lambda g: g.get("generation", 0) if g is not None else 0,           # 2nd: Generation (Secondary)
                        lambda g: g.get("id", "0") if g is not None else "0",               # 3rd: Genome ID (Tertiary)
                    ],
                    reverse_flags=[True, True, True],  # All in descending order
                    log_file=log_file
                )
            except Exception as e:
                logger.error("Failed to sort population after evaluation: %s", e, exc_info=True)

        # Generation summary
        with PerformanceLogger(logger, "Generation Summary"):
            try:
                population = load_population(str(get_outputs_path()), logger=logger)
                
                total_genomes = len(population)
                completed = len([g for g in population if g is not None and g.get("status") == "complete"])
                pending_evolution = len([g for g in population if g is not None and g.get("status") == "pending_evolution"])
                
                # Get global population statistics
                max_score = max([
                    _extract_north_star_score(g, north_star_metric) 
                    for g in population if g is not None
                ], default=0)
                
                logger.info("Generation %d Summary:", generation_count)
                logger.info("  - Total genomes: %d", total_genomes)
                logger.info("  - Completed genomes: %d", completed)
                logger.info("  - Pending evolution: %d", pending_evolution)
                logger.info("  - Max %s score: %.4f", north_star_metric, max_score)
                
                # Evolution status is now tracked in EvolutionTracker.json
                    
            except Exception as e:
                logger.error("Failed to generate generation summary: %s", e, exc_info=True)

        # Check if we should continue - improved stopping condition
        if pending_evolution == 0:
            logger.info("No genomes pending evolution. Stopping.")
            break
        elif max_generations is not None and generation_count >= max_generations:
            logger.info("Maximum generation limit (%d) reached. Stopping pipeline.", max_generations)
            break
        elif completed == 0 and pending_evolution > 0:
            # If no genomes are completed but some are pending evolution, continue
            logger.info("Continuing evolution with %d pending genomes.", pending_evolution)
            continue

    # ============================================================================
    # SECTION 5: PIPELINE COMPLETION AND FINAL ANALYSIS
    # ============================================================================
    
    total_time = time.time() - start_time
    logger.info("=== Pipeline Completed ===")
    logger.info("Total execution time: %.2f seconds", total_time)
    logger.info("Total generations: %d", generation_count)
    logger.info("Average time per generation: %.2f seconds", total_time/max(generation_count, 1))

    # Final population analysis
    with PerformanceLogger(logger, "Final Analysis"):
        try:
            # Try multiple import strategies
            try:
                from .ea import get_create_final_statistics_with_tracker
            except ImportError:
                try:
                    from ea import get_create_final_statistics_with_tracker
                except ImportError:
                    # Final fallback: import from src.ea
                    from src.ea import get_create_final_statistics_with_tracker
            
            # Load population for basic stats
            population = load_population(str(get_outputs_path()), logger=logger)
            
            # Load evolution tracker for comprehensive stats
            evolution_tracker_path = get_outputs_path() / "EvolutionTracker.json"
            if evolution_tracker_path.exists():
                with open(evolution_tracker_path, 'r', encoding='utf-8') as f:
                    evolution_tracker = json.load(f)
                
                # Create comprehensive final statistics using tracker
                create_final_statistics_with_tracker = get_create_final_statistics_with_tracker()
                final_stats = create_final_statistics_with_tracker(
                        evolution_tracker, 
                        north_star_metric, 
                        total_time, 
                        generation_count,
                        logger=logger
                    )
                    
                logger.info("Enhanced Final Statistics:")
                logger.info("  - Total prompts: %d", final_stats["execution_summary"]["total_prompts"])
                logger.info("  - Completed prompts: %d", final_stats["execution_summary"]["completed_prompts"])
                logger.info("  - Completion rate: %.1f%%", final_stats["execution_summary"]["completion_rate"])
                logger.info("  - Total generations: %d", final_stats["generation_statistics"]["total_generations"])
                logger.info("  - Average generations per prompt: %.1f", final_stats["generation_statistics"]["average_generations_per_prompt"])
                logger.info("  - Best average score: %.4f", final_stats["score_statistics"]["best_average_score"])
                logger.info("  - Max score achieved: %.4f", final_stats["score_statistics"]["max_score"])
                logger.info("  - Total variants created: %d", final_stats["variant_statistics"]["total_variants_created"])
                logger.info("  - Average variants per generation: %.1f", final_stats["variant_statistics"]["average_variants_per_generation"])
                
            else:
                # Fallback to basic statistics if tracker doesn't exist
                logger.warning("Evolution tracker not found, using basic statistics")
                final_stats = {
                    "total_genomes": len(population),
                    "successful_genomes": len([g for g in population if g is not None and g.get("status") == "complete"]),
                    "average_score": sum([_extract_north_star_score(g, north_star_metric) 
                                        for g in population if g is not None]) / max(len([g for g in population if g is not None]), 1),
                    "execution_time_seconds": total_time,
                    "generations_completed": generation_count
                }
                
                logger.info("Basic Final Statistics:")
                logger.info("  - Total genomes: %d", final_stats['total_genomes'])
                logger.info("  - Successful genomes: %d", final_stats['successful_genomes'])
                logger.info("  - Average %s score: %.4f", north_star_metric, final_stats['average_score'])
            
            with open(get_outputs_path() / "final_statistics.json", "w") as f:
                json.dump(final_stats, f, indent=2)
                
        except Exception as e:
            logger.error("Failed to generate final statistics: %s", e, exc_info=True)

# ============================================================================
# SECTION 6: MAIN ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Evolutionary Text Generation and Safety Analysis Framework")
    parser.add_argument("--generations", type=int, default=None, 
                       help="Maximum number of evolution generations. If not set, runs until north star metric is achieved.")
    parser.add_argument("--threshold", type=float, default=0.99,
                       help="North star metric threshold for stopping evolution (default: 0.99)")
    parser.add_argument("--moderation-methods", nargs="+", choices=["google", "openai", "all"], default=["google"],
                       help="Moderation methods to use: 'google' (Perspective API), 'openai' (OpenAI Moderation), 'all' (both). Default: google")
    parser.add_argument("model_names", nargs="*", default=[], 
                       help="Model names to use (currently not used)")
    args = parser.parse_args()
    
    try:
        main(model_names=args.model_names, max_generations=args.generations, 
             north_star_threshold=args.threshold, moderation_methods=args.moderation_methods)
    except KeyboardInterrupt:
        print("\nPipeline interrupted by user.")
        cleanup_multiprocessing()
        sys.exit(1)
    except Exception as e:
        print(f"Fatal error: {e}")
        cleanup_multiprocessing()
        sys.exit(1)